[{"id":11,"title":"Multi-source Log Clustering in Distributed Systems","doi":"10.1007/978-981-33-6385-4_4","description":"null","venue":"","listofauthors":"John E. Raffety, Brooklynn Stone, J. Svacina, Connor Woodahl, T. Cerný, Pavel Tisnovsky","citations":0,"year":2021,"publisher":"Springer Singapore","pages":"31-41","volume":null,"number":null,"bibtex":"@incollection{2021,\n\tdoi = {10.1007/978-981-33-6385-4_4},\n\turl = {https://doi.org/10.1007%2F978-981-33-6385-4_4},\n\tyear = 2021,\n\tpublisher = {Springer Singapore},\n\tpages = {31--41},\n\tauthor = {Jackson Raffety and Brooklynn Stone and Jan Svacina and Connor Woodahl and Tomas Cerny and Pavel Tisnovsky},\n\ttitle = {Multi-source Log Clustering in Distributed Systems}\n}","authorsSemantic":[1]},{"id":12,"title":"On Log Analysis and Stack Trace Use to Improve Program Slicing","doi":"10.1007/978-981-33-6385-4_25","description":"null","venue":"","listofauthors":"Vincent Bushong, Jacob Curtis, Russell Sanders, Mark Du, T. Cerný, Karel Frajták, Pavel Tisnovsky, Dongwan Shin","citations":0,"year":2021,"publisher":"Springer Singapore","pages":"265-275","volume":null,"number":null,"bibtex":"@incollection{2021,\n\tdoi = {10.1007/978-981-33-6385-4_25},\n\turl = {https://doi.org/10.1007%2F978-981-33-6385-4_25},\n\tyear = 2021,\n\tpublisher = {Springer Singapore},\n\tpages = {265--275},\n\tauthor = {Vincent Bushong and Jacob Curtis and Russell Sanders and Mark Du and Tomas Cerny and Karel Frajtak and Pavel Tisnovsky and Dongwan Shin},\n\ttitle = {On Log Analysis and Stack Trace Use to Improve Program Slicing}\n}","authorsSemantic":[1]},{"id":13,"title":"Online Energy Scheduling Policies in Energy Harvesting Enabled D2D Communications","doi":"10.1109/TII.2020.3005440","description":"Energy efficiency plays a vital role in device-to-device communications, which has been recognized as a key challenge. In this article, we study the implications of the peak power constraints and processing cost on the energy scheduling policy, and find that the peak power should be used around the time slots of each energy arrival, and the only time slot in which the intermittent communication may occur is the last time slot. A new optimal energy scheduling algorithm is devised based on these observations. Also, we propose a near-optimal energy scheduling algorithm that simultaneously takes into account the peak power constraints and the processing cost. Our simulation results demonstrate the effectiveness of the proposed energy scheduling algorithm and the validity of the mathematical analyses.","venue":"IEEE Transactions on Industrial Informatics","listofauthors":"Jun Huang, Baohua Yu, Cong-Cong Xing, T. Cerný, Zhaolong Ning","citations":1,"year":2021,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"5678-5687","volume":"17","number":"8","bibtex":"@article{2021,\n\tdoi = {10.1109/tii.2020.3005440},\n\turl = {https://doi.org/10.1109%2Ftii.2020.3005440},\n\tyear = 2021,\n\tmonth = {aug},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {17},\n\tnumber = {8},\n\tpages = {5678--5687},\n\tauthor = {Jun Huang and Baohua Yu and Cong-Cong Xing and Tomas Cerny and Zhaolong Ning},\n\ttitle = {Online Energy Scheduling Policies in Energy Harvesting Enabled D2D Communications}\n}","authorsSemantic":[1]},{"id":14,"title":"Database-Conscious End-to-End Testing for Reactive Systems using Containerization","doi":"10.5220/0010494403770383","description":"null","venue":"ICEIS","listofauthors":"Denton Wood, T. Cerný","citations":0,"year":2021,"publisher":"SCITEPRESS - Science and Technology Publications","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2021,\n\tdoi = {10.5220/0010494403770383},\n\turl = {https://doi.org/10.5220%2F0010494403770383},\n\tyear = 2021,\n\tpublisher = {{SCITEPRESS} - Science and Technology Publications},\n\tauthor = {Denton Wood and Tom{\\'{a}}{\\v{s}} {\\v{C}}ern{\\'{y}}},\n\ttitle = {Database-Conscious End-to-End Testing for Reactive Systems using Containerization}\n}","authorsSemantic":[1]},{"id":15,"title":"On Automatic Software Architecture Reconstruction of Microservice Applications","doi":"10.1007/978-981-33-6385-4_21","description":"null","venue":"","listofauthors":"Andrew Walker, Ian Laird, T. Cerný","citations":0,"year":2021,"publisher":"Springer Singapore","pages":"223-234","volume":null,"number":null,"bibtex":"@incollection{2021,\n\tdoi = {10.1007/978-981-33-6385-4_21},\n\turl = {https://doi.org/10.1007%2F978-981-33-6385-4_21},\n\tyear = 2021,\n\tpublisher = {Springer Singapore},\n\tpages = {223--234},\n\tauthor = {Andrew Walker and Ian Laird and Tomas Cerny},\n\ttitle = {On Automatic Software Architecture Reconstruction of Microservice Applications}\n}","authorsSemantic":[1]},{"id":16,"title":"Automated Microservice Code-Smell Detection","doi":"10.1007/978-981-33-6385-4_20","description":"null","venue":"","listofauthors":"Andrew Walker, Dipta Das, T. Cerný","citations":0,"year":2021,"publisher":"Springer Singapore","pages":"211-221","volume":null,"number":null,"bibtex":"@incollection{2021,\n\tdoi = {10.1007/978-981-33-6385-4_20},\n\turl = {https://doi.org/10.1007%2F978-981-33-6385-4_20},\n\tyear = 2021,\n\tpublisher = {Springer Singapore},\n\tpages = {211--221},\n\tauthor = {Andrew Walker and Dipta Das and Tomas Cerny},\n\ttitle = {Automated Microservice Code-Smell Detection}\n}","authorsSemantic":[1]},{"id":17,"title":"Pyclone: A Python Code Clone Test Bank Generator","doi":"10.1007/978-981-33-6385-4_22","description":"null","venue":"","listofauthors":"Schaeffer Duncan, Andrew Walker, Caleb DeHaan, Stephanie Alvord, T. Cerný, Pavel Tisnovsky","citations":0,"year":2021,"publisher":"Springer Singapore","pages":"235-243","volume":null,"number":null,"bibtex":"@incollection{2021,\n\tdoi = {10.1007/978-981-33-6385-4_22},\n\turl = {https://doi.org/10.1007%2F978-981-33-6385-4_22},\n\tyear = 2021,\n\tpublisher = {Springer Singapore},\n\tpages = {235--243},\n\tauthor = {Schaeffer Duncan and Andrew Walker and Caleb DeHaan and Stephanie Alvord and Tomas Cerny and Pavel Tisnovsky},\n\ttitle = {Pyclone: A Python Code Clone Test Bank Generator}\n}","authorsSemantic":[1]},{"id":18,"title":"A Comprehensive Enterprise System Metamodel for Quality Assurance","doi":"10.1007/978-981-33-6385-4_23","description":"null","venue":"","listofauthors":"J. Svacina, Vincent Bushong, Dipta Das, T. Cerný","citations":0,"year":2021,"publisher":"Springer Singapore","pages":"245-252","volume":null,"number":null,"bibtex":"@incollection{2021,\n\tdoi = {10.1007/978-981-33-6385-4_23},\n\turl = {https://doi.org/10.1007%2F978-981-33-6385-4_23},\n\tyear = 2021,\n\tpublisher = {Springer Singapore},\n\tpages = {245--252},\n\tauthor = {Jan Svacina and Vincent Bushong and Dipta Das and Tomas Cerny},\n\ttitle = {A Comprehensive Enterprise System Metamodel for Quality Assurance}\n}","authorsSemantic":[1]},{"id":19,"title":"Session details: Theme: Software design and development: QASM - Quality assurance and software mining track","doi":"10.1145/3462428","description":"null","venue":"","listofauthors":"T. Cerný, Miroslav Bures, Richard Lipka, B. Rossi","citations":0,"year":2021,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2021,\n\tdoi = {10.1145/3462428},\n\turl = {https://doi.org/10.1145%2F3462428},\n\tyear = 2021,\n\tmonth = {mar},\n\tpublisher = {{ACM}},\n\tauthor = {Tomas Cerny and Miroslav Bures and Richard Lipka and Bruno Rossi},\n\ttitle = {Session details: Theme: Software design and development: {QASM} - Quality assurance and software mining track}\n}","authorsSemantic":[1]},{"id":20,"title":"Automated error log resolution: a case study","doi":"10.1145/3412841.3442004","description":"Debugging and error resolution has become increasingly time-consuming and difficult for all domains of software development. Error logs have become very important when it comes to debugging and error resolution. To remedy the problems presented in the logs, typically, a search on online forums would shed light on the solution. We present a novel approach to utilizing these logs in conjunction with external Question and Answer forums to compute and expedite resolution by suggesting a solution to runtime errors. Since log format is non-standard and use cases can vary widely, our architecture allows for extreme customization for the intended ecosystem as well as a great degree of fine-tuning. We evaluated our solution in a case study and made our implementation open-source for the community.","venue":"SAC","listofauthors":"Mark Fuller, Elizabeth Brighton, Micah Schiewe, Dipta Das, T. Cerný, Pavel Tisnovsky","citations":0,"year":2021,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2021,\n\tdoi = {10.1145/3412841.3442004},\n\turl = {https://doi.org/10.1145%2F3412841.3442004},\n\tyear = 2021,\n\tmonth = {mar},\n\tpublisher = {{ACM}},\n\tauthor = {Mark Fuller and Elizabeth Brighton and Micah Schiewe and Dipta Das and Tomas Cerny and Pavel Tisnovsky},\n\ttitle = {Automated error log resolution}\n}","authorsSemantic":[1]},{"id":151,"title":"The Pocket Guide to TCP/IP Sockets : C Version","doi":null,"description":"null","venue":"","listofauthors":"M. J. Donahoo, K. Calvert","citations":12,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":152,"title":"A Greedy Approach For Improving Update Processing In Intermittently Synchronized Databases","doi":null,"description":"null","venue":"","listofauthors":"W. Yee, E. Omiecinski, S. Navathe, M. Ammar, M. J. Donahoo, S. Malik","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":21,"title":"On automated RBAC assessment by constructing a centralized perspective for microservice mesh","doi":"10.7717/peerj-cs.376","description":"It is important in software development to enforce proper restrictions on protected services and resources. Typically software services can be accessed through REST API endpoints where restrictions can be applied using the Role-Based Access Control (RBAC) model. However, RBAC policies can be inconsistent across services, and they require proper assessment. Currently, developers use penetration testing, which is a costly and cumbersome process for a large number of APIs. In addition, modern applications are split into individual microservices and lack a unified view in order to carry out automated RBAC assessment. Often, the process of constructing a centralized perspective of an application is done using Systematic Architecture Reconstruction (SAR). This article presents a novel approach to automated SAR to construct a centralized perspective for a microservice mesh based on their REST communication pattern. We utilize the generated views from SAR to propose an automated way to find RBAC inconsistencies.","venue":"PeerJ Comput. Sci.","listofauthors":"Dipta Das, Andrew Walker, Vincent Bushong, J. Svacina, T. Cerný, V. Matyas","citations":0,"year":2021,"publisher":"PeerJ","pages":"e376","volume":"7","number":null,"bibtex":"@article{2021,\n\tdoi = {10.7717/peerj-cs.376},\n\turl = {https://doi.org/10.7717%2Fpeerj-cs.376},\n\tyear = 2021,\n\tmonth = {feb},\n\tpublisher = {{PeerJ}},\n\tvolume = {7},\n\tpages = {e376},\n\tauthor = {Dipta Das and Andrew Walker and Vincent Bushong and Jan Svacina and Tomas Cerny and Vashek Matyas},\n\ttitle = {On automated {RBAC} assessment by constructing a centralized perspective for microservice mesh}\n}","authorsSemantic":[1]},{"id":22,"title":"Differential privacy applied to smart meters: a mapping study","doi":"10.1145/3412841.3442360","description":"Smart meters and the smart grid will allow utility companies and customers to monitor their electricity and utility usage in fine-grained detail instead of the previously common monthly or yearly measurements. With this fine-grained detail comes serious privacy concerns. One of the most promising solutions for measuring and preserving privacy loss is differential privacy. Both differential privacy and the smart grid are relatively young developments that will require more research before they can be confidently implemented worldwide. With this systematic mapping study, we will provide an overview of the current literature and attempt to determine the future directions the research may take.","venue":"SAC","listofauthors":"Jacob Marks, Brandon Montano, Jiwan Chong, Manjusha Raavi, Raisa Islam, T. Cerný, Dongwan Shin","citations":0,"year":2021,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2021,\n\tdoi = {10.1145/3412841.3442360},\n\turl = {https://doi.org/10.1145%2F3412841.3442360},\n\tyear = 2021,\n\tmonth = {mar},\n\tpublisher = {{ACM}},\n\tauthor = {Jacob Marks and Brandon Montano and Jiwan Chong and Manjusha Raavi and Raisa Islam and Tomas Cerny and Dongwan Shin},\n\ttitle = {Differential privacy applied to smart meters}\n}","authorsSemantic":[1]},{"id":23,"title":"Behavior control-based approach to influencing user's cybersecurity actions using mobile news app","doi":"10.1145/3412841.3442103","description":"In this paper, we propose that the theory of planned behavior (TPB) with the additional factors of awareness and context-based information can be used to positively influence users' cybersecurity behavior. A research model based on TPB is developed and validated using a user study. As a proof-of-concept, we developed a mobile cybersecurity news app that incorporates context-based information such as location, search history, and usage information of other mobile apps into its article recommendations and warning notifications to address user awareness better. Through a survey of 100 participants, the proposed research model was validated, and it was confirmed that context-based information positively influences users' awareness in cybersecurity.","venue":"SAC","listofauthors":"Vince Lombardi, Sarah Ortiz, Jen Phifer, T. Cerný, Dongwan Shin","citations":0,"year":2021,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2021,\n\tdoi = {10.1145/3412841.3442103},\n\turl = {https://doi.org/10.1145%2F3412841.3442103},\n\tyear = 2021,\n\tmonth = {mar},\n\tpublisher = {{ACM}},\n\tauthor = {Vincent Lombardi and Sarah Ortiz and Jen Phifer and Tomas Cerny and Dongwan Shin},\n\ttitle = {Behavior control-based approach to influencing user{\\textquotesingle}s cybersecurity actions using mobile news app}\n}","authorsSemantic":[1]},{"id":24,"title":"Securing Internet of Things Devices Using The Network Context","doi":"10.1109/TII.2019.2954100","description":"Internet of Things (IoT) devices have been widely adopted in recent years. Unlike conventional information systems, IoT solutions have greater access to real-world contextual data and are typically deployed in an environment that cannot be fully controlled, and these circumstances create new challenges and opportunities. In this article, we leverage the knowledge that an IoT device has about its network context to provide an additional security factor. The device periodically scans a network and reports a list of all devices in the network. The server analyzes movements in the network and subsequently reacts to suspicious events. This article describes how our method can detect network changes, retrieved only from scanning devices in the network. To demonstrate the proposed solution, we perform a multiweek case study on a network with hundreds of active devices and confirm that our method can detect network anomalies or changes.","venue":"IEEE Transactions on Industrial Informatics","listofauthors":"Michal Trnka, J. Svacina, T. Cerný, Eunjee Song, Jiman Hong, Miroslav Bures","citations":2,"year":2020,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"4017-4027","volume":"16","number":"6","bibtex":"@article{2020,\n\tdoi = {10.1109/tii.2019.2954100},\n\turl = {https://doi.org/10.1109%2Ftii.2019.2954100},\n\tyear = 2020,\n\tmonth = {jun},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {16},\n\tnumber = {6},\n\tpages = {4017--4027},\n\tauthor = {Michal Trnka and Jan Svacina and Tomas Cerny and Eunjee Song and Jiman Hong and Miroslav Bures},\n\ttitle = {Securing Internet of Things Devices Using The Network Context}\n}","authorsSemantic":[1,9]},{"id":25,"title":"On cloud computing infrastructure for existing code-clone detection algorithms","doi":"10.1145/3392350.3392351","description":"Microservice Architecture (MSA) is becoming a design standard for modern cloud-based software systems. However, even though cloud-based applications have been thoroughly explored with regards to ne...","venue":"","listofauthors":"Andrew Walker, T. Cerný","citations":4,"year":2020,"publisher":"Association for Computing Machinery (ACM)","pages":"5-14","volume":"20","number":"1","bibtex":"@article{2020,\n\tdoi = {10.1145/3392350.3392351},\n\turl = {https://doi.org/10.1145%2F3392350.3392351},\n\tyear = 2020,\n\tmonth = {apr},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {20},\n\tnumber = {1},\n\tpages = {5--14},\n\tauthor = {Andrew Walker and Tomas Cerny},\n\ttitle = {On cloud computing infrastructure for existing code-clone detection algorithms}\n}","authorsSemantic":[1]},{"id":68,"title":"Aspect-Oriented User Interfaces Design Integration to Angular 2 Framework","doi":"10.1109/ICITCS.2016.7740314","description":"Conventional user interface design introduces multiple complexities for the development perspective. This is usually caused by the information restatements and mostly by mixing various concerns that result in tangling the user interface code. This paper address the above issue with aspect- oriented user interface design approach that can more effectively copy with separation of concerns and thus result with reduced complexity of development and maintenance. Next, we present a small case study of an approach prototype to demonstrate its capabilities.","venue":"2016 6th International Conference on IT Convergence and Security (ICITCS)","listofauthors":"Filip Rysavy, T. Cerný, M. Tomásek","citations":0,"year":2016,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1109/icitcs.2016.7740314},\n\turl = {https://doi.org/10.1109%2Ficitcs.2016.7740314},\n\tyear = 2016,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Filip Rysavy and Tomas Cerny and Martin Tomasek},\n\ttitle = {Aspect-Oriented User Interfaces Design Integration to Angular 2 Framework}\n}","authorsSemantic":[1]},{"id":26,"title":"On Vulnerability and Security Log analysis: A Systematic Literature Review on Recent Trends","doi":"10.1145/3400286.3418261","description":"Log analysis is a technique of deriving knowledge from log files containing records of events in a computer system. A common application of log analysis is to derive critical information about a system's security issues and intrusions, which subsequently leads to being able to identify and potentially stop intruders attacking the system. However, many systems produce a high volume of log data with high frequency, posing serious challenges in analysis. This paper contributes with a systematic literature review and discusses current trends, advancements, and future directions in log security analysis within the past decade. We summarized current research strategies with respect to technology approaches from 34 current publications. We identified limitations that poses challenges to future research and opened discussion on issues towards logging mechanism in the software systems. Findings of this study are relevant for software systems as well as software parts of the Internet of Things (IoT) systems.","venue":"RACS","listofauthors":"J. Svacina, John E. Raffety, Connor Woodahl, Brooklynn Stone, T. Cerný, Miroslav Bures, Dongwan Shin, Karel Frajták, Pavel Tisnovsky","citations":4,"year":2020,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2020,\n\tdoi = {10.1145/3400286.3418261},\n\turl = {https://doi.org/10.1145%2F3400286.3418261},\n\tyear = 2020,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Jan Svacina and Jackson Raffety and Connor Woodahl and Brooklynn Stone and Tomas Cerny and Miroslav Bures and Dongwan Shin and Karel Frajtak and Pavel Tisnovsky},\n\ttitle = {On Vulnerability and Security Log analysis}\n}","authorsSemantic":[1]},{"id":27,"title":"Automated Code-Smell Detection in Microservices Through Static Analysis: A Case Study","doi":"10.3390/app10217800","description":"Microservice Architecture (MSA) is becoming the predominant direction of new cloud-based applications. There are many advantages to using microservices, but also downsides to using a more complex architecture than a typical monolithic enterprise application. Beyond the normal poor coding practices and code smells of a typical application, microservice-specific code smells are difficult to discover within a distributed application setup. There are many static code analysis tools for monolithic applications, but tools to offer code-smell detection for microservice-based applications are lacking. This paper proposes a new approach to detect code smells in distributed applications based on microservices. We develop an MSANose tool to detect up to eleven different microservice specific code smells and share it as open-source. We demonstrate our tool through a case study on two robust benchmark microservice applications and verify its accuracy. Our results show that it is possible to detect code smells within microservice applications using bytecode and/or source code analysis throughout the development process or even before its deployment to production.","venue":"","listofauthors":"Andrew Walker, Dipta Das, T. Cerný","citations":3,"year":2020,"publisher":"MDPI AG","pages":"7800","volume":"10","number":"21","bibtex":"@article{2020,\n\tdoi = {10.3390/app10217800},\n\turl = {https://doi.org/10.3390%2Fapp10217800},\n\tyear = 2020,\n\tmonth = {nov},\n\tpublisher = {{MDPI} {AG}},\n\tvolume = {10},\n\tnumber = {21},\n\tpages = {7800},\n\tauthor = {Andrew Walker and Dipta Das and Tomas Cerny},\n\ttitle = {Automated Code-Smell Detection in Microservices Through Static Analysis: A Case Study}\n}","authorsSemantic":[1]},{"id":28,"title":"Mapping Study on Constraint Consistency Checking in Distributed Enterprise Systems","doi":"10.1145/3400286.3418257","description":"Constraint consistency errors in distributed systems can lead to fatal consequences when left unobserved and undetected. The primary goal of quality engineers should be to avoid system inconsistencies in general. However, it is typically a much more straight forward process in monolith-like systems with one codebase than in distributed solutions where heterogeneity occurs across modules. In this paper, we raise the research question of what is the existing state-of-the-art and research literature practice when it comes to consistency checking in distributed systems. We conducted a systematic search for existing work and assess the evidence to categorize the approaches and to identify used techniques. Identified works offer interesting directions and achievements. Often the works share tool prototypes and instruments to build on the top of when performing further research in this direction and we share them in this paper. Finally, we discuss open challenges and gaps in this field to promote the interest of the research audience.","venue":"RACS","listofauthors":"T. Cerný, Andrew Walker, J. Svacina, Vincent Bushong, Dipta Das, Karel Frajták, Miroslav Bures, Pavel Tisnovsky","citations":3,"year":2020,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2020,\n\tdoi = {10.1145/3400286.3418257},\n\turl = {https://doi.org/10.1145%2F3400286.3418257},\n\tyear = 2020,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Tomas Cerny and Andrew Walker and Jan Svacina and Vincent Bushong and Dipta Das and Karel Frajtak and Miroslav Bures and Pavel Tisnovsky},\n\ttitle = {Mapping Study on Constraint Consistency Checking in Distributed Enterprise Systems}\n}","authorsSemantic":[1]},{"id":29,"title":"Open-source tools and benchmarks for code-clone detection: past, present, and future trends","doi":"10.1145/3381307.3381310","description":"A fragment of source code that is identical or similar to another is a code-clone. Code-clones make it difficult to maintain applications as they create multiple points within the code that bugs must be fixed, new rules enforced, or design decisions imposed. As applications grow larger and larger, the pervasiveness of code-clones likewise grows. To face the code-clone related issues, many tools and algorithms have been proposed to find and document code-clones within an application. In this paper, we present the historical trends in code-clone detection tools to show how we arrived at the current implementations. We then present our results from a systematic mapping study on current (2009-2019) code-clone detection tools with regards to technique, open-source nature, and language coverage. Lastly, we propose future directions for code-clone detection tools. This paper provides the essentials to understanding the code-clone detection process and the current state-of-art solutions.","venue":"SIAP","listofauthors":"Andrew Walker, T. Cerný, Eungee Song","citations":9,"year":2020,"publisher":"Association for Computing Machinery (ACM)","pages":"28-39","volume":"19","number":"4","bibtex":"@article{2020,\n\tdoi = {10.1145/3381307.3381310},\n\turl = {https://doi.org/10.1145%2F3381307.3381310},\n\tyear = 2020,\n\tmonth = {jan},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {19},\n\tnumber = {4},\n\tpages = {28--39},\n\tauthor = {Andrew Walker and Tomas Cerny and Eungee Song},\n\ttitle = {Open-source tools and benchmarks for code-clone detection}\n}","authorsSemantic":[1]},{"id":30,"title":"Session details: Theme: Software design and development: CASoM - Code analysis and software mining track","doi":"10.1145/3389642","description":"null","venue":"","listofauthors":"T. Cerný, Premek Brada, Miroslav Bures","citations":0,"year":2020,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2020,\n\tdoi = {10.1145/3389642},\n\turl = {https://doi.org/10.1145%2F3389642},\n\tyear = 2020,\n\tmonth = {mar},\n\tpublisher = {{ACM}},\n\tauthor = {Tomas Cerny and Premek Brada and Miroslav Bures},\n\ttitle = {Session details: Theme: Software design and development: {CASoM} - Code analysis and software mining track}\n}","authorsSemantic":[1]},{"id":307,"title":"The Functional Hardware Design Language","doi":null,"description":"The Functional Hardware Design Language is an expandable language that is designed to make it easy to specify gate-level. FHDL currently supports the specification of gates, high-level functional blocks, state machines, ROM and PLA contents, and parameterized functional blocks. The language is easily extendable. THE FUNCTIONAL HARDWARE DESIGN LANGUAGE Peter M. Maurer Department of Computer Science Baylor University Waco, TX 76798","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":308,"title":"The Hyper-Linear Package","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":31,"title":"Semantic code clone detection for enterprise applications","doi":"10.1145/3341105.3374117","description":"Enterprise systems are widely adopted across industries as methods of solving complex problems. As software complexity increases, the software's codebase becomes harder to manage and maintenance costs raise significantly. One such source of cost-raising complexity and code bloat is that of code clones. We proposed an approach to identify semantic code clones in enterprise frameworks by using control flow graphs (CFGs) and applying various proprietary similarity functions to compare enterprise targeted metadata for each pair of CFGs. This approach enables us to detect semantic code clones with high accuracy within a time complexity of O(n2) where n is equal to the number of CFGs composed in the enterprise application (usually around hundreds). We demonstrated our solution on a blind study utilizing a production enterprise application.","venue":"SAC","listofauthors":"J. Svacina, Jonathan Simmons, T. Cerný","citations":1,"year":2020,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2020,\n\tdoi = {10.1145/3341105.3374117},\n\turl = {https://doi.org/10.1145%2F3341105.3374117},\n\tyear = 2020,\n\tmonth = {mar},\n\tpublisher = {{ACM}},\n\tauthor = {Jan Svacina and Jonathan Simmons and Tomas Cerny},\n\ttitle = {Semantic code clone detection for enterprise applications}\n}","authorsSemantic":[1]},{"id":32,"title":"On Matching Log Analysis to Source Code: A Systematic Mapping Study","doi":"10.1145/3400286.3418262","description":"Logging is a vital part of the software development process. Developers use program logging to monitor program execution and identify errors and anomalies. These errors may also cause uncaught exceptions and generate stack traces that help identify the point of error. Both of these sources contain information that can be matched to points in the source code, but manual log analysis is challenging for large systems that create large volumes of logs and have large codebases. In this paper, we contribute a systematic mapping study to determine the state-of-the-art tools and methods used to perform automatic log analysis and stack trace analysis and match the extracted information back to the program's source code. We analyzed 16 publications that address this issue, summarizing their strategies and goals, and we identified open research directions from this body of work.","venue":"RACS","listofauthors":"Vincent Bushong, Russell Sanders, Jacob Curtis, Mark Du, T. Cerný, Karel Frajták, Miroslav Bures, Pavel Tisnovsky, Dongwan Shin","citations":3,"year":2020,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2020,\n\tdoi = {10.1145/3400286.3418262},\n\turl = {https://doi.org/10.1145%2F3400286.3418262},\n\tyear = 2020,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Vincent Bushong and Russell Sanders and Jacob Curtis and Mark Du and Tomas Cerny and Karel Frajtak and Miroslav Bures and Pavel Tisnovsky and Dongwan Shin},\n\ttitle = {On Matching Log Analysis to Source Code}\n}","authorsSemantic":[1]},{"id":33,"title":"On Code Analysis Opportunities and Challenges for Enterprise Systems and Microservices","doi":"10.1109/ACCESS.2020.3019985","description":"Code analysis brings excellent benefits to software development, maintenance, and quality assurance. Various tools can uncover code defects or even software bugs in a range of seconds. For many projects and developers, the code analysis tools became essential in their daily routines. However, how can code analysis help in an enterprise environment? Enterprise software solutions grow in scale and complexity. These solutions no longer involve only plain objects and basic language constructs but operate with various components and mechanisms simplifying the development of such systems. Enterprise software vendors have adopted various development and design standards; however, there is a gap between what constructs the enterprise frameworks use and what current code analysis tools recognize. This manuscript aims to challenge the mainstream research directions of code analysis and motivate for a transition towards code analysis of enterprise systems with interesting problems and opportunities. In particular, this manuscript addresses selected enterprise problems apparent for monolithic and distributed enterprise solutions. It also considers challenges related to the recent architectural push towards a microservice architecture. Along with open-source proof-of-concept prototypes to some of the challenges, this manuscript elaborates on code analysis directions and their categorization. Furthermore, it suggests one possible perspective of the problem area using aspect-oriented programming.","venue":"IEEE Access","listofauthors":"T. Cerný, J. Svacina, Dipta Das, Vincent Bushong, Miroslav Bures, Pavel Tisnovsky, Karel Frajták, Dongwan Shin, Jun Huang","citations":5,"year":2020,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"159449-159470","volume":"8","number":null,"bibtex":"@article{2020,\n\tdoi = {10.1109/access.2020.3019985},\n\turl = {https://doi.org/10.1109%2Faccess.2020.3019985},\n\tyear = 2020,\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {8},\n\tpages = {159449--159470},\n\tauthor = {Tomas Cerny and Jan Svacina and Dipta Das and Vincent Bushong and Miroslav Bures and Pavel Tisnovsky and Karel Frajtak and Dongwan Shin and Jun Huang},\n\ttitle = {On Code Analysis Opportunities and Challenges for Enterprise Systems and Microservices}\n}","authorsSemantic":[1]},{"id":34,"title":"Failure Prediction by Utilizing Log Analysis: A Systematic Mapping Study","doi":"10.1145/3400286.3418263","description":"In modern computing, log files provide a wealth of information regarding the past of a system, including the system failures and security breaches that cost companies and developers a fortune in both time and money. While this information can be used to attempt to recover from a problem, such an approach merely mitigates the damage that has already been done. Detecting problems, however, is not the only information that can be gathered from log files. It is common knowledge that segments of log files, if analyzed correctly, can yield a good idea of what the system is likely going to do next in real-time, allowing a system to take corrective action before any negative actions occur. In this paper, the authors put forth a systematic map of this field of log prediction, screening several hundred papers and finally narrowing down the field to approximately 30 relevant papers. These papers, when broken down, give a good idea of the state of the art, methodologies employed, and future challenges that still must be overcome. Findings and conclusions of this study can be applied to a variety of software systems and components, including classical software systems, as well as software parts of control, or the Internet of Things (IoT) systems.","venue":"RACS","listofauthors":"Dipta Das, Micah Schiewe, Elizabeth Brighton, Mark Fuller, T. Cerný, Miroslav Bures, Karel Frajták, Dongwan Shin, Pavel Tisnovsky","citations":3,"year":2020,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2020,\n\tdoi = {10.1145/3400286.3418263},\n\turl = {https://doi.org/10.1145%2F3400286.3418263},\n\tyear = 2020,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Dipta Das and Micah Schiewe and Elizabeth Brighton and Mark Fuller and Tomas Cerny and Miroslav Bures and Karel Frajtak and Dongwan Shin and Pavel Tisnovsky},\n\ttitle = {Failure Prediction by Utilizing Log Analysis}\n}","authorsSemantic":[1]},{"id":40,"title":"Case study on data communication in microservice architecture","doi":"10.1145/3338840.3355659","description":"Microservice Architecture is becoming a design standard for modern cloud-based software systems. However, data communication management remains a challenge. This is especially apparent when migrating from an existing monolithic system into microservices. In this paper, we report on data synchronization and improvement of the data-source performance. We faced these challenges in production-level development. Two case studies illustrate and describe our approach. To address data synchronization we propose using an automated data streaming system between databases. To improve the performance of a data-source we introduced a solution with the distributed cache. We discuss the balance between the performance and coupling and point out situations where our architectures are appropriate.","venue":"RACS","listofauthors":"Antonin Smid, Ruolin Wang, T. Cerný","citations":1,"year":2019,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1145/3338840.3355659},\n\turl = {https://doi.org/10.1145%2F3338840.3355659},\n\tyear = 2019,\n\tmonth = {sep},\n\tpublisher = {{ACM}},\n\tauthor = {Antonin Smid and Ruolin Wang and Tomas Cerny},\n\ttitle = {Case study on data communication in microservice architecture}\n}","authorsSemantic":[1]},{"id":35,"title":"Aspects of Quality in Internet of Things (IoT) Solutions: A Systematic Mapping Study","doi":"10.1109/ACCESS.2019.2893493","description":"Internet of Things (IoT) is an emerging technology that has the promising power to change our future. Due to the market pressure, IoT systems may be released without sufficient testing. However, it is no longer acceptable to release IoT systems to the market without assuring the quality. As in the case of new technologies, the quality assurance process is a challenging task. This paper shows the results of the first comprehensive and systematic mapping study to structure and categories the research evidence in the literature starting in 2009 when the early publication of IoT papers for IoT quality assurance appeared. The conducted research is based on the most recent guidelines on how to perform systematic mapping studies. A set of research questions is defined carefully regarding the quality aspects of the IoT. Based on these questions, a large number of evidence and research papers is considered in the study (478 papers). We have extracted and analyzed different levels of information from those considered papers. Also, we have classified the topics addressed in those papers into categories based on the quality aspects. The study results carry out different areas that require more work and investigation in the context of IoT quality assurance. The results of the study can help in a further understanding of the research gaps. Moreover, the results show a roadmap for future research directions.","venue":"IEEE Access","listofauthors":"Bestoun S. Ahmed, Miroslav Bures, Karel Frajták, T. Cerný","citations":28,"year":2019,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"13758-13780","volume":"7","number":null,"bibtex":"@article{2019,\n\tdoi = {10.1109/access.2019.2893493},\n\turl = {https://doi.org/10.1109%2Faccess.2019.2893493},\n\tyear = 2019,\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {7},\n\tpages = {13758--13780},\n\tauthor = {Bestoun S. Ahmed and Miroslav Bures and Karel Frajtak and Tomas Cerny},\n\ttitle = {Aspects of Quality in Internet of Things ({IoT}) Solutions: A Systematic Mapping Study}\n}","authorsSemantic":[1]},{"id":36,"title":"Intelligent token-based code clone detection system for large scale source code","doi":"10.1145/3338840.3355654","description":"A code clone refers to code fragments in the source code that are identical or similar to each other. Code clones lead difficulties in software maintenance, bug fixing, present poor design and increase the system size. Code clone detection techniques and tools have been proposed by many researchers, however, there is a lack of clone detection techniques especially for large scale repositories. In this paper, we present a token-based clone detector called Intelligent Clone Detection Tool (ICDT) that can detect both exact and near-miss clones from large repositories using a standard workstation environment. In order to evaluate the scalability and the efficiency of ICDT, we use the most recent benchmark which is a big benchmark of real clones, BigCloneBench. In addition, we compare ICDT to four publicly available and state-of-the-art tools.","venue":"RACS","listofauthors":"Abdulrahman Abu Elkhail, J. Svacina, T. Cerný","citations":2,"year":2019,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1145/3338840.3355654},\n\turl = {https://doi.org/10.1145%2F3338840.3355654},\n\tyear = 2019,\n\tmonth = {sep},\n\tpublisher = {{ACM}},\n\tauthor = {Abdulrahman Abu Elkhail and Jan Svacina and Tomas Cerny},\n\ttitle = {Intelligent token-based code clone detection system for large scale source code}\n}","authorsSemantic":[1]},{"id":37,"title":"Aspect-oriented challenges in system integration with microservices, SOA and IoT","doi":"10.1080/17517575.2018.1462406","description":"ABSTRACT The era of system integration through service-oriented architecture is rapidly becoming a legacy. New systems increasingly rely on the successor microservice architecture. However, distributed applications can also play a significant role in the internet-of-things. In this paper, we gather evidence demonstrating cross-cutting issues in these areas and highlight how aspect-oriented programming addresses them. Specifically, the purpose of this paper is to provide a roadmap for using aspect-oriented programming to deal effectively with crosscuts in application design, systems interaction, and integration.","venue":"Enterp. Inf. Syst.","listofauthors":"T. Cerný","citations":18,"year":2018,"publisher":"Informa UK Limited","pages":"467-489","volume":"13","number":"4","bibtex":"@article{2018,\n\tdoi = {10.1080/17517575.2018.1462406},\n\turl = {https://doi.org/10.1080%2F17517575.2018.1462406},\n\tyear = 2018,\n\tmonth = {apr},\n\tpublisher = {Informa {UK} Limited},\n\tvolume = {13},\n\tnumber = {4},\n\tpages = {467--489},\n\tauthor = {Tomas Cerny},\n\ttitle = {Aspect-oriented challenges in system integration with microservices, {SOA} and {IoT}}\n}","authorsSemantic":[1]},{"id":38,"title":"On Automated Role-Based Access Control Assessment in Enterprise Systems","doi":"10.1007/978-981-15-1465-4_38","description":"Software system security gets a lot of attention from the industry for its crucial role in protecting private resources. Typically, users access a system’s services via an application programming interface (API). This API must be protected to prevent unauthorized access. One way that developers deal with this challenge is by using role-based access control where each entry point is associated with a set of user roles. However, entry points may use the same methods from lower layers in the application with inconsistent permissions. Currently, developers use integration or penetration testing which demands a lot of effort to test authorization inconsistencies. This paper proposes an automated method to test role-based access control in enterprise applications. Our method verifies inconsistencies within the application using authorization role definitions that are associated with the API entry points. By analyzing the method calls and entity accesses on subsequent layers, inconsistencies across the entire application can be extracted. We demonstrate our solution in a case study and discuss our preliminary results.","venue":"","listofauthors":"Andrew Walker, J. Svacina, J. Simmons, T. Cerný","citations":6,"year":2019,"publisher":"Springer Singapore","pages":"375-385","volume":null,"number":null,"bibtex":"@incollection{2019,\n\tdoi = {10.1007/978-981-15-1465-4_38},\n\turl = {https://doi.org/10.1007%2F978-981-15-1465-4_38},\n\tyear = 2019,\n\tmonth = {dec},\n\tpublisher = {Springer Singapore},\n\tpages = {375--385},\n\tauthor = {Andrew Walker and Jan Svacina and Johnathan Simmons and Tomas Cerny},\n\ttitle = {On Automated Role-Based Access Control Assessment in Enterprise Systems}\n}","authorsSemantic":[1]},{"id":39,"title":"On Limitations of Modern Static Analysis Tools","doi":"10.1007/978-981-15-1465-4_57","description":"Static analysis is one of the most important tools for developers in the modern software industry. However, due to limitations by current tools, many developers opt out of using static analysis in their development process. Some of these limitations include the lack of a concise, coherent overview, missing support for multiple repository applications and multiple languages and lastly a lack of standardized integration mechanisms for third-party frameworks. We propose an evaluation metric for static analysis tools and offer a comparison of many common static analysis tools. To demonstrate the goal of our metric we introduce the Fabric8-Analytics Quality Assurance Tool as a benchmark of a tool which successfully passes our evaluation metric. We demonstrate usage of this tool via a case study on the Fabric8-Analytics Framework, a framework for finding vulnerabilities in application dependencies. We issue a challenge to developers of modern static analysis tools to make their tools more usable and appealing to developers.","venue":"","listofauthors":"Andrew Walker, M. Coffey, Pavel Tisnovsky, T. Cerný","citations":3,"year":2019,"publisher":"Springer Singapore","pages":"577-586","volume":null,"number":null,"bibtex":"@incollection{2019,\n\tdoi = {10.1007/978-981-15-1465-4_57},\n\turl = {https://doi.org/10.1007%2F978-981-15-1465-4_57},\n\tyear = 2019,\n\tmonth = {dec},\n\tpublisher = {Springer Singapore},\n\tpages = {577--586},\n\tauthor = {Andrew Walker and Michael Coffey and Pavel Tisnovsky and Tomas Cerny},\n\ttitle = {On Limitations of Modern Static Analysis Tools}\n}","authorsSemantic":[1]},{"id":41,"title":"On Relating Code Smells to Security Vulnerabilities","doi":"10.1109/BigDataSecurity-HPSC-IDS.2019.00013","description":"In recent years there has been an abundance of well-known software design problems that fall under a variety of different terms such as flaws or code smells. Nowadays software systems place considerable importance to security concerns related to code flaws that lead to software vulnerabilities. In this paper, we present a study to identify the relationship between code smells and vulnerabilities in software code. Our study provides information about the impact of code smells on software security and considers open source implementation of technologies under the Apache Tomcat software. The results show the relationship between the code smells and the security vulnerabilities. While most code smells, have a slight impact, one particular smell is identified to have a more considerable impact.","venue":"2019 IEEE 5th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)","listofauthors":"Abdulrahman Abu Elkhail, T. Cerný","citations":3,"year":2019,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1109/bigdatasecurity-hpsc-ids.2019.00013},\n\turl = {https://doi.org/10.1109%2Fbigdatasecurity-hpsc-ids.2019.00013},\n\tyear = 2019,\n\tmonth = {may},\n\tpublisher = {{IEEE}},\n\tauthor = {Abdulrahman Abu Elkhail and Tomas Cerny},\n\ttitle = {On Relating Code Smells to Security Vulnerabilities}\n}","authorsSemantic":[1]},{"id":42,"title":"On isolation-driven automated module decomposition","doi":"10.1145/3264746.3264756","description":"Contemporary enterprise systems focus primarily on performance and development/maintenance costs. Dealing with cyber-threats and system compromise is relegated to good coding (i.e., defensive programming) and secure environment (e.g., patched OS, firewalls, etc.). This approach, while a necessary start, is not sufficient. Such security relies on no missteps, and compromise only need a single flaw; consequently, we must design for compromise and mitigate its impact. One approach is to utilize fine-grained modularization and isolation. In such a system, decomposition ensures that compromise of a single module presents limited and known risk to data/resource theft and denial. We propose mechanisms for automating such modular composition and consider its system performance impact.","venue":"RACS","listofauthors":"T. Cerný, Filip Sedlisky, M. J. Donahoo","citations":0,"year":2018,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2018,\n\tdoi = {10.1145/3264746.3264756},\n\turl = {https://doi.org/10.1145%2F3264746.3264756},\n\tyear = 2018,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Tomas Cerny and Filip Sedlisky and Michael J. Donahoo},\n\ttitle = {On isolation-driven automated module decomposition}\n}","authorsSemantic":[2,1]},{"id":43,"title":"Survey on Compromise-Defensive System Design","doi":"10.1007/978-981-13-1056-0_51","description":"Conventional enterprise application design methodologies emphasize performance, scalability, and development/maintenance costs. Often such applications deal with access to confidential data (e-commerce, health, etc.). A single flaw in the application may lead to a compromise, exposing computational resources and sensitive data, such as private information, trade secrets, etc. Traditionally, security for enterprise applications focused on prevention; however, recent experience demonstrates that exploitation of infrastructure, operating systems, libraries, frameworks, personnel, etc. are almost unavoidable. While prevention should certainly remain the first line of defense, system architects must also incorporate designs to enable breach containment and response. In this paper, we survey related research on software application design that targets isolation, where the compromise of a single module presents a knowable and scope-limited worst-case impact.","venue":"ICISA","listofauthors":"T. Cerný, M. J. Donahoo","citations":1,"year":2018,"publisher":"Springer Singapore","pages":"513-521","volume":null,"number":null,"bibtex":"@incollection{2018,\n\tdoi = {10.1007/978-981-13-1056-0_51},\n\turl = {https://doi.org/10.1007%2F978-981-13-1056-0_51},\n\tyear = 2018,\n\tmonth = {jul},\n\tpublisher = {Springer Singapore},\n\tpages = {513--521},\n\tauthor = {Tomas Cerny and Michael Jeff Donahoo},\n\ttitle = {Survey on Compromise-Defensive System Design}\n}","authorsSemantic":[2,1]},{"id":44,"title":"Contextual understanding of microservice architecture: current and future directions","doi":"10.1145/3183628.3183631","description":"Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key differences between these two approaches and their features, we can design a more effective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the differences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.","venue":"SIAP","listofauthors":"T. Cerný, M. J. Donahoo, Michal Trnka","citations":74,"year":2018,"publisher":"Association for Computing Machinery (ACM)","pages":"29-45","volume":"17","number":"4","bibtex":"@article{2018,\n\tdoi = {10.1145/3183628.3183631},\n\turl = {https://doi.org/10.1145%2F3183628.3183631},\n\tyear = 2018,\n\tmonth = {jan},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {17},\n\tnumber = {4},\n\tpages = {29--45},\n\tauthor = {Tomas Cerny and Michael J. Donahoo and Michal Trnka},\n\ttitle = {Contextual understanding of microservice architecture}\n}","authorsSemantic":[2,1]},{"id":45,"title":"Pattern Matching Based Sensor Identification Layer for an Android Platform","doi":"10.1155/2018/4734527","description":"As sensor-related technologies have been developed, smartphones obtain more information from internal and external sensors. This interaction accelerates the development of applications in the Internet of Things environment. Due to many attributes that may vary the quality of the IoT system, sensor manufacturers provide their own data format and application even if there is a well-defined standard, such as ISO/IEEE 11073 for personal health devices. In this paper, we propose a client-server-based sensor adaptation layer for an Android platform to improve interoperability among nonstandard sensors. Interoperability is an important quality aspect for the IoT that may have a strong impact on the system especially when the sensors are coming from different sources. Here, the server compares profiles that have clues to identify the sensor device with a data packet stream based on a modified Boyer-Moore-Horspool algorithm. Our matching model considers features of the sensor data packet. To verify the operability, we have implemented a prototype of this proposed system. The evaluation results show that the start and end pattern of the data packet are more efficient when the length of the data packet is longer.","venue":"Wirel. Commun. Mob. Comput.","listofauthors":"Hong Min, Taesik Kim, Junyoung Heo, T. Cerný, S. Sankaran, Bestoun S. Ahmed, Jinmang Jung","citations":0,"year":2018,"publisher":"Hindawi Limited","pages":"1-11","volume":"2018","number":null,"bibtex":"@article{2018,\n\tdoi = {10.1155/2018/4734527},\n\turl = {https://doi.org/10.1155%2F2018%2F4734527},\n\tyear = 2018,\n\tmonth = {oct},\n\tpublisher = {Hindawi Limited},\n\tvolume = {2018},\n\tpages = {1--11},\n\tauthor = {Hong Min and Taesik Kim and Junyoung Heo and Tomas Cerny and Sriram Sankaran and Bestoun S. Ahmed and Jinman Jung},\n\ttitle = {Pattern Matching Based Sensor Identification Layer for an Android Platform}\n}","authorsSemantic":[1]},{"id":46,"title":"Internet of Things: Current Challenges in the Quality Assurance and Testing Methods","doi":"10.1007/978-981-13-1056-0_61","description":"Contemporary development of the Internet of Things (IoT) technology brings a number of challenges in the Quality Assurance area. Current issues related to security, user's privacy, the reliability of the service, interoperability, and integration are discussed. All these create a demand for specific Quality Assurance methodology for the IoT solutions. In the paper, we present the state of the art of this domain and we discuss particular areas of system testing discipline, which is not covered by related work sufficiently so far. This analysis is supported by results of a recent survey we performed among ten IoT solutions providers, covering various areas of IoT applications.","venue":"ICISA","listofauthors":"Miroslav Bures, T. Cerný, Bestoun S. Ahmed","citations":19,"year":2018,"publisher":"Springer Singapore","pages":"625-634","volume":null,"number":null,"bibtex":"@incollection{2018,\n\tdoi = {10.1007/978-981-13-1056-0_61},\n\turl = {https://doi.org/10.1007%2F978-981-13-1056-0_61},\n\tyear = 2018,\n\tmonth = {jul},\n\tpublisher = {Springer Singapore},\n\tpages = {625--634},\n\tauthor = {Miroslav Bures and Tomas Cerny and Bestoun S. Ahmed},\n\ttitle = {Internet of Things: Current Challenges in the Quality Assurance and Testing Methods}\n}","authorsSemantic":[1]},{"id":47,"title":"Aspect oriented context-aware and event-driven data processing for internet of things","doi":"10.1145/3264746.3264761","description":"The Internet of Things is currently getting significant interest from the scientific community. Academia and industry are both focused on moving ahead in attempts to put Internet of Things in practical use. Sensors and other devices in the Internet of Things networks generate tremendous amounts of data. Most of the times those data carry some contextual information and thus could be used for context-aware application. However, handling the vast amount of data becomes increasingly demanding task. In this article we propose event-driven solution for context-aware applications. In our method events are generated by Internet of Things devices and further propagated to subscribed actions. It support event filtering based on the data the event carries with him, like temperature or location. We demonstrate feasibility of our solution and compare it with traditional approach.","venue":"RACS","listofauthors":"Michal Trnka, J. Svacina, T. Cerný, Eunjee Song","citations":0,"year":2018,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2018,\n\tdoi = {10.1145/3264746.3264761},\n\turl = {https://doi.org/10.1145%2F3264746.3264761},\n\tyear = 2018,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Michal Trnka and Jan Svacina and Tomas Cerny and Eunjee Song},\n\ttitle = {Aspect oriented context-aware and event-driven data processing for internet of things}\n}","authorsSemantic":[1,9]},{"id":48,"title":"Survey of Authentication and Authorization for the Internet of Things","doi":"10.1155/2018/4351603","description":"The Internet of Things is currently getting significant interest from the scientific community. Academia and industry are both focused on moving ahead in attempts to enhance usability, maintainability, and security through standardization and development of best practices. We focus on security because of its impact as one of the most limiting factors to wider Internet of Things adoption. Numerous research areas exist in the security domain, ranging from cryptography to network security to identity management. This paper provides a survey of existing research applicable to the Internet of Things environment at the application layer in the areas of identity management, authentication, and authorization. We survey and analyze more than 200 articles, categorize them, and present current trends in the Internet of Things security domain.","venue":"Secur. Commun. Networks","listofauthors":"Michal Trnka, T. Cerný, Nathaniel Stickney","citations":25,"year":2018,"publisher":"Hindawi Limited","pages":"1-17","volume":"2018","number":null,"bibtex":"@article{2018,\n\tdoi = {10.1155/2018/4351603},\n\turl = {https://doi.org/10.1155%2F2018%2F4351603},\n\tyear = 2018,\n\tmonth = {jun},\n\tpublisher = {Hindawi Limited},\n\tvolume = {2018},\n\tpages = {1--17},\n\tauthor = {Michal Trnka and Tomas Cerny and Nathaniel Stickney},\n\ttitle = {Survey of Authentication and Authorization for the Internet of Things}\n}","authorsSemantic":[1]},{"id":49,"title":"Degree of Similarity of Root Trees","doi":"10.1007/978-981-13-1056-0_57","description":"Adaptive User Interfaces (UI) provide better user experience as users a receive personalized presentation. These UIs heavily rely on contextual data. Context helps the application to recognize user needs and thus adjust the UI. First time user receives a generalized experience; however, as the user uses the application more often it gathers lots of contextual data, such as the history of actions. This allows to statistically classify user in a user cluster and based on that adapt the UI presentation. This paper considers methods to find a measure of similarity of graphs to support adaptive UIs. To achieve this, it considers rooted trees. It states known approaches, which could be used for calculation of this measure. It focuses on the Simhash algorithm and describes its implementation in the SimCom experimental comparative application. Its results show that Simhash can be used for comparing the rooted trees. The main aim of this paper is to show novel view on how to use graph algorithms and clustering of trees into adaptive application structure.","venue":"ICISA","listofauthors":"Jirí Sebek, Petr Vondrus, T. Cerný","citations":0,"year":2018,"publisher":"Springer Singapore","pages":"581-591","volume":null,"number":null,"bibtex":"@incollection{2018,\n\tdoi = {10.1007/978-981-13-1056-0_57},\n\turl = {https://doi.org/10.1007%2F978-981-13-1056-0_57},\n\tyear = 2018,\n\tmonth = {jul},\n\tpublisher = {Springer Singapore},\n\tpages = {581--591},\n\tauthor = {Jiri Sebek and Petr Vondrus and Tomas Cerny},\n\ttitle = {Degree of Similarity of Root Trees}\n}","authorsSemantic":[1]},{"id":50,"title":"Security Challenges in Smart City Applications","doi":null,"description":"Recent technological innovations have resulted in a deployment of a large number of interconnected IoT devices for augmenting personal lifestyle. Urbanization is happening at a remarkable rate, and it is estimated to reach 5 billion population by 2030, likely resulting in dense clustering of these devices. Such proliferation requires advanced management approaches using the latest IT platforms and techniques for adapting city-related services to this new expectation. Such emerging integration of technologies like ubiquitous sensing, heterogeneous network infrastructure, and intelligent Information processing and control systems faces several security challenges. Our research focuses on providing an overview based on research efforts in smart cities addressing these security challenges and solutions for smart transportation, living, healthcare, and energy. We highlight the effects of malicious attacks and their countermeasures while addressing discrimination from false positives such a Flash Event (FE). Finally, we discuss future research directions and limitations.","venue":"","listofauthors":"Safwan Mawlood Hussein, M. J. Donahoo, T. Cerný","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[1]},{"id":51,"title":"Using Wi-Fi Enabled Internet of Things Devices for Context-Aware Authentication","doi":"10.1007/978-981-13-1056-0_62","description":"The increasing spread and adoption of the Internet of Things allows for novel methods to gather information about a user’s context, which can be used for enhanced authentication. In this article, we focus on context-aware authentication using information about Wi-Fi networks from a user’s wearables or nearables. We propose an additional factor for multi-factor authentication based on the other devices present on the same Wi-Fi network. Devices periodically discover all available peer MAC addresses. During subsequent authentication attempts, the network state is compared to previous network states saved under functionally similar conditions. If the devices on the network change significantly, a flag is raised and further action can be triggered. We also demonstrate the solution as a proof of concept.","venue":"ICISA","listofauthors":"Michal Trnka, Filip Rysavy, T. Cerný, Nathaniel Stickney","citations":4,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n<title>Error: DOI Not Found</title>\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n\n<link rel=\"icon\" href=\"/static/img/favicon.png\" />\n<link rel=\"shortcut icon\" href=\"/static/favicon.ico\" type=\"image/x-icon\" /> \n<link href=\"/static/style/new-style2.css\" rel=\"stylesheet\" type=\"text/css\" />\n</head>\n\n<body>\n\n\n<div style=\"background:#fcb426\">\n<img src=\"/static/img/banner-413.gif\" alt=\"Logo\" width=\"620\" height=\"137\" border=\"0\" />\n</div>\n\n<div style=\"height:1px;background:#000000\"></div>\n<div style=\"height:1px;background:#54524f\"></div>\n<div style=\"height:1px;background:#f6911e\"></div>\n\n\n<!-- TABLE FOR NAVIGATION BAR -->\n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" id=\"navtable\" align=\"center\">\n<tr>\n    <td width=\"34\" height=\"26\" bgcolor=\"#231f20\"><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"1\" height=\"1\" /></td>\n    \n    <td height=\"26\" bgcolor=\"#231f20\" class=\"navtext\">\n    <a href=\"http://www.doi.org/index.html\">HOME</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/hb.html\">HANDBOOK</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/factsheets.html\">FACTSHEETS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/faq.html\">FAQs</a> &nbsp;|&nbsp; <a href=\"http://www.doi.org/resources.html\">RESOURCES</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/users.html\">USERS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/announce.html\">NEWS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/idf-members/index.html\">MEMBERS AREA</a>\n    </td>    \n  </tr>\n</table>\n<!-- END TABLE FOR NAVIGATION BAR -->\n\n<div style=\"height:1px;background:#e3a44d\"></div>\n<div style=\"height:3px;background:#4d4942\"></div>\n\n\n\n<!-- TABLE FOR CONTENT -->      \n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" bgcolor=\"#ffffff\">\n<tr>\n<td colspan=\"6\">\n<img src=\"/static/img/transparent.gif\" alt=\"\" width=\"100\" height=\"20\" border=\"0\" />\n</td>\n</tr>\n\n<tr>\n\n<td valign=\"top\">\n\n<h2>DOI Not Found</h2>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n<h3>10.1007/978-981-13-1056-0_62</h3>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n\n<p>This DOI cannot be found in the DOI System.  Possible reasons are:</p>\n\n\n<ul>\n\n<li style=\"padding-bottom: .5em;\">The DOI is incorrect in your source. Search for the item by name, title, or other metadata using a search engine.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI was copied incorrectly. Check to see that the string includes all the characters before and after the slash and no sentence punctuation marks.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI has not been activated yet.  Please try again later, and report the problem if the error continues.</li>\n\n</ul>\n\n\n\n<div class=\"divider\">&nbsp;</div>\n\n<p>You may report this error to the responsible DOI Registration Agency using the form below.  Include your email address to receive confirmation and feedback.</p>\n\n<div style=\"padding-left: 4em;\">\n\n<form action=\"/notfound\" method=\"post\" enctype=\"application/x-www-form-urlencoded\" name=\"notFoundForm\">\n\n\n<table border=\"0\" cellspacing=\"3\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>\n\n<table border=\"0\" align=\"center\" cellpadding=\"3\" cellspacing=\"3\">\n<tbody><tr>\n<th  align=\"right\" scope=\"row\"><label>DOI:</label></th>\n<td><input name=\"missingHandle\" type=\"text\" value=\"10.1007/978-981-13-1056-0_62\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\"><label>URL of Web Page Listing the DOI:</label></th>\n<td><input name=\"referringPage\" type=\"text\" value=\"\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\">Your Email Address:</th>\n<td><input name=\"userEmailAddress\" type=\"text\" value=\"Please enter your email address\" size=\"42\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\" valign=\"top\">Additional Information About the Error:</th>\n<td><textarea name=\"comments\" cols=\"30\" rows=\"6\"></textarea></td>\n</tr>\n</tbody>\n</table>\n\n</td>\n</tr>\n<tr>\n\n<td align=\"right\"><p><input name=\"send\" type=\"submit\" value=\"Submit Error Report\" /></p></td>\n</tr>\n</tbody>\n</table>\n\n</form>\n</div>\n\n\n\n\n</td>\n<td><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"20\" height=\"20\" border=\"0\" /></td>\n</tr>\n</table>\n\n<div class=\"divider-full\">&nbsp;</div>\n\n<!-- TABLE FOR FOOTER -->\n\n<table  border=\"0\" cellpadding=\"0\" cellspacing=\"0\" align=\"center\">\n\n<tr>\n<td align=\"center\" colspan=\"2\">\n<a href=\"/help.html\">DOI Resolution Documentation</a>\n</td>\n</tr>\n\n<tr>\n<td align=\"left\" height=\"40\">\n<img src=\"/static/img/Logo_TM.png\" alt=\"DOI_disc_logo\" width=\"24\" height=\"24\" />\n</td>\n\n<td align=\"left\">\n<span style=\"padding-left: 0px; font-size: 11px;\"><span style=\"vertical-align: super;\">&reg;</span>, DOI<span style=\"vertical-align: super;\">&reg;</span>, DOI.ORG<span style=\"vertical-align: super;\">&reg;</span>, and shortDOI<span style=\"vertical-align: super;\">&reg;</span> are trademarks of the International DOI Foundation.</span>\n</td>       \n</tr>\n</table>\n</body>\n</html>\n","authorsSemantic":[1]},{"id":52,"title":"Second Screen Engagement of Event Spectators","doi":"10.1155/2018/3845123","description":"An effective means of engaging spectators at live events involves providing real-time information from a variety of sources. Consumers demand personalized experience; thus, a single channel perspective fails. Modern entertainment must extend to spectator mobile devices and adapt content to individual interests. Moreover, such systems should take advantage of venue screens to engage in sharing live information, aggregated social media, etc. We propose a second screen application, providing each audience member a personalized perspective, involving mobile devices equipped with Wi-Fi, and spanning to venue screens in hotels, halls, arenas, elevators, etc. Such a system engages both local audience and remote spectators. Our work provides a case study involving experience from the deployment of such an application at the ACM-ICPC World Finals with audiences at the event and around the world. We analyze and categorize its features, consider its impact on the audience, and measure its demands.","venue":"Adv. Hum. Comput. Interact.","listofauthors":"T. Cerný, M. J. Donahoo","citations":4,"year":2018,"publisher":"Hindawi Limited","pages":"1-20","volume":"2018","number":null,"bibtex":"@article{2018,\n\tdoi = {10.1155/2018/3845123},\n\turl = {https://doi.org/10.1155%2F2018%2F3845123},\n\tyear = 2018,\n\tmonth = {jul},\n\tpublisher = {Hindawi Limited},\n\tvolume = {2018},\n\tpages = {1--20},\n\tauthor = {Tomas Cerny and Michael Jeff Donahoo},\n\ttitle = {Second Screen Engagement of Event Spectators}\n}","authorsSemantic":[2,1]},{"id":53,"title":"Aspect-driven context-aware services","doi":"10.15439/2017F397","description":"Nowadays enterprise software solutions must deal with ever-growing complexity and a multitude of business processes. The mainstream system design decomposes the system into small reusable services. While these services isolate certain system logic and address efficient elasticity towards growing user demands, there are multiple issues related to such a design, such as limitations to deal with restated information, information reuse or the ability to address cross-cutting concerns across multiple services. This paper highlights limitations of service-oriented architecture and proposes an alternative decomposition through aspect-driven service-oriented architecture. Such architecture involves adaptive, context-aware services preserving simple maintenance while addressing information reuse and crosscuts across services. The paper provides a formal description of the proposed architecture as well as a demonstration through a case study, showing approach properties and benefits.","venue":"2017 Federated Conference on Computer Science and Information Systems (FedCSIS)","listofauthors":"Karel Cemus, Filip Klimes, T. Cerný","citations":2,"year":2017,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2017,\n\tdoi = {10.15439/2017f397},\n\turl = {https://doi.org/10.15439%2F2017f397},\n\tyear = 2017,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Karel Cemus and Filip Klimes and Tomas Cerny},\n\ttitle = {Aspect-driven Context-aware Services}\n}","authorsSemantic":[1]},{"id":54,"title":"Automated extraction of business documentation in enterprise information systems","doi":"10.1145/3040575.3040576","description":"Extracting business documentation from ever-evolving information systems is a challenging task that requires a lot of effort, focus and technical expertise. Such a documentation provides a detailed overview of the system, lists domain model, operations, pre and post-conditions that impact the business flow. While all this information is captured by a particular software system, domain experts interested in such knowledge have limited access to it, either due to the lack of technical and programming expertise, or because the knowledge tangles throughout the entire system.\n This paper introduces a novel approach in aspect-driven information systems enabling automated extraction of business documentation and its transformation to various perspectives allowing domain experts to acquire needed and up-to-date information and knowledge. Moreover, our approach makes it possible to transform system business rules into a formal language description verifiable by checkers validating the feasibility of business operations. We demonstrate the approach in a case study indicating its benefits.","venue":"SIAP","listofauthors":"Karel Cemus, T. Cerný","citations":1,"year":2017,"publisher":"Association for Computing Machinery (ACM)","pages":"5-13","volume":"16","number":"4","bibtex":"@article{2017,\n\tdoi = {10.1145/3040575.3040576},\n\turl = {https://doi.org/10.1145%2F3040575.3040576},\n\tyear = 2017,\n\tmonth = {jan},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {16},\n\tnumber = {4},\n\tpages = {5--13},\n\tauthor = {Karel Cemus and Tomas Cerny},\n\ttitle = {Automated extraction of business documentation in enterprise information systems}\n}","authorsSemantic":[1]},{"id":55,"title":"Separation of concerns for distributed cross-platform context-aware user interfaces","doi":"10.1007/s10586-017-0794-7","description":"Modern applications aim to provide attractive, efficient and adaptive user interfaces (UIs). The UI code developed in conventional design approaches brings numerous of difficulties exacerbating the development and maintenance efforts resulting in limited separation of concerns. The limitation comes from multiple cross-cuts of tangled concerns, e.g. data representation tangled with layout, security, business rules, localization, etc. This results in high information restatement, code duplication, and tedious maintenance. This paper introduces an approach that separates UI concerns bringing a single focal point for particular concern definitions improving readability and maintenance. The approach performs concern tangling at runtime through a weaver considering the contextual information influencing the result. This enables UI context-awareness, while significantly reduces the development and maintenance efforts. Furthermore, we apply this approach into a distributed environment, which allows us to construct the same UI on various platforms and devices involving a single concerns description streamed from the server.","venue":"Cluster Computing","listofauthors":"Karel Cemus, Filip Klimes, O. Kratochvil, T. Cerný","citations":6,"year":2017,"publisher":"Springer Science and Business Media LLC","pages":"2355-2362","volume":"20","number":"3","bibtex":"@article{2017,\n\tdoi = {10.1007/s10586-017-0794-7},\n\turl = {https://doi.org/10.1007%2Fs10586-017-0794-7},\n\tyear = 2017,\n\tmonth = {mar},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {20},\n\tnumber = {3},\n\tpages = {2355--2362},\n\tauthor = {Karel Cemus and Filip Klimes and Ondrej Kratochvil and Tomas Cerny},\n\ttitle = {Separation of concerns for distributed cross-platform context-aware user interfaces}\n}","authorsSemantic":[1]},{"id":56,"title":"Context-Aware Security Using Internet of Things Devices","doi":"10.1007/978-981-10-4154-9_81","description":"Current trends aim to extend software applications with context-awareness. Nowadays, there are already various approaches enabling security based on context, unfortunately there have limitations. However, the challenging topic is how to obtain as much context information about user as possible. Current progress in Internet of Things domain could be leveraged to obtain more context data. We propose a method to formalize context based on Internet of Things devices and use it for application context-aware security. Our approach is based on composition of a tree topology correlating to the user’s devices for recurring situations. Based on changes in the tree we determine unusual behavior, trigger events or invoke specific actions.","venue":"ICISA","listofauthors":"Michal Trnka, M. Tomásek, T. Cerný","citations":0,"year":2017,"publisher":"Springer Singapore","pages":"706-713","volume":null,"number":null,"bibtex":"@incollection{2017,\n\tdoi = {10.1007/978-981-10-4154-9_81},\n\turl = {https://doi.org/10.1007%2F978-981-10-4154-9_81},\n\tyear = 2017,\n\tpublisher = {Springer Singapore},\n\tpages = {706--713},\n\tauthor = {Michal Trnka and Martin Tomasek and Tomas Cerny},\n\ttitle = {Context-Aware Security Using Internet of Things Devices}\n}","authorsSemantic":[1]},{"id":57,"title":"Prioritized Process Test: More Efficiency in Testing of Business Processes and Workflows","doi":"10.1007/978-981-10-4154-9_67","description":"Testing business processes and workflows in information systems, while aiming to cover all possible paths, requires high efforts demanding considerable costs. In this paper, we propose an algorithm generating a path-based test cases from the system model, based on weighted directed graph. The approach brings an alternative to the currently established test requirements concept. The algorithm reflects various levels of priorities of particular functions in the tested system, previously defined by the test designer. When compared to simulated naive approaches based on reverse reduction of test set, our proposed algorithm produces more efficient test cases in terms of number of the total test steps, whilst keeping the same level of test coverage of the priority functions of the tested system.","venue":"ICISA","listofauthors":"Miroslav Bures, T. Cerný, Matej Klima","citations":9,"year":2017,"publisher":"Springer Singapore","pages":"585-593","volume":null,"number":null,"bibtex":"@incollection{2017,\n\tdoi = {10.1007/978-981-10-4154-9_67},\n\turl = {https://doi.org/10.1007%2F978-981-10-4154-9_67},\n\tyear = 2017,\n\tpublisher = {Springer Singapore},\n\tpages = {585--593},\n\tauthor = {Miroslav Bures and Tomas Cerny and Matej Klima},\n\ttitle = {Prioritized Process Test: More Efficiency in Testing of Business Processes and Workflows}\n}","authorsSemantic":[1]},{"id":58,"title":"Static Testing Using Different Types of CRUD Matrices","doi":"10.1007/978-981-10-4154-9_68","description":"Static testing leads to early detection of defects throughout a project software development. This results in reduced costs and risks in the development process. Various types of static tests can be performed. In this paper, we propose extensions to contemporary static testing techniques based on CRUD matrices. In particular, we consider cross-verification between various types of CRUD matrices made by different parties at different stages of the project. This leads into extended the verification consistency of a CRUD matrix. In our evaluation, proposed techniques lead to significantly more consistent test Data Cycle Test cases, when involving our static testing techniques. Moreover, our results indicate positive impact on lowering the number of defects that usually remain undetected under the system test.","venue":"ICISA","listofauthors":"Miroslav Bures, T. Cerný","citations":0,"year":2017,"publisher":"Springer Singapore","pages":"594-602","volume":null,"number":null,"bibtex":"@incollection{2017,\n\tdoi = {10.1007/978-981-10-4154-9_68},\n\turl = {https://doi.org/10.1007%2F978-981-10-4154-9_68},\n\tyear = 2017,\n\tpublisher = {Springer Singapore},\n\tpages = {594--602},\n\tauthor = {Miroslav Bures and Tomas Cerny},\n\ttitle = {Static Testing Using Different Types of {CRUD} Matrices}\n}","authorsSemantic":[1]},{"id":59,"title":"Testing the consistency of business data objects using extended static testing of CRUD matrices","doi":"10.1007/s10586-017-1118-7","description":"Static testing is used to detect software defects in the earlier phases of the software development lifecycle, which makes the total costs caused by defects lower and the software development project less risky. Different types of static testing have been introduced and are used in software projects. In this paper, we focus on static testing related to data consistency in a software system. In particular, we propose extensions to contemporary static testing techniques based on CRUD matrices, employing cross-verifications between various types of CRUD matrices made by different parties at various stages of the software project. Based on performed experiments, the proposed static testing technique significantly improves the consistency of Data Cycle Test cases. Together with this trend, we observe growing potential of test cases to detect data consistency defects in the system under test, when utilizing the proposed technique.","venue":"Cluster Computing","listofauthors":"Miroslav Bures, T. Cerný, Karel Frajták, Bestoun S. Ahmed","citations":2,"year":2017,"publisher":"Springer Science and Business Media LLC","pages":"963-976","volume":"22","number":"S1","bibtex":"@article{2017,\n\tdoi = {10.1007/s10586-017-1118-7},\n\turl = {https://doi.org/10.1007%2Fs10586-017-1118-7},\n\tyear = 2017,\n\tmonth = {aug},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {22},\n\tnumber = {S1},\n\tpages = {963--976},\n\tauthor = {Miroslav Bures and Tomas Cerny and Karel Frajtak and Bestoun S. Ahmed},\n\ttitle = {Testing the consistency of business data objects using extended static testing of {CRUD} matrices}\n}","authorsSemantic":[1]},{"id":60,"title":"Disambiguation and Comparison of SOA, Microservices and Self-Contained Systems","doi":"10.1145/3129676.3129682","description":"There is an industrial shift from Service-Oriented Architectures (SOA) into Microservices; however, a quick review of online resources on these topics reveals a range of different understandings of these two architectures. Individuals often mix terms, grant false advantages or expect different quality attributes and properties. The purpose of this paper is to provide readers a solid understanding of the differences between these two architectures and their features. We provide both research and industry perspectives to point out strengths and weaknesses of both architectural directions, and we point out many shortcomings in both approaches that are not addressed by the architecture. Finally, based on this we propose challenges for future research.","venue":"RACS","listofauthors":"T. Cerný, M. J. Donahoo, J. Pechanec","citations":29,"year":2017,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2017,\n\tdoi = {10.1145/3129676.3129682},\n\turl = {https://doi.org/10.1145%2F3129676.3129682},\n\tyear = 2017,\n\tmonth = {sep},\n\tpublisher = {{ACM}},\n\tauthor = {Tomas Cerny and Michael J. Donahoo and Jiri Pechanec},\n\ttitle = {Disambiguation and Comparison of {SOA}, Microservices and Self-Contained Systems}\n}","authorsSemantic":[2,1]},{"id":61,"title":"Automated User Interface Generation Involving Field Classification","doi":"10.13052/JSN2445-9739.2017.004","description":"Software applications are designed with a concrete purpose in mind, specified by business owners basing on the individual requirements. The user-system interaction is specified in the analysis phase. This phase specifies inputs, outputs, interaction, etc. These elements could be specified separately based on the target platform. The designers know that the mobile clients could have a different application flow than desktop clients. However, this is not a rule and designer rarely think about the user context or application. This implies that outputs, inputs and interactions do not change automatically during the software lifecycle. In this paper we present techniques that can determine whether the user, which is in a particular context, should be required to spend his/her time to fill in fields that are not needed for accomplishing a specific business task. Moreover, these techniques are able to determine whether the fields should display or not, as well as how the system might interact with the user. Next, we present a computational architecture capable of these types of determinations and we demonstrate this technique in case study. Finally, we show how automatically combine user interface generation with our approach.","venue":"","listofauthors":"M. Tomásek, T. Cerný","citations":2,"year":2017,"publisher":"River Publishers","pages":"53-78","volume":"2017","number":"1","bibtex":"@article{2017,\n\tdoi = {10.13052/jsn2445-9739.2017.004},\n\turl = {https://doi.org/10.13052%2Fjsn2445-9739.2017.004},\n\tyear = 2017,\n\tpublisher = {River Publishers},\n\tvolume = {2017},\n\tnumber = {1},\n\tpages = {53--78},\n\tauthor = {Matrtin Tomasek and Tomas Cerny and   and},\n\ttitle = {Automated User Interface Generation Involving Field Classification}\n}","authorsSemantic":[1]},{"id":62,"title":"Aspect, Rich, and Anemic Domain Models in Enterprise Information Systems","doi":"10.1007/978-3-662-49192-8_36","description":"The research shows that maintenance of enterprise information systems consumes about 65---75i¾?% of the software development time and about 40---60i¾?% of maintenance efforts are devoted to software understanding. This paper compares the Anemic Domain Model used by the three-layered architecture followed by Java EE and .NET platforms and the Rich Domain Model often deployed into many conventional MVC-like web frameworks to a novel Aspect Domain Model followed by the Aspect-driven design. While all these models strive to avoid information restatement, they greatly differ in the underlying idea and resulting efficiency. This research compares considered models based on development efficacy, maintainability and their impact on the rest of the system. We evaluate qualities such as information cohesion, coupling and restatement, and discuss related maintenance efforts of the novel approach in the context of existing approaches.","venue":"SOFSEM","listofauthors":"Karel Cemus, T. Cerný, Lubos Matl, M. J. Donahoo","citations":3,"year":2016,"publisher":"Springer Berlin Heidelberg","pages":"445-456","volume":null,"number":null,"bibtex":"@incollection{2016,\n\tdoi = {10.1007/978-3-662-49192-8_36},\n\turl = {https://doi.org/10.1007%2F978-3-662-49192-8_36},\n\tyear = 2016,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {445--456},\n\tauthor = {Karel Cemus and Tomas Cerny and Lubos Matl and Michael J. Donahoo},\n\ttitle = {Aspect, Rich, and Anemic Domain Models in Enterprise Information Systems}\n}","authorsSemantic":[2,1]},{"id":63,"title":"Survey on Second Screen Systems","doi":"10.1109/ICITCS.2016.7740374","description":"Today's consumers demand personalized experience for live events. The standard, single-for-all entertainment perspective fails to satisfy consumer expectations on broadcast of real-time information from various sources. Future entertainment environments should presume spectators equipped with mobile devices that can be involved in adapting to the entertainment and personal interests. This article surveys solutions with the ability to provide personalized perspectives to the audience. Such second screen applications may involve use of variety of devices, such as mobile phones equipped with WiFi, venue screens in hotel rooms, meeting halls, arenas, elevators, etc. Furthermore, functionality of such applications is considered regarding to interaction among with the remote spectators and the event.","venue":"2016 6th International Conference on IT Convergence and Security (ICITCS)","listofauthors":"T. Cerný, M. J. Donahoo","citations":1,"year":2016,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1109/icitcs.2016.7740374},\n\turl = {https://doi.org/10.1109%2Ficitcs.2016.7740374},\n\tyear = 2016,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Tomas Cerny and Michael Donahoo},\n\ttitle = {Survey on Second Screen Systems}\n}","authorsSemantic":[2,1]},{"id":64,"title":"On energy impact of web user interface approaches","doi":"10.1007/s10586-016-0665-7","description":"Developers base selection of a User Interface (UI) development approach on functionality, development and maintenance costs, usability, responsiveness, etc. User expectations continue to grow for greater functionality and continuous interactivity, extending demands on computational resources. To facility scale, recent approaches push more UI computation to clients. Such client-side delegation of functionality increase, continuous usage, and localized computation create ever-growing energy demands, which may negatively impact battery life on mobile platforms. Nonetheless, developers given little attention to the power demands aspects of UI framework selection. We evaluate the impact of contemporary UI framework selection on resource utilization and energy consumption. We suggest an alternative delivery approach designed to preserve low energy demands on clients while still allowing offloading of computation from server to client. Our work focuses on web-based mobile applications; however, we believe our approach to energy demand reduction and framework evaluation to be generally applicable.","venue":"Cluster Computing","listofauthors":"T. Cerný, M. J. Donahoo","citations":6,"year":2016,"publisher":"Springer Science and Business Media LLC","pages":"1853-1863","volume":"19","number":"4","bibtex":"@article{2016,\n\tdoi = {10.1007/s10586-016-0665-7},\n\turl = {https://doi.org/10.1007%2Fs10586-016-0665-7},\n\tyear = 2016,\n\tmonth = {oct},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {19},\n\tnumber = {4},\n\tpages = {1853--1863},\n\tauthor = {Tomas Cerny and Michael Jeff Donahoo},\n\ttitle = {On energy impact of web user interface approaches}\n}","authorsSemantic":[2,1]},{"id":65,"title":"Distributed Multi-Platform Context-Aware User Interface for Information Systems","doi":"10.1109/ICITCS.2016.7740327","description":"User Interface (UI) of Enterprise Information Systems deals with various concerns such as the model structure, layouts, widgets, business rules, and localization tangled together. Unfortunately, conventional development approaches fail to efficiently maintain these concerns and forces developers to manually tangle them together. That results in high information restatement, code duplication, and error-prone maintenance. In this paper, we apply Aspect-Driven Design Approach to separate the concerns in UI, and have them automatically tangled together at runtime. Such separation introduces the single point to maintain, and the runtime weaving enables us to deliver context-aware UI. Furthermore, we apply this concept into distributed environment. This allows us to construct UI in various platforms on multiple client devices from the single concerns description streamed from the server.","venue":"2016 6th International Conference on IT Convergence and Security (ICITCS)","listofauthors":"Karel Cemus, Filip Klimes, O. Kratochvil, T. Cerný","citations":1,"year":2016,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1109/icitcs.2016.7740327},\n\turl = {https://doi.org/10.1109%2Ficitcs.2016.7740327},\n\tyear = 2016,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Karel Cemus and Filip Klimes and Ondrej Kratochvil and Tomas Cerny},\n\ttitle = {Distributed Multi-Platform Context-Aware User Interface for Information Systems}\n}","authorsSemantic":[1]},{"id":66,"title":"Session details: Volume II: Software design and development, and system software and security: Enterprise application development and design track","doi":"10.1145/3252780","description":"null","venue":"","listofauthors":"T. Cerný","citations":0,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/3252780},\n\turl = {https://doi.org/10.1145%2F3252780},\n\tyear = 2016,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\tauthor = {Tomas Cerny},\n\ttitle = {Session details: Volume {II}: Software design and development, and system software and security: Enterprise application development and design track}\n}","authorsSemantic":[1]},{"id":67,"title":"AOP-Based Approach for Local Data Management in Adaptive Interfaces","doi":"10.1109/ICITCS.2016.7740313","description":"This paper explores context-aware computing and emphasizes on utilizing context for adaptive user interfaces. It proposes an approach for adaptive user interfaces design involving the context of hardware elements of devices on the Android mobile platform. It proposes a concern separating style of aspect-oriented programming, where the main aspect module provides developers with access to the context of device sensors to adapt individual application aspects. Such an approach brings novel mechanisms to design advanced aspects and enable automatic derivation of adaptive user interfaces at runtime.","venue":"2016 6th International Conference on IT Convergence and Security (ICITCS)","listofauthors":"Jirí Sebek, T. Cerný","citations":2,"year":2016,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1109/icitcs.2016.7740313},\n\turl = {https://doi.org/10.1109%2Ficitcs.2016.7740313},\n\tyear = 2016,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Jiri Sebek and Tomas Cerny},\n\ttitle = {{AOP}-Based Approach for Local Data Management in Adaptive Interfaces}\n}","authorsSemantic":[1]},{"id":69,"title":"On security level usage in context-aware role-based access control","doi":"10.1145/2851613.2851664","description":"Huge contemporary trend is adding context awareness into software applications. It allows both better user experience as well as a lot useful features for application owner. Nowadays, there are various approaches enabling particular context awareness but none of them concerns security. We tackle this problem and describe it further in the paper. Our solution extends role based access control with certain context awareness elements. Based on already existing solutions we propose own lightweight, universal solutions, which allows instant enhancement of current RBAC even in existing applications. The uniqueness of our solution is based on using security levels, which are granted to user based on his context. Security levels represents how the users can be trusted and are determined during users login procedure. The levels are used as additional security constrain so to access resources in application user need to have not only right permission granted through roles, but also to have corresponding level.","venue":"SAC","listofauthors":"Michal Trnka, T. Cerný","citations":22,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/2851613.2851664},\n\turl = {https://doi.org/10.1145%2F2851613.2851664},\n\tyear = 2016,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\tauthor = {Michal Trnka and Tomas Cerny},\n\ttitle = {On security level usage in context-aware role-based access control}\n}","authorsSemantic":[1]},{"id":70,"title":"Business Documentation Derivation from Aspect-driven Enterprise Information Systems","doi":"10.1145/2987386.2987402","description":"Business documentation provides important overview of the system. It lists implemented business operations and their preconditions and post-conditions, as well as the architecture of the domain model. Such information is valuable for domain experts, who are able to review it, validate the conditions, and help to address errors in early phases of development. However, it is challenging to derive this documentation from existing systems as the business rules are tangled into source code. In this paper, we introduce a novel technique to business documentation derivation from aspect-driven enterprise information systems. These systems isolate business rules in a single focal point and we demonstrate its transformation into business documentation. In the combination with code inspection, we deliver reliable and easy to generate documentation significantly simplifying involvement of domain experts into development. Furthermore, we show the transformation of business rules into a formal language verifiable by a checker to validate the feasibility of business operations.","venue":"RACS","listofauthors":"Karel Cemus, T. Cerný","citations":0,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/2987386.2987402},\n\turl = {https://doi.org/10.1145%2F2987386.2987402},\n\tyear = 2016,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Karel Cemus and Tomas Cerny},\n\ttitle = {Business Documentation Derivation from Aspect-driven Enterprise Information Systems}\n}","authorsSemantic":[1]},{"id":71,"title":"Survey on Concern Separation in Service Integration","doi":"10.1007/978-3-662-49192-8_42","description":"Ever-changing business processes in large software systems, integration of heterogeneous data sources as well as the desire for legacy service integration drive software design towards reusable, platform-independent, web-accessible microservices. Such independently deployable services provide an interface for retrieval and data manipulation in machine-readable formats. While this approach brings many advantages from the perspective of service integration aiming to separate data manipulation from business processing, the standard approaches provide only limited structural semantics and constraints provided through the interface. This leads to considerable information restatement and repeated decisions in integrating components, which considerably impacts development and maintenance efforts. Integration component operability becomes highly sensitive to interaction with underlying services, which are possibly composed of other services. The sensitivity is especially apparent in the structural semantics of produced and consumed information that must correlate on both sides of the interaction. This paper surveys service integration from the perspective of separation of concerns. In order to reduce the coupling and information restatement on the integration component side, it suggests introducing multiple communication channels with additional information that apply in the service interaction, extending the integration component's ability to derive service expected information structural semantics, constraints or business rules. Finally, we consider the impact of this new approach from the development and maintenance perspectives.","venue":"SOFSEM","listofauthors":"T. Cerný, M. J. Donahoo","citations":6,"year":2016,"publisher":"Springer Berlin Heidelberg","pages":"518-531","volume":null,"number":null,"bibtex":"@incollection{2016,\n\tdoi = {10.1007/978-3-662-49192-8_42},\n\turl = {https://doi.org/10.1007%2F978-3-662-49192-8_42},\n\tyear = 2016,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {518--531},\n\tauthor = {Tomas Cerny and Michael J. Donahoo},\n\ttitle = {Survey on Concern Separation in Service Integration}\n}","authorsSemantic":[2,1]},{"id":72,"title":"Towards Shared Security through Distributed Separation of Concerns","doi":"10.1145/2987386.2987394","description":"When considering distributed enterprise applications interacting with data, one can rarely omit its security concerns that must enforce data integrity and prevent users from disallowed actions. Current trends of application design tend to deal with permissions internally in each particular interacting application, although certain knowledge from the consumer application perspective is needed. This unfortunately leads to restated knowledge that fails to correlate in time, once a particular interacting application evolves or changes. This paper, considers how an convenient security sharing should look like in distributed enterprise system. Next, it puts the ideal case next to the context of existing approaches, which it surveys.","venue":"RACS","listofauthors":"T. Cerný, Michal Trnka, M. J. Donahoo","citations":2,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/2987386.2987394},\n\turl = {https://doi.org/10.1145%2F2987386.2987394},\n\tyear = 2016,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Tomas Cerny and Michal Trnka and Michael J. Donahoo},\n\ttitle = {Towards Shared Security through Distributed Separation of Concerns}\n}","authorsSemantic":[2,1]},{"id":73,"title":"Identity Management of Devices in Internet of Things Environment","doi":"10.1109/ICITCS.2016.7740343","description":"Significant interest in internet of things drives both research and industry production these days. A lot of important questions has been solved but some remain opened. One of the essential unresolved issue is the identity management of single devices. This paper proposes solution for management of devices for internet of things. The solution is based on central identity store. Each device has an associated account in the store with corresponding roles. For any communication the device retrieves OAuth 2.0 token and uses it to certify itself in every network connection. The proposed framework creates trusted environment and enables rapid response for any security events.","venue":"2016 6th International Conference on IT Convergence and Security (ICITCS)","listofauthors":"Michal Trnka, T. Cerný","citations":16,"year":2016,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1109/icitcs.2016.7740343},\n\turl = {https://doi.org/10.1109%2Ficitcs.2016.7740343},\n\tyear = 2016,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Michal Trnka and Tomas Cerny},\n\ttitle = {Identity Management of Devices in Internet of Things Environment}\n}","authorsSemantic":[1]},{"id":309,"title":"Deduction by Induction","doi":"10.4135/9781526435897.n9","description":"Introduction I was amazed when I read the article “What Do You Get When You Cross a Power Sum with an Iraqi Bank Note” by Professor Boudreaux in the October 2009 Magazine[1]. Of course I also had to take a peek at a couple of the references[2,3], and I found them amazing as well. All these approaches to finding closed-form formulas for power sums are complex, wonderful, and truly impressive. However, I disagree with Professor Boudreaux about one thing. Not all approaches to power sums can be categorized as below (this is a direct quote from page 289 of [1]):","venue":"","listofauthors":"P. Maurer","citations":23,"year":0,"publisher":"SAGE Publications, Inc.","pages":"28-31","volume":null,"number":null,"bibtex":"@incollection{1,\n\tdoi = {10.4135/9781526435897.n9},\n\turl = {https://doi.org/10.4135%2F9781526435897.n9},\n\tpublisher = {{SAGE} Publications, Inc.},\n\tpages = {28--31},\n\ttitle = {Deduction/Induction}\n}","authorsSemantic":[6]},{"id":310,"title":"The PC-Set Method Software Package","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":74,"title":"Context-Aware User Interface Field Classification","doi":"10.1109/ICITCS.2016.7740319","description":"Software applications are designed with a concrete purpose in mind, specified by business owners basing on the individual requirements. The user-system interaction is specified in the analysis phase. This phase specifies inputs, outputs, interaction etc. These elements could be specified separately based on the target platform. The designers know that the mobile clients could have a different application flow than desktop clients. However, this is not a rule and designers often times do not think about the user context nor do they consider the application context. This implies that outputs, inputs and interactions do not change automatically during the software life cycle. In this paper we present techniques that are able to determine whether the user, which is in a particular context, should be required to spend his/her time to fill in fields that are not needed for accomplishing a specific business task. Moreover, these techniques are able to determine whether the fields should display or are not necessary, as well as how the system might interact with the user. Finally, we present a computational architecture that is able to make these types of determinations.","venue":"2016 6th International Conference on IT Convergence and Security (ICITCS)","listofauthors":"M. Tomásek, T. Cerný","citations":0,"year":2016,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1109/icitcs.2016.7740319},\n\turl = {https://doi.org/10.1109%2Ficitcs.2016.7740319},\n\tyear = 2016,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Martin Tomasek and Tomas Cerny},\n\ttitle = {Context-Aware User Interface Field Classification}\n}","authorsSemantic":[1]},{"id":75,"title":"Adaptive Application Structure Design for Java EE Applications","doi":"10.1145/2987386.2987417","description":"Adaptive Application Structure (AAS) is such a structure that adjusts itself based on the current context. It brings benefits to end users, as it adapts to their specific personal needs. While applications themselves are designed in a particular way by developers, each user is using a particular application differently. Unfortunately, development complexities rise, which necessarily reflects in the development and maintenance efforts increase. The main aim is to mitigate the efforts and to introduce a novel approach to design AAS. A framework to design ASS-based applications is considered and evaluated.","venue":"RACS","listofauthors":"Jirí Sebek, T. Cerný, Karel Richta","citations":1,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/2987386.2987417},\n\turl = {https://doi.org/10.1145%2F2987386.2987417},\n\tyear = 2016,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Jiri Sebek and Tomas Cerny and Karel Richta},\n\ttitle = {Adaptive Application Structure Design for Java {EE} Applications}\n}","authorsSemantic":[1]},{"id":76,"title":"Authentication and Authorization Rules Sharing for Internet of Things","doi":"10.13052/JSN2445-9739.2017.003","description":"Significant interest in internet of things drives both research and industry production these days. A lot of important questions has been solved but some remain opened. One of the essential unresolved issue is the identity management of particular end devices. Having possibility to share authorization and authentication rules across network between various sensors, applications and users have clear advantages. It reduces duplication of those policies, while ensuring that they are coherent across the board. There are various proposals, methods and frameworks for identity management in normal environment. However, just few of them is usable in internet of things environment and even less have been made directly for the usage in the internet of things devices. This paper proposes solution for management of devices for internet of things. The solution is based on central identity store. Each device has an associated account in the store that any device and application in the network can verify the device’s identity against. The central element does not provide only authentication of the devices, but the devices can be associated with different roles. Those roles can be used for authorization. Case study for the proposed framework is built on top of the current web standards as OpenID Connect, OAuth and JSON Web Token. Also, central identity store is based on well-established open source solution. Journal of Software Networking, 35–52. doi: 10.13052/jsn2445-9739.2017.003 c © 2017 River Publishers. All rights reserved. 36 M. Trnka and T. Cerny The communication scheme is very simple – after the device’s account is established, it retrieves OAuth 2.0 token and uses it to certify itself in every network connection. The token does not only contain authentication information, but also roles assigned to the device. The central element thus creates trusted environment and enables rapid response for any security events.","venue":"","listofauthors":"Michal Trnka, T. Cerný","citations":4,"year":2016,"publisher":"River Publishers","pages":"35-52","volume":"2017","number":"1","bibtex":"@article{2016,\n\tdoi = {10.13052/jsn2445-9739.2017.003},\n\turl = {https://doi.org/10.13052%2Fjsn2445-9739.2017.003},\n\tyear = 2016,\n\tpublisher = {River Publishers},\n\tvolume = {2017},\n\tnumber = {1},\n\tpages = {35--52},\n\tauthor = {Michal Trnka and Tomas Cerny and   and},\n\ttitle = {Authentication and Authorization Rules Sharing for Internet of Things}\n}","authorsSemantic":[1]},{"id":77,"title":"On Metadata Extension to Derive Data Presentations with Angular 2","doi":"10.1109/ICITCS.2016.7740358","description":"From the viewpoint of how the World Wide Web and web application development evolves, there is a certain schema of designing web applications. The Robust Server application (backend) handles database management, as well as a broad range of tasks, and the Client application (frontend) provides data for presentation to users and mediating the connection with the server. Contemporary javascript-based frameworks, such as Angular 2 or ReactJS, extend the frontend code volumes. Such development approach has on large with information restatement and this exacerbates maintenance cost. This paper addresses this issue utilizing the Aspect-Oriented Programming concept, demonstrating this approach through javascript framework An- gular 2, and comparing it with the conventional development approach.","venue":"2016 6th International Conference on IT Convergence and Security (ICITCS)","listofauthors":"Z. Brabec, T. Cerný, Jirí Sebek","citations":1,"year":2016,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1109/icitcs.2016.7740358},\n\turl = {https://doi.org/10.1109%2Ficitcs.2016.7740358},\n\tyear = 2016,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Zdenek Brabec and Tomas Cerny and Jiri Sebek},\n\ttitle = {On Metadata Extension to Derive Data Presentations with Angular 2}\n}","authorsSemantic":[1]},{"id":78,"title":"Energy Impact of Web User Interface Technology on Mobile Devices","doi":"10.1109/ICITCS.2016.7740329","description":"Designing and maintaining modern web applications becomes more and more challenging since mobile, battery powered devices widely spread, and perhaps soon playing a major role throughout the everyday interaction with such applications. Application technologies follow this trend and evolve rapidly, but what are the main benefits of using such technologies for the end users aside from a progressive user interface design? It this article we present a simple case study of the impact of various front-end frameworks and application architecture approaches on mobile devices. Specifically, we consider the impact on client energy consumption and performance.","venue":"2016 6th International Conference on IT Convergence and Security (ICITCS)","listofauthors":"Jan Helbich, T. Cerný","citations":1,"year":2016,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1109/icitcs.2016.7740329},\n\turl = {https://doi.org/10.1109%2Ficitcs.2016.7740329},\n\tyear = 2016,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Jan Helbich and Tomas Cerny},\n\ttitle = {Energy Impact of Web User Interface Technology on Mobile Devices}\n}","authorsSemantic":[1]},{"id":311,"title":"The GF(2) General Linear Group for Dimensions 2, 3, 4, and 5","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":312,"title":"A Search Strategy Using a Hamming-Distance Oracle","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":313,"title":"10. Produktion eines Linienfluges","doi":"10.1524/9783486841848.282","description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":"OLDENBOURG WISSENSCHAFTSVERLAG","pages":null,"volume":null,"number":null,"bibtex":"@incollection{1,\n\tdoi = {10.1524/9783486841848.282},\n\turl = {https://doi.org/10.1524%2F9783486841848.282},\n\tpublisher = {{OLDENBOURG} {WISSENSCHAFTSVERLAG}},\n\ttitle = {10. Produktion eines Linienfluges}\n}","authorsSemantic":[6]},{"id":79,"title":"Separating out Platform-independent Particles of User Interfaces","doi":"10.1007/978-3-662-46578-3_112","description":"User Interfaces (UIs) visualize a wide range of various underlying computer application concerns. Such orthogonal concerns present in even the simplest UIs. The expectation of support for users from various backgrounds, location, different technical skills, etc. serves to increase concern complexity. Nowadays users typically remotely access to applications from a variety of platforms including web, mobile or even standalone clients. Providing platform-specific support for multiple UIs further increases the concern complexity. Such a wide-range of concerns often results in a significant portion of the UI description being restated using platform-specific components, which brings extended development, and maintenance efforts. This paper aims to separate out the platform-independent particles of UI that could be reused across various platforms. Such separation supports reduction of information restatement, development and maintenance effort. The platform-independent particles are provided in a machine-readable format to support their reuse in platform-specific UIs.","venue":"","listofauthors":"T. Cerný, M. J. Donahoo","citations":4,"year":2015,"publisher":"Springer Berlin Heidelberg","pages":"941-948","volume":null,"number":null,"bibtex":"@incollection{2015,\n\tdoi = {10.1007/978-3-662-46578-3_112},\n\turl = {https://doi.org/10.1007%2F978-3-662-46578-3_112},\n\tyear = 2015,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {941--948},\n\tauthor = {Tomas Cerny and Michael J. Donahoo},\n\ttitle = {Separating out Platform-independent Particles of User Interfaces}\n}","authorsSemantic":[2,1]},{"id":80,"title":"On distributed concern delivery in user interface design","doi":"10.2298/CSIS141202021C","description":"Increasing demands on user interface (UI) usability, adaptability, and \n dynamic behavior drives ever-growing development and maintenance complexity. \n Traditional UI design techniques result in complex descriptions for data \n presentations with significant information restatement. In addition, multiple \n concerns in UI development leads to descriptions that exhibit concern \n tangling, which results in high fragment replication. Concern-separating \n approaches address these issues; however, they fail to maintain the \n separation of concerns for execution tasks like rendering or UI delivery to \n clients. During the rendering process at the server side, the separation \n collapses into entangled concerns that are provided to clients. Such \n client-side entanglement may seem inconsequential since the clients are \n simply displaying what is sent to them; however, such entanglement \n compromises client performance as it results in problems such as replication, \n fragment granularity ill-suited for effective caching, etc. This paper \n considers advantages brought by concern-separation from both perspectives. It \n proposes extension to the aspect-oriented UI design with distributed concern \n delivery (DCD) for client-server applications. Such an extension lessens the \n serverside involvement in UI assembly and reduces the fragment replication in \n provided UI descriptions. The server provides clients with individual UI \n concerns, and they become partially responsible for the UI assembly. This \n change increases client-side concern reuse and extends caching opportunities, \n reducing the volume of transmitted information between client and server to \n improve UI responsiveness and performance. The underlying aspect-oriented UI \n design automates the server-side derivation of concerns related to data \n presentations adapted to runtime context, security, conditions, etc. \n Evaluation of the approach is considered in a case study applying DCD to an \n existing, production web application. Our results demonstrate decreased \n volumes of UI descriptions assembled by the server-side and extended \n client-side caching abilities, reducing required data/fragment transmission, \n which improves UI responsiveness. Furthermore, we evaluate the potential \n benefits of DCD integration implications in selected UI frameworks.","venue":"Comput. Sci. Inf. Syst.","listofauthors":"T. Cerný, Miroslav Macik, M. J. Donahoo, J. Janousek","citations":18,"year":2015,"publisher":"National Library of Serbia","pages":"655-681","volume":"12","number":"2","bibtex":"@article{2015,\n\tdoi = {10.2298/csis141202021c},\n\turl = {https://doi.org/10.2298%2Fcsis141202021c},\n\tyear = 2015,\n\tpublisher = {National Library of Serbia},\n\tvolume = {12},\n\tnumber = {2},\n\tpages = {655--681},\n\tauthor = {Tomas Cerny and Miroslav Macik and Michael Donahoo and Jan Janousek},\n\ttitle = {On distributed concern delivery in user interface design}\n}","authorsSemantic":[2,1]},{"id":81,"title":"On Concern-separation of Data Presentations in User Interfaces","doi":null,"description":"null","venue":"","listofauthors":"T. Cerný","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[1]},{"id":82,"title":"Enterprise information systems: comparison of aspect-driven and MVC-like Approaches","doi":"10.1145/2811411.2811477","description":"Design of an enterprise information system significantly impacts its development and maintenance efforts. The research shows that maintenance consumes about 65--75% of the software development time and about 40--60% of maintenance efforts are devoted to software understanding [2, 9]. This paper compares the Aspect-driven design approach as applied to the three-layered architecture to the MVC-like design approach implemented by many conventional web frameworks. While both approaches strive to avoid information restatement, they differ greatly in the underlying idea; thus, this work compares based on development efficacy and ease of maintenance. We highlight their differences and qualities, such as information cohesion, coupling and restatement, and discuss their maintenance efforts. We also investigate their ease of use, deployments, and provide recommendations on when to use each approach.","venue":"RACS","listofauthors":"Karel Cemus, T. Cerný, Lubos Matl, M. J. Donahoo","citations":2,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/2811411.2811477},\n\turl = {https://doi.org/10.1145%2F2811411.2811477},\n\tyear = 2015,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Karel Cemus and Tomas Cerny and Lubos Matl and Michael J. Donahoo},\n\ttitle = {Enterprise information systems}\n}","authorsSemantic":[2,1]},{"id":83,"title":"Automated Business Rules Transformation into a Persistence Layer","doi":"10.1016/j.procs.2015.08.391","description":"Abstract Enterprise Information Systems maintain data with respect to various business processes. These processes consist of business operations restricted by business rules expressed as preconditions and post-conditions. Each rule must be considered and enforced throughout the system, from user interface to persistence storage. Such rule evaluation in multiple contexts results in both significant rule restatement and high maintenance complexity, as there is no single focal point for capturing and reusing these rules. In this paper, we apply the Aspect-Oriented Design Approach to the persistence layer to simplify business rules management, enforce business rules throughout the system and consequently decrease development and maintenance efforts. Our preliminary results show that it is possible to define business rules in a single place and then apply them automatically in a persistence layer. We retrieve data sets restricted by given operation post-conditions with respect to current execution context without any manual rule restatement. This paper provides a small case study emphasizing the benefits and future challenges.","venue":"SCSE","listofauthors":"Karel Cemus, T. Cerný, M. J. Donahoo","citations":14,"year":2015,"publisher":"Elsevier BV","pages":"312-318","volume":"62","number":null,"bibtex":"@article{2015,\n\tdoi = {10.1016/j.procs.2015.08.391},\n\turl = {https://doi.org/10.1016%2Fj.procs.2015.08.391},\n\tyear = 2015,\n\tpublisher = {Elsevier {BV}},\n\tvolume = {62},\n\tpages = {312--318},\n\tauthor = {Karel Cemus and Tomas Cerny and Michael J. Donahoo},\n\ttitle = {Automated Business Rules Transformation into a Persistence Layer}\n}","authorsSemantic":[2,1]},{"id":84,"title":"On web services UI in user interface generation in standalone applications","doi":"10.1145/2811411.2811537","description":"Every production-level software application is like a living organism that changes and evolves throughout its lifecycle. Most of the changes related to application data impact the User Interface (UI). Usually, the change propagation to UI relates to input validations, data consistency as well as the UI behavior. In the case that UI involves data transfer from web services then the provided data format must correlate. This paper presents an approach that involves Model-Driven Development (MDD) to generate platform-independent definitions and provides these definitions to clients through web services. These definitions are generated based on templates, data types, annotations, security and application context using code-inspection. This approach decreases the effort involved in the development phase as well as in maintenance. Next, it prevents data inconsistencies and it makes it possible to specify the client's UI at runtime by the server. We consider the message format, input validation rules, type safety and then evaluate the proposed solution through a prototype and present its abilities, advantages and limitations from the UI perspective. Furthermore, we perform usability tests to determine suitability of the proposed approach for practical usage.","venue":"RACS","listofauthors":"M. Tomásek, T. Cerný","citations":2,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/2811411.2811537},\n\turl = {https://doi.org/10.1145%2F2811411.2811537},\n\tyear = 2015,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Martin Tomasek and Tomas Cerny},\n\ttitle = {On web services {UI} in user interface generation in standalone applications}\n}","authorsSemantic":[1]},{"id":464,"title":"9 USING ROLES TO CHARACTERIZE MODEL FAMILIES","doi":null,"description":"The development of reusable requirements and design artifacts often requires one to characterize families of problem and solution models. This paper presents a metamodeling approach to characterizing a family of models. A characterization is expressed as a Role Model that consists of roles that can be played by UML model elemets. In this paper we describe how a family of UML static structural diagrams that have the structural properties defined by a pattern can be characterized by a Static Role Model (SRM). The Abstract Factory pattern is used to illustrate how SRMs can be used to specify reusable designs expressed as patterns.","venue":"","listofauthors":"R. France, Dae-Kyoo Kim, Eunjee Song, Sudipto Ghosh","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":85,"title":"On Aspect-Oriented Programming in Adaptive User Interfaces","doi":"10.1109/ICISSEC.2015.7371024","description":"Adaptive User Interfaces (AUIs) provide better usability, user satisfaction as well as personalized experience. AUIs bring broad benefits to end users, but their construction introduces numerous difficulties caused by extended development and maintenance efforts. One option to deal with the above problems is to generate various User Interface (UI) descriptions from a model or existing code. This paper considers the Aspect-Oriented Programming (AOP)-based approach for the AUI design. Integrations of the approach with contemporary AUI frameworks, such as XML User Interface Language (XUL), User interface protocol (UIP) and JavaServer Faces (JSF) are compared from the perspective of the development and maintenance on a case study involving context- aware (CA) application. The outcome of this paper presents advantages, disadvantages and limitations of the particular frameworks for construction of AUIs.","venue":"2015 2nd International Conference on Information Science and Security (ICISS)","listofauthors":"Jirí Sebek, Michal Trnka, T. Cerný","citations":2,"year":2015,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1109/icissec.2015.7371024},\n\turl = {https://doi.org/10.1109%2Ficissec.2015.7371024},\n\tyear = 2015,\n\tmonth = {dec},\n\tpublisher = {{IEEE}},\n\tauthor = {Jiri Sebek and Michal Trnka and Tomas Cerny},\n\ttitle = {On Aspect-Oriented Programming in Adaptive User Interfaces}\n}","authorsSemantic":[1]},{"id":86,"title":"Evaluation of approaches to business rules maintenance in enterprise information systems","doi":"10.1145/2811411.2811476","description":"Enterprise information systems maintain a significant number of business rules defining complex business processes and data constraints. Conventional frameworks distribute such rules over multiple layers, making consistent rule location discovery difficult. This lack of rule centrality results in complicated and even error-prone development and maintenance as future application changes may lead to inconsistencies. In this paper, we consider these aspects of business rule definitions in various software design approaches from the software metrics perspective. We research several design approaches in a case study, compare their qualities regarding business rule definitions/maintenance, and demonstrate the difficulties with these approaches. The results of the study indicate that conventional approaches are insufficient in this area. Future design approaches that involve separation of concerns provide mechanisms to address the inefficiency.","venue":"RACS","listofauthors":"Karel Cemus, T. Cerný, M. J. Donahoo","citations":4,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/2811411.2811476},\n\turl = {https://doi.org/10.1145%2F2811411.2811476},\n\tyear = 2015,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Karel Cemus and Tomas Cerny and Michael J. Donahoo},\n\ttitle = {Evaluation of approaches to business rules maintenance in enterprise information systems}\n}","authorsSemantic":[2,1]},{"id":87,"title":"Evaluation of Separated Concerns in Web-based Delivery of User Interfaces","doi":"10.1007/978-3-662-46578-3_111","description":"User Interfaces (UI) play a significant role in contemporary web applications. Responsiveness and performance are influenced by the UI design, complexity of its features, the amount of transmitted information, as well as by network conditions. While traditional web delivery approaches separate out presentation of UI in the form of Cascading Style Sheets (CSS), a large number of presentation concerns are left tangled together in the structural description used for data presentations. Such tangling impedes concern reuse, which impacts the description size as well as caching options. This paper evaluates separation of UI concerns from the perspective of UI delivery. Concerns are distributed to clients through various resources/channels, which impacts the UI composition at the client-side. This decreases the volume of transmitted information and extends caching options. The efficacy is demonstrated through experiments.","venue":"","listofauthors":"T. Cerný, Lubos Matl, Karel Cemus, M. J. Donahoo","citations":5,"year":2015,"publisher":"Springer Berlin Heidelberg","pages":"933-940","volume":null,"number":null,"bibtex":"@incollection{2015,\n\tdoi = {10.1007/978-3-662-46578-3_111},\n\turl = {https://doi.org/10.1007%2F978-3-662-46578-3_111},\n\tyear = 2015,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {933--940},\n\tauthor = {Tomas Cerny and Lubos Matl and Karel Cemus and Michael J. Donahoo},\n\ttitle = {Evaluation of Separated Concerns in Web-based Delivery of User Interfaces}\n}","authorsSemantic":[2,1]},{"id":88,"title":"Proceedings of the 2015 Conference on research in adaptive and convergent systems","doi":"10.1145/2811411","description":"With the expansion of both the Internet and the advanced information technology development profession, reliable and convergent computing has attracted increasing interest in both academia and industry. To cope with this important problem, the Research in Adaptive and Convergent Systems (RACS) provides a forum for exchanging highly original ideas about an important class of computing systems. The RACS aims primarily at researchers who have experience in reliable and convergent computing systems and are engaged in the design and implementation of new computing applications. Each year RACS brings together engineers and scientists from diverse communities with interests in practical computing technologies and creates an environment for them to discuss and report experimental results, novel designs, work-in-progress, experiences, case studies, and trend-setting ideas.","venue":"RACS","listofauthors":"E. Nadimi, T. Cerný, Sung-Ryul Kim, W. Wang","citations":2,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@misc{2015,\n\tdoi = {10.1145/2811411},\n\turl = {https://doi.org/10.1145%2F2811411},\n\tyear = 2015,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\ttitle = {Proceedings of the 2015 Conference on research in adaptive and convergent systems}\n}","authorsSemantic":[1]},{"id":89,"title":"Context-aware Role-based Access Control Using Security Levels","doi":"10.1145/2811411.2811498","description":"Security of software applications is a very challenging and extensive topic. to keep up with the trend of personalized context aware applications the security design must adapt to it. this paper presents context awareness into the role based access control. it will describe already existing solutions, point out their key ideas and propose our rbac lightweight extension. it is universal and allows instant enhancement of current rbac even in current applications. the proposed solution is based on security levels which are assigned to users based on context. security levels represent how the users can be trusted and they are determined during the login procedure. the levels are used as additional security constraints to access resources. in application, the user needs to possesses not only the right permission granted through rbac roles, but also have a corresponding level.","venue":"RACS","listofauthors":"Michal Trnka, T. Cerný","citations":2,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/2811411.2811498},\n\turl = {https://doi.org/10.1145%2F2811411.2811498},\n\tyear = 2015,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Michal Trnka and Tomas Cerny},\n\ttitle = {Context-aware Role-based Access Control Using Security Levels}\n}","authorsSemantic":[1]},{"id":314,"title":"Using conjugate symmetries to enhance gate-level simulations","doi":"10.1109/DATE.2006.244010","description":"State machine based simulation of Boolean functions is substantially faster if the function being simulated is symmetric. Unfortunately function symmetries are comparatively rare. Conjugate symmetries can be used to reduce the state space for functions that have no detectable symmetries, allowing the benefits of symmetry to be applied to a much wider class of functions. Substantial improvements in simulation speed, from 30-40% have been realized using these techniques","venue":"Proceedings of the Design Automation & Test in Europe Conference","listofauthors":"P. Maurer","citations":3,"year":2006,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2006,\n\tdoi = {10.1109/date.2006.244010},\n\turl = {https://doi.org/10.1109%2Fdate.2006.244010},\n\tyear = 2006,\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer},\n\ttitle = {Using conjugate symmetries to enhance gate-level simulations}\n}","authorsSemantic":[6]},{"id":90,"title":"On separation of platform-independent particles in user interfaces","doi":"10.1007/s10586-015-0471-7","description":"The complexity of user interface (UI) design grows quickly with the number of application concerns. Such complexity compounds with additional requirement of contextual-awareness (i.e., adapt to user location, skill level, etc.) and support of heterogeneous devices and platforms (e.g., web, mobile app). Implementation support of such a wide-range of orthogonal concerns often results in restatement of a significant portion of the UI description using platform-specific components. Replication requires repeated implementation decision, greatly increasing development costs since each version/context variant may need separate development. Naturally, such replication also produces error prone maintenance because code updates must correlate among all replicas. Using separation of concerns, the application can be decomposed into fine-grain fragments, which we call particles, some of which are platform independent and others are not. Using this decomposition, this paper addresses the above inefficiency by dynamically composing particles at runtime that match user demands, context, and target platform.","venue":"Cluster Computing","listofauthors":"T. Cerný, M. J. Donahoo","citations":20,"year":2015,"publisher":"Springer Science and Business Media LLC","pages":"1215-1228","volume":"18","number":"3","bibtex":"@article{2015,\n\tdoi = {10.1007/s10586-015-0471-7},\n\turl = {https://doi.org/10.1007%2Fs10586-015-0471-7},\n\tyear = 2015,\n\tmonth = {jul},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {18},\n\tnumber = {3},\n\tpages = {1215--1228},\n\tauthor = {Tomas Cerny and Michael J. Donahoo},\n\ttitle = {On separation of platform-independent particles in user interfaces}\n}","authorsSemantic":[2,1]},{"id":91,"title":"Impact of Remote User Interface Design and Delivery on Energy Demand","doi":"10.1109/ICISSEC.2015.7371005","description":"Client-side User Interface (UI) for web applications clearly plays a critical role in user performance and efficiency. Growing user expectations drive UI design to greater functionality with ever increasing expectations for rich and continuous interactivity. Such increases require greater and greater computational resources. At the same time, web applications are increasingly accessed through mobile, battery-powered devices, such as notebooks, tablets, smartphones, and even watches. In effect, users are simultaneously increasing dependence on battery power and the pace of battery discharge with demanding applications. While UI design often considers factors such as usability, bandwidth consumption, etc., little consideration is given to the impact rendering and delivery design have on energy consumption. While we may expect novel technologies to expand battery capacity, the demands consistently outpace improvements. Careful consideration of UI design strategy may reduce the energy demands placed to the user's device. This paper presents a study considering existing UI design and delivery strategies and evaluates their impact on energy consumption.","venue":"2015 2nd International Conference on Information Science and Security (ICISS)","listofauthors":"T. Cerný, M. J. Donahoo","citations":2,"year":2015,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1109/icissec.2015.7371005},\n\turl = {https://doi.org/10.1109%2Ficissec.2015.7371005},\n\tyear = 2015,\n\tmonth = {dec},\n\tpublisher = {{IEEE}},\n\tauthor = {Tomas Cerny and Michael J. Donahoo},\n\ttitle = {Impact of Remote User Interface Design and Delivery on Energy Demand}\n}","authorsSemantic":[2,1]},{"id":92,"title":"Effective manycast messaging for Kademlia network","doi":"10.1145/2695664.2695903","description":"Peer-to-peer (P2P) communication plays an ever-expanding role in critical applications with rapidly growing user bases. In addition to well-known P2P systems for data sharing (e.g., BitTorrent), P2P provides the core mechanisms in VoIP (e.g., Skype), distributed currency (e.g., BitCoin), etc. There are many communication commonalities in P2P applications; consequently, we can factor these communication primitives into overlay services. Such services greatly simplify P2P application development and even allow P2P infrastructures to host multiple applications, instead of each having its own network. Note well that such services must be both self-scaling and robust to meet the needs of large, ad-hoc user networks. One such service is Distributed Hash Table (DHT) providing a dictionary-like location service, useful in many types of P2P applications. Building on the DHT primitives for search and store, we can add even more powerful group communication services to increase network capabilities with nodes-group formation and messaging (manycast, anycast, multicast, etc.). We begin by providing a survey of DHT networks and their group communication extensions. Next, we propose extensions for the Kademlia DHT to allow group communication and compare its properties with existing group communication services in the Pastry network. We place particular on manycast as it is a more generalized form of communication that has received little attention from the research community. Using these empirical results, we show which network is best suited to particular communication situations.","venue":"SAC","listofauthors":"Lubos Matl, T. Cerný, M. J. Donahoo","citations":1,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/2695664.2695903},\n\turl = {https://doi.org/10.1145%2F2695664.2695903},\n\tyear = 2015,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\tauthor = {Lubos Matl and Tomas Cerny and Michael J. Donahoo},\n\ttitle = {Effective manycast messaging for Kademlia network}\n}","authorsSemantic":[2,1]},{"id":93,"title":"Aspect-Driven Design of Information Systems","doi":"10.1007/978-3-319-04298-5_16","description":"Contemporary enterprise web applications deal with a large stack of different kinds of concerns involving business rules, security policies, cross-cutting configuration, etc. At the same time, increasing demands on user interface complexity make designers to consider the above concerns in the presentation. To locate a concern knowledge, we try to identify an appropriate system component with the concern definition. Unfortunately, this is not always possible, since there exist concerns cross-cutting multiple components. Thus to capture the entire knowledge we need to locate multiple components. In addition to it, often, we must restate the knowledge in the user interface because of technological incompatibility between the knowledge source and the user interface language. Such design suffers from tangled and hard to read code, due to the cross-cutting concerns and also from restated information and duplicated knowledge. This leads to a product that is hard to maintain, a small change becomes expensive, error-prone and tedious due to the necessity of manual changes in multiple locations.","venue":"SOFSEM","listofauthors":"Karel Cemus, T. Cerný","citations":17,"year":2014,"publisher":"Springer International Publishing","pages":"174-186","volume":null,"number":null,"bibtex":"@incollection{2014,\n\tdoi = {10.1007/978-3-319-04298-5_16},\n\turl = {https://doi.org/10.1007%2F978-3-319-04298-5_16},\n\tyear = 2014,\n\tpublisher = {Springer International Publishing},\n\tpages = {174--186},\n\tauthor = {Karel Cemus and Tomas Cerny},\n\ttitle = {Aspect-Driven Design of Information Systems}\n}","authorsSemantic":[1]},{"id":94,"title":"Proceedings of the 2014 Conference on Research in Adaptive and Convergent Systems, RACS 2014, Towson, Maryland, USA, October 5-8, 2014","doi":"10.1145/2663761","description":"With the expansion of both the Internet and the advanced information technology development profession, adaptive and convergent computing is of growing interest in both academia and industry. To cope with this important problem, the Research in Adaptive and Convergent Systems (RACS) provides a forum for exchanging highly original ideas about adaptive and convergent computing systems. The RACS aims primarily at researchers who have experience in adaptive and convergent computing systems and are engaged in the design and implementation of new computing applications. Each year RACS brings together engineers and scientists from diverse communities with interests in practical adaptive and convergent computing technologies and creates an environment for them to discuss and report experimental results, novel designs, work-in-progress, experiences, case studies, and trend-setting ideas.","venue":"RACS","listofauthors":"E. Nadimi, T. Cerný, Sung-Ryul Kim, W. Wang","citations":1,"year":2014,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@misc{2014,\n\tdoi = {10.1145/2663761},\n\turl = {https://doi.org/10.1145%2F2663761},\n\tyear = 2014,\n\tpublisher = {{ACM} Press},\n\ttitle = {Proceedings of the 2014 Conference on Research in Adaptive and Convergent Systems - {RACS} {\\textquotesingle}14}\n}","authorsSemantic":[1]},{"id":95,"title":"Efficient description and cache performance in Aspect-Oriented user interface design","doi":"10.15439/2014F244","description":"Increasing demands on web user interface (UI) usability, adaptability, and dynamic behavior drives ever growing development and maintenance complexity. Conventional design approaches scale poorly with such rising complexity, resulting in rapidly increasing costs. Much of the complexity centers around data presentation and processing. Recent work greatly reduces such data complexity through the application of Aspect-Oriented UI (AOUI) design, which separates various UI concerns; however, rendering in conventional and even AOUI approaches fails to maintain this separation, often resulting in high repetitions of concern fragments due to tangling. Even worse, mixing of dynamic and immutable components greatly limits caching efficacy as each have differing lifetimes. We extend AOUI design to push down concern separation to rendering, which reduces description size, through repetition reduction, and enables separate caching of individual concerns. Our results show considerable size reduction of UI descriptions for data presentations, faster load times and extended caching capabilities.","venue":"2014 Federated Conference on Computer Science and Information Systems","listofauthors":"T. Cerný, Miroslav Macik, M. J. Donahoo, J. Janousek","citations":6,"year":2014,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2014,\n\tdoi = {10.15439/2014f244},\n\turl = {https://doi.org/10.15439%2F2014f244},\n\tyear = 2014,\n\tmonth = {sep},\n\tpublisher = {{IEEE}},\n\tauthor = {Tom{\\'{a}}{\\v{s}} {\\v{C}}ern{\\'{y}} and Miroslav Macik and Michael J. Donahoo and Jan Janousek},\n\ttitle = {Efficient Description and Cache Performance in Aspect-Oriented User Interface Design}\n}","authorsSemantic":[2,1]},{"id":96,"title":"Platform-Aware Rich-Form Generation for Adaptive Systems through Code-Inspection","doi":"10.1007/978-3-642-39062-3_55","description":"This paper introduces a framework for adaptive user interface (UI) development. Our framework facilitates development and maintenance efforts through code inspection. Information already captured elsewhere is reused in the UI rather than restated. In our approach, inspected information is transformed in multiple stages through an aspect-oriented approach. As each stage may be influenced at runtime, our approach allows systems to be built with context-aware adaptive UIs. In addition, the selection of UI elements and their layout is generated using optimal metrics. The output of our approach can be influenced by the target platform. Our approach to UI is shown in detail in a case study.","venue":"SouthCHI","listofauthors":"Miroslav Macik, T. Cerný, Jindrich Basek, P. Slavík","citations":5,"year":2013,"publisher":"Springer Berlin Heidelberg","pages":"768-784","volume":null,"number":null,"bibtex":"@incollection{2013,\n\tdoi = {10.1007/978-3-642-39062-3_55},\n\turl = {https://doi.org/10.1007%2F978-3-642-39062-3_55},\n\tyear = 2013,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {768--784},\n\tauthor = {Miroslav Macik and Tomas Cerny and Jindrich Basek and Pavel Slavik},\n\ttitle = {Platform-Aware Rich-Form Generation for Adaptive Systems through Code-Inspection}\n}","authorsSemantic":[1]},{"id":97,"title":"Towards effective adaptive user interfaces design","doi":"10.1145/2513228.2513278","description":"The increasing use of Web-based applications continues to broaden the user groups of enterprise applications at large. The importance of providing easy-to-use user interfaces (UIs) that conform to each user's specific preferences, such as different skill levels, capabilities and physical locations has, therefore, been significantly increasing. Unfortunately, designing a single UI satisfying all end users remains challenging. To address this issue, researchers and developer are looking to Adaptive User Interfaces (AUIs) that aim to provide end users with more personalized user interaction experiences. However, very few production system provide such malleable interfaces due to the excessive cost for the development and maintenance.\n In this paper, we propose a technique that provides AUIs for production enterprise systems while reducing development and maintenance efforts to a level comparable with a single UI development, called Rich Entity Aspect/Audit Design (READ). READ complies with application development standards used in industry to support an easy transition from design to production systems. We conclude by evaluating our approach along with a case study that demonstrates reduction in development and maintenance efforts while preserving performance.","venue":"RACS","listofauthors":"T. Cerný, M. J. Donahoo, Eunjee Song","citations":14,"year":2013,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2013,\n\tdoi = {10.1145/2513228.2513278},\n\turl = {https://doi.org/10.1145%2F2513228.2513278},\n\tyear = 2013,\n\tpublisher = {{ACM} Press},\n\tauthor = {Tomas Cerny and Michael J. Donahoo and Eunjee Song},\n\ttitle = {Towards effective adaptive user interfaces design}\n}","authorsSemantic":[2,1,9]},{"id":98,"title":"Context-sensitive, cross-platform user interface generation","doi":"10.1007/s12193-013-0141-0","description":"User interfaces (UI) of software applications play a crucial part in communication with users. Attractive UIs often lead to market success, and thus there is a significant incentive to provide users with malleable UIs that can adapt as much as possible to their needs. However, such UIs require significant development and maintenance efforts. In this paper, we describe a context model based on ability-based design that is well suited to the purposes of automated UI generation. We then introduce a platform that delivers adaptive UIs across various platforms. We use runtime combinatoric optimisation to support usability and to generate context-sensitive UIs. Since the development and maintenance of such UIs can be complex, our platform integrates a module for code-inspection for data-oriented applications to reduce these efforts. It also utilises a visual editor to simplify manual UI design.","venue":"Journal on Multimodal User Interfaces","listofauthors":"Miroslav Macik, T. Cerný, P. Slavík","citations":27,"year":2014,"publisher":"Springer Science and Business Media LLC","pages":"217-229","volume":"8","number":"2","bibtex":"@article{2014,\n\tdoi = {10.1007/s12193-013-0141-0},\n\turl = {https://doi.org/10.1007%2Fs12193-013-0141-0},\n\tyear = 2014,\n\tmonth = {feb},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {8},\n\tnumber = {2},\n\tpages = {217--229},\n\tauthor = {Miroslav Macik and Tomas Cerny and Pavel Slavik},\n\ttitle = {Context-sensitive, cross-platform user interface generation}\n}","authorsSemantic":[1]},{"id":104,"title":"Cooperative web cache","doi":null,"description":"Internet web service delivery expectations grow every year but the underlying technology does not scale naturally to deal with greater demand for services. A simple service, once becoming popular, requires the service provider to invest in powerful hardware in order to deal with sudden spike in client interest. A standard solution involves employing a content distribution network resulting in spiralling costs with increasing load. In this paper we argue it is possible to scale web service delivery more naturally assuming clients take part in content replication. We propose a design and a prototype implementation of a P2P overlay network that shows optimistic preliminary results. An extensive simulation is provided in order to establish performance possibilities of such a network with no extra related costs or demands on hardware.","venue":"2011 18th International Conference on Systems, Signals and Image Processing","listofauthors":"T. Cerný, Petr Praus, Slávka Jaromerská, Lubos Matl, J. Donahoo","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[1]},{"id":376,"title":"Automatic routing of integrated circuit connections: a tutorial","doi":"10.1109/ICC.1990.117186","description":"An introduction to automatic integrated-circuit routing is presented. Particular attention is given to the types of routing algorithm, global and detailed routing, and improving routing and routability.<<ETX>>","venue":"IEEE International Conference on Communications, Including Supercomm Technical Sessions","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/icc.1990.117186},\n\turl = {https://doi.org/10.1109%2Ficc.1990.117186},\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer},\n\ttitle = {Automatic routing of integrated circuit connections: a tutorial}\n}","authorsSemantic":[6]},{"id":99,"title":"Aspect-driven, data-reflective and context-aware user interfaces design","doi":"10.1145/2577554.2577561","description":"The increasing use of Web-based applications continues to broaden the user groups of enterprise applications at large. Since ordinary users often equate the quality of user interface (UI) with the quality of the entire application, the importance of providing easy-to-use UIs has been significantly increasing. Unfortunately, designing a single UI satisfying all end users remains challenging. To address this issue, researchers and developers are looking to Context-aware/Adaptive UIs (CUIs) that aim to provide end users with more personalized user interaction experiences. Although multiple proposals have been made, very few production systems provide such malleable interfaces due to the excessive cost of development and maintenance.\n In this paper, we propose a technique that aims to reduce development and maintenance efforts of CUI to a level comparable with a single UI. Unlike most of the existing CUI approaches, our technique does not involve an external UI model. Instead, it aims to reflect runtime-information and structures already captured in the application, while extending them to provide an appropriate CUI. With this technique, developers do not design forms or tables directly for each page or panel. Instead they design generic and reusable transformation rules capable of presenting application data instances in the UI while considering the runtime context. To demonstrate our technique and its impact on CUI development and maintenance, we provide a case study. Moreover, we present our experience from its application to an existing production-level enterprise application, with high demands on performance.","venue":"SIAP","listofauthors":"T. Cerný, Karel Cemus, M. J. Donahoo, Eunjee Song","citations":33,"year":2013,"publisher":"Association for Computing Machinery (ACM)","pages":"53-66","volume":"13","number":"4","bibtex":"@article{2013,\n\tdoi = {10.1145/2577554.2577561},\n\turl = {https://doi.org/10.1145%2F2577554.2577561},\n\tyear = 2013,\n\tmonth = {dec},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {13},\n\tnumber = {4},\n\tpages = {53--66},\n\tauthor = {Tomas Cerny and Karel Cemus and Michael J. Donahoo and Eunjee Song},\n\ttitle = {Aspect-driven, data-reflective and context-aware user interfaces design}\n}","authorsSemantic":[2,1,9]},{"id":100,"title":"Towards a Smart, Self-scaling Cooperative Web Cache","doi":"10.1007/978-3-642-27660-6_36","description":"The traditional client/server architecture for web service delivery fails to naturally scale. This results in growing costs to the service provider for powerful hardware or extensive use of Content Distribution Networks. A P2P overlay network provides inherent scalability with multiple benefits to both clients and servers. In this paper, we provide analysis, design and prototype implementation of Cooperative Web Cache, which allows us to scale web service delivery and cope with demand spikes by employing clients in content replication. To demonstrate performance capabilities, we provide a prototype emulation for both client and server.","venue":"SOFSEM","listofauthors":"T. Cerný, Petr Praus, Slávka Jaromerská, Lubos Matl, M. J. Donahoo","citations":4,"year":2012,"publisher":"Springer Berlin Heidelberg","pages":"443-455","volume":null,"number":null,"bibtex":"@incollection{2012,\n\tdoi = {10.1007/978-3-642-27660-6_36},\n\turl = {https://doi.org/10.1007%2F978-3-642-27660-6_36},\n\tyear = 2012,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {443--455},\n\tauthor = {Tom{\\'{a}}{\\v{s}} {\\v{C}}ern{\\'{y}} and Petr Praus and Sl{\\'{a}}vka Jarom{\\v{e}}{\\v{r}}sk{\\'{a}} and Lubo{\\v{s}} Matl and Michael J. Donahoo},\n\ttitle = {Towards a Smart, Self-scaling Cooperative Web Cache}\n}","authorsSemantic":[2,1]},{"id":101,"title":"Impact of user interface generation on maintenance","doi":"10.1109/CSAE.2012.6272847","description":"User interface development and maintenance is one of the most time consuming parts of software application development. User interfaces provide numerous graphical visualizations of user data, these are often influenced by multiple factors such as available interface elements, data constraints, layouts, user operating devices or user rights. The complexity of the development and maintenance raises from duplicated and restated information. Duplication occurs multiple times in the application, for example data model already contains data constraints and these are restated in the interface. Not only information but also decisions are duplicated when an interface element is selected and bound to a particular data field. Machine driven code-inspection and its transformation to user interface brings the way to address complex efforts related to user interface. In this paper we present code-inspection approach to automate user interface development and maintenance, we also provide a case study that compares the approach with manual development.","venue":"2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE)","listofauthors":"T. Cerný, V. Chalupa, M. J. Donahoo","citations":2,"year":2012,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2012,\n\tdoi = {10.1109/csae.2012.6272847},\n\turl = {https://doi.org/10.1109%2Fcsae.2012.6272847},\n\tyear = 2012,\n\tmonth = {may},\n\tpublisher = {{IEEE}},\n\tauthor = {Tomas Cerny and Vaclav Chalupa and Michael J. Donahoo},\n\ttitle = {Impact of user interface generation on maintenance}\n}","authorsSemantic":[2,1]},{"id":102,"title":"ELISA: Extensible Layer for Internet Services and Applications","doi":"10.1007/978-1-4614-7540-8_24","description":"Contemporary P2P services and applications often design and implement the entire system from the base ground or builds on low-level frameworks. This leads to significant development and maintenance efforts and often reinvention of the wheel. In this chapter we propose extensible layer that does not expose low-level implementation details to client application developers, while providing them with well-known services and communication mechanisms built on efficient and scalable substrates. Analysis and design of architecture for this extensible layer are described. Special attention is given to security concerns and manycast communication approach. Our preliminary implementation is emulated and evaluated to establish functional and performance possibilities of the prototype.","venue":"ISD","listofauthors":"Lubos Matl, V. Kloucek, Viktor B. Bohdal, Jan Kubr, T. Cerný","citations":1,"year":2013,"publisher":"Springer US","pages":"309-321","volume":null,"number":null,"bibtex":"@incollection{2013,\n\tdoi = {10.1007/978-1-4614-7540-8_24},\n\turl = {https://doi.org/10.1007%2F978-1-4614-7540-8_24},\n\tyear = 2013,\n\tpublisher = {Springer {US}},\n\tpages = {309--321},\n\tauthor = {Lubos Matl and Vladimir Kloucek and Viktor B. Bohdal and Jan Kubr and Tomas Cerny},\n\ttitle = {{ELISA}: Extensible Layer for Internet Services and Applications}\n}","authorsSemantic":[1]},{"id":103,"title":"Towards Smart User Interface Design","doi":"10.1109/ICISA.2012.6220929","description":"User interface development and maintenance presents a burden for many developers. UI development approaches often restate information already captured in the application model such as entity attributes, validation, security, etc. Changes in application model often require many subsequent changes to the UI. Such duplication creates additional maintenance requirements for synchronization (at a minimum) and often is a source for errors (i.e., when model and UI disagree). Adding to the difficulties of creation and maintenance, typical UI implementations often tangle multiple concerns together such as presentation, validation, layout, security, etc. In this paper, we provide an approach that reduces information duplication and untangles mixed concerns. The capability of runtime UI generation can render user-specific UI, reduce conditional evaluation, and integrate third party security frameworks. To evaluate our approach, we provide a case study that demonstrates reduction of maintenance efforts, separation of concerns and performance of runtime UI generation.","venue":"2012 International Conference on Information Science and Applications","listofauthors":"T. Cerný, V. Chalupa, M. J. Donahoo","citations":6,"year":2012,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2012,\n\tdoi = {10.1109/icisa.2012.6220929},\n\turl = {https://doi.org/10.1109%2Ficisa.2012.6220929},\n\tyear = 2012,\n\tmonth = {may},\n\tpublisher = {{IEEE}},\n\tauthor = {Tomas Cerny and Vaclav Chalupa and Michael J. Donahoo},\n\ttitle = {Towards Smart User Interface Design}\n}","authorsSemantic":[2,1]},{"id":105,"title":"How to reduce costs of business logic maintenance","doi":"10.1109/CSAE.2011.5953174","description":"Three tier enterprise applications introduce multiple challenges for software engineers. Although we can divide the application into three tiers, we still need to design properly each tier internally to achieve multiple design qualities. The middle business tier captures logic in which we associate objects, validate business rules, etc. Often multiple cross-cutting concerns are mixed in the services which results in bloated, highly coupled design with very low cohesion. In this paper we present a case study that we develop based on our four year experience with enterprise application that struggled from multiple weak design decisions. We emphasize multiple aspects that should be decoupled from the rest of the services which increase service cohesion and results in better readability, maintenance, testability, reuse and error-avoidance. Our “best practices” suggestions for business tier are generally applicable and allow the designer to separate service concerns into multiple units allowing to achieve the mentioned quality attributes.","venue":"2011 IEEE International Conference on Computer Science and Automation Engineering","listofauthors":"T. Cerný, M. J. Donahoo","citations":11,"year":2011,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2011,\n\tdoi = {10.1109/csae.2011.5953174},\n\turl = {https://doi.org/10.1109%2Fcsae.2011.5953174},\n\tyear = 2011,\n\tmonth = {jun},\n\tpublisher = {{IEEE}},\n\tauthor = {Tomas Cerny and Michael J. Donahoo},\n\ttitle = {How to reduce costs of business logic maintenance}\n}","authorsSemantic":[2,1]},{"id":106,"title":"UML-based enhanced rich form generation","doi":"10.1145/2103380.2103420","description":"The Model Driven Development (MDD) has provided a new way of engineering today's rapidly changing requirements into the implementation. However, the development of the user interface (UI) part of an application has not benefited much from MDD although today's UIs are complex software components and play an essential role in the usability of an application. It is a common practice that developers create view forms manually by referring to entity beans to determine their content. However, such kind of manual creation is very error-prone and thus makes the system maintenance difficult. One promise in MDD is that we can generate code from UML models, but existing design models in MDD does not capture enough information that are required to generate desired UI fragments. This paper presents our approach addressing these issues. The approach makes it possible to generate complex UIs, rich view forms, that fully satisfy both designers and end-users and to enforce system access control.","venue":"RACS","listofauthors":"T. Cerný, Eunjee Song","citations":7,"year":2011,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2011,\n\tdoi = {10.1145/2103380.2103420},\n\turl = {https://doi.org/10.1145%2F2103380.2103420},\n\tyear = 2011,\n\tpublisher = {{ACM} Press},\n\tauthor = {Tomas Cerny and Eunjee Song},\n\ttitle = {{UML}-based enhanced rich form generation}\n}","authorsSemantic":[1,9]},{"id":107,"title":"Competitive and Collaborative Approach Towards a More Effective Education in Computer Science","doi":"10.30935/CEDTECH/6050","description":"To provide computer scientists with good materials and interesting topics in a class does not necessarily mean that their education is of a high quality, students need to be motivated and evolve skills needed in a real-life employment. Social skills, teamwork, collaboration and competition are valuable aspects they should know in other to become professionals. This paper presents a study with intention to improve education of computer science students in employment-like environments. The study utilizes experience with competitive and collaborative learning in education and Programming Olympiads. Multiple methodological aspects are applied and discussed with regard to students’ evaluation. The results show increased student motivation and interest in the course, which produces larger workload in the class.","venue":"","listofauthors":"T. Cerný, Bozena Mannová","citations":10,"year":2011,"publisher":"Bastas Publications","pages":null,"volume":"2","number":"2","bibtex":"@article{2011,\n\tdoi = {10.30935/cedtech/6050},\n\turl = {https://doi.org/10.30935%2Fcedtech%2F6050},\n\tyear = 2011,\n\tmonth = {jun},\n\tpublisher = {Bastas Publications},\n\tvolume = {2},\n\tnumber = {2},\n\tauthor = {Tomas Cerny and Bozena Mannova},\n\ttitle = {Competitive and Collaborative Approach Towards a More Effective Education in Computer Science}\n}","authorsSemantic":[1]},{"id":108,"title":"SScAC: Towards a Framework for Small-Scale Software Architectures Comparison","doi":"10.1007/978-3-642-18381-2_40","description":"We present a framework for small-scale software architecture comparison (SScAC). Although a considerable chunk of software architectures are developed in small teams, not much related work exists on this topic. The proposed framework introduces a method to formalize these comparisons and aims to be simple enough to be used in small-scale projects at the same time. Still we believe it is of sufficient complexity to support comparisons that take into account different aspects of solved problem. The main purpose of the framework is to ease certain architectural choices by giving the designer a reasoned recommendation based on previously specified requirements on system's qualities. It can also help validate the suitability of chosen design patterns. We show the practical use of the framework on case study solving architectural decision for Key Word In Context.","venue":"SOFSEM","listofauthors":"Petr Praus, Slávka Jaromerská, T. Cerný","citations":1,"year":2011,"publisher":"Springer Berlin Heidelberg","pages":"482-493","volume":null,"number":null,"bibtex":"@incollection{2011,\n\tdoi = {10.1007/978-3-642-18381-2_40},\n\turl = {https://doi.org/10.1007%2F978-3-642-18381-2_40},\n\tyear = 2011,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {482--493},\n\tauthor = {Petr Praus and Sl{\\'{a}}vka Jarom{\\v{e}}{\\v{r}}sk{\\'{a}} and Tom{\\'{a}}{\\v{s}} {\\v{C}}ern{\\'{y}}},\n\ttitle = {{SScAC}: Towards a Framework for Small-Scale Software Architectures Comparison}\n}","authorsSemantic":[1]},{"id":109,"title":"PERFORMANCE OPTIMIZATION FOR ENTERPRISE WEB APPLICATIONS THROUGH REMOTE CLIENT SIMULATION","doi":null,"description":"Contemporary enterprise web applications provide complex user interface functionality to attract users with desktop-like features. Naturally, such increasing UI complexity and user expectation results in greater application resource demands, which may degrade performance. System performance is measured by application responsiveness for end users in service delivery. Web application performance optimization occurs at all levels of the application, from server hardware, to database, etc. In this paper, we focus on the network delivery of application assets, identify bottlenecks in the service delivery, and provide suggestions for optimization. To accomplish this, we provide tools to simulate a variety of remote user communication scenarios and identify the application assets with the greatest impact on application performance. Based on the simulation results for the production application, we provide optimization options, which may be iteratively applied and simulated until the desired application performance is achieved.","venue":"","listofauthors":"T. Cerný, M. J. Donahoo","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[1]},{"id":377,"title":"Development of schematic capture support for FHDL","doi":"10.1109/SECON.1990.117852","description":"A user-friendly graphical interface to the Florida hardware design language (FHDL), the schematic capture program, is presented. It was designed primarily to interact with FHDL by incorporating into the design of the graphics package the capabilities for producing FHDL descriptions of circuits. The system is window based and entirely menu driven. User interaction takes place through a set of walking menus or through the mouse. Keyboard input is used to label circuits and to name files that contain circuit descriptions. The development and implementation of the program are discussed.<<ETX>>","venue":"IEEE Proceedings on Southeastcon","listofauthors":"Nitin V. Bhate, A. Tokuta, P. Maurer","citations":1,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/secon.1990.117852},\n\turl = {https://doi.org/10.1109%2Fsecon.1990.117852},\n\tpublisher = {{IEEE}},\n\tauthor = {N. Bhate and A. Tokuta and P. Maurer},\n\ttitle = {Development of schematic capture support for {FHDL}}\n}","authorsSemantic":[6]},{"id":110,"title":"A Tool for Evaluation and Optimization of Web Application Performance","doi":null,"description":"One of the main goals of web application developers is to design fast systems that can respond to users in short time. Fast systems response increases user satisfaction. Although this fact is well known there does not exist many tools that could help to determine system bottlenecks for distant users. The main issue for web applications is that users come from all around the globe and the application is most of the time served in one location. We may evaluate the application in laboratory conditions, but these differ from the real ones. Network characteristics may significantly impact the true bottleneck. Many techniques are employed for performance improvements, but there is not an easy way to test the impact of the improvements for a distant user in the real environment. We propose and provide a debugging web proxy, CZProxy, that can simulate the environment for distant users. Our proxy can simulate both latency stretch and bandwidth restrictions, allowing optimization over a variety of end-user connection scenarios.","venue":"","listofauthors":"T. Cerný, M. J. Donahoo","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[1]},{"id":111,"title":"MetaMorPic: Self-Contained Photo Archival and Presentation","doi":"10.1007/978-1-4419-9790-6_12","description":"Cost associated with the maintenance and scale of custom services presents one of the most significant barriers to entry for information providers. One solution proposes pushing computation into the cloud where providers like Amazon maintain a scalable, world-wide platform for virtualization. While this avoids certain hardware maintenance costs, service providers must still maintain clusters of virtual machines. To address this, contextual services like Flickr provide a complete solution; unfortunately, this limits information providers to the available services and look-and-feel. We propose a compromise solution that combines existing services and applications. Such a solution decreases development and maintenance costs by providing standardized services with third-party maintenance, while allowing customizable functionality and look-and-feel. We present a specific example of this type of blended solution in our MetaMorPic system, which provides photo archiving and presentation capabilities using third-party software and services.","venue":"ISD","listofauthors":"T. Cerný, M. J. Donahoo","citations":3,"year":2011,"publisher":"Springer New York","pages":"149-158","volume":null,"number":null,"bibtex":"@incollection{2011,\n\tdoi = {10.1007/978-1-4419-9790-6_12},\n\turl = {https://doi.org/10.1007%2F978-1-4419-9790-6_12},\n\tyear = 2011,\n\tpublisher = {Springer New York},\n\tpages = {149--158},\n\tauthor = {Tomas Cerny and Michael J. Donahoo},\n\ttitle = {{MetaMorPic}: Self-Contained Photo Archival and Presentation}\n}","authorsSemantic":[2,1]},{"id":112,"title":"FormBuilder : A Novel Approach to Deal with View Development and Maintenance ?","doi":null,"description":"In most web applications, the attributes of entity classes directly determine the content of corresponding view forms. In addition, these entity classes often encapsulate constraints on associated properties. For example, a User entity class may have an email property with constraints on the form of the address; consequently, the view form for creating/updating users should include an email field and perform validation on the submitted data. Unfortunately, view form development is often done manually, an error-prone and tedious process. Form error detection is particularly difficult because the errors only manifest themselves at runtime because of weak type safety and limited mechanisms for constraint verification. In addition, entity modification may cause inconsistency with the corresponding form. In this paper, we propose a new tool, FormBuilder, which automates the development and maintenance of view forms. The application of this tool in production applications has repeatedly demonstrated the practical contribution of this approach.","venue":"","listofauthors":"T. Cerný, M. J. Donahoo","citations":4,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[1]},{"id":113,"title":"A Profile Approach to Using UML Models for Rich Form Generation","doi":"10.1109/ICISA.2010.5480315","description":"The Model Driven Development (MDD) has provided a new way of engineering today's rapidly changing requirements into the implementation. However, the development of user interface (UI) part of an application has not benefit much from MDD although today's UIs are complex software components and they play an essential role in the usability of an application. As one of the most common UI examples, consider view forms that are used for collecting data from the user. View forms are usually generated with a lot of manual efforts after the implementation. For example, in case of Java 2 Enterprise Edition (Java EE) web applications, developers create all view forms manually by referring to entity beans to determine the content of forms, but such manual creation is pretty tedious and certainly very much error-prone and makes the system maintenance difficult. One promise in MDD is that we can generate code from UML models. Existing design models in MDD, however, cannot provide all class attributes that are required to generate the practical code of UI fragments. In this paper, we propose a UML profile for view form generation as an extension of the object relational mapping (ORM) profile. A profile form of hibernate validator is also introduced to implement the practical view form generation that includes an user input validation.","venue":"2010 International Conference on Information Science and Applications","listofauthors":"T. Cerný, Eunjee Song","citations":9,"year":2010,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2010,\n\tdoi = {10.1109/icisa.2010.5480315},\n\turl = {https://doi.org/10.1109%2Ficisa.2010.5480315},\n\tyear = 2010,\n\tpublisher = {{IEEE}},\n\tauthor = {Tomas Cerny and Eunjee Song},\n\ttitle = {A Profile Approach to Using {UML} Models for Rich Form Generation}\n}","authorsSemantic":[1,9]},{"id":114,"title":"Scheduling with Soft CLP(FD) Solver","doi":null,"description":"The implementation of the Soft CLP(FD) solver is presented. The\nsolver can be applied to solve general problems with both hard\nand soft constraints. The representatives of the problems which\ncan be solved with the solver are presented.","venue":"","listofauthors":"T. Cerný, Hana Rudová","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[1]},{"id":115,"title":"Security Challenges in Smart City Applications","doi":null,"description":"Recent technological innovations have resulted in a deployment of a large number of interconnected IoT devices for augmenting personal lifestyle. Urbanization is happening at a remarkable rate, and it is estimated to reach 5 billion population by 2030, likely resulting in dense clustering of these devices. Such proliferation requires advanced management approaches using the latest IT platforms and techniques for adapting city-related services to this new expectation. Such emerging integration of technologies like ubiquitous sensing, heterogeneous network infrastructure, and intelligent Information processing and control systems faces several security challenges. Our research focuses on providing an overview based on research efforts in smart cities addressing these security challenges and solutions for smart transportation, living, healthcare, and energy. We highlight the effects of malicious attacks and their countermeasures while addressing discrimination from false positives such a Flash Event (FE). Finally, we discuss future research directions and limitations.","venue":"","listofauthors":"Safwan Mawlood Hussein, M. J. Donahoo, T. Cerný","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":116,"title":"PERFORMANCE OPTIMIZATION FOR ENTERPRISE WEB APPLICATIONS THROUGH REMOTE CLIENT SIMULATION","doi":null,"description":"Contemporary enterprise web applications provide complex user interface functionality to attract users with desktop-like features. Naturally, such increasing UI complexity and user expectation results in greater application resource demands, which may degrade performance. System performance is measured by application responsiveness for end users in service delivery. Web application performance optimization occurs at all levels of the application, from server hardware, to database, etc. In this paper, we focus on the network delivery of application assets, identify bottlenecks in the service delivery, and provide suggestions for optimization. To accomplish this, we provide tools to simulate a variety of remote user communication scenarios and identify the application assets with the greatest impact on application performance. Based on the simulation results for the production application, we provide optimization options, which may be iteratively applied and simulated until the desired application performance is achieved.","venue":"","listofauthors":"T. Cerný, M. J. Donahoo","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":117,"title":"A Tool for Evaluation and Optimization of Web Application Performance","doi":null,"description":"One of the main goals of web application developers is to design fast systems that can respond to users in short time. Fast systems response increases user satisfaction. Although this fact is well known there does not exist many tools that could help to determine system bottlenecks for distant users. The main issue for web applications is that users come from all around the globe and the application is most of the time served in one location. We may evaluate the application in laboratory conditions, but these differ from the real ones. Network characteristics may significantly impact the true bottleneck. Many techniques are employed for performance improvements, but there is not an easy way to test the impact of the improvements for a distant user in the real environment. We propose and provide a debugging web proxy, CZProxy, that can simulate the environment for distant users. Our proxy can simulate both latency stretch and bandwidth restrictions, allowing optimization over a variety of end-user connection scenarios.","venue":"","listofauthors":"T. Cerný, M. J. Donahoo","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":118,"title":"Secure group communications for Delay-Tolerant Networks","doi":null,"description":"The increased demand for mobile communication and use of mobile devices in high-latency, resource impoverished environments has spurred the development and growth of Delay-Tolerant Networks (DTN). DTNs aim to provide interoperability between a range of heterogeneous networks, operating under resource-constrained circumstances and traditional infrastructure networks such as the Internet. Because of the circumstances, DTNs possess some interesting characteristics that make a traditional end-to-end security paradigm unsuitable and increase the value of the overlay's resources. Controlling access to overlay resources and providing for secure group communications over unknown intermediate networks is essential. We propose a novel solution based on previous work in secure group communications using key-graphs and in extension to work on scalable access authorization in self-organizing overlays to provide a scalable mechanism for access control and secure group communications in DTNs. Since resources are especially limited, our implementation focuses on minimizing the traffic on the overlay associated with the maintenance of our solution.","venue":"2010 International Conference for Internet Technology and Secured Transactions","listofauthors":"Paul T. Edelman, M. J. Donahoo, David B. Sturgill","citations":7,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":119,"title":"FormBuilder : A Novel Approach to Deal with View Development and Maintenance ?","doi":null,"description":"In most web applications, the attributes of entity classes directly determine the content of corresponding view forms. In addition, these entity classes often encapsulate constraints on associated properties. For example, a User entity class may have an email property with constraints on the form of the address; consequently, the view form for creating/updating users should include an email field and perform validation on the submitted data. Unfortunately, view form development is often done manually, an error-prone and tedious process. Form error detection is particularly difficult because the errors only manifest themselves at runtime because of weak type safety and limited mechanisms for constraint verification. In addition, entity modification may cause inconsistency with the corresponding form. In this paper, we propose a new tool, FormBuilder, which automates the development and maintenance of view forms. The application of this tool in production applications has repeatedly demonstrated the practical contribution of this approach.","venue":"","listofauthors":"T. Cerný, M. J. Donahoo","citations":4,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":120,"title":"Under the Hood","doi":"10.1016/B978-0-12-374540-8.00009-2","description":"This chapter discusses the relationship between the programming constructs and the underlying protocol implementations. Knowing that these socket data structures exist and how they are affected by the underlying protocols is useful because they control various aspects of the behavior of the various Socket objects. Data passed in a single invocation of the output stream's write() method at the sender can be spread across multiple invocations of the input stream's read() method at the other end; and a single read() may return data passed in multiple write()s. When a program attempts to create a socket with a particular local port number, the existing sockets are checked to make sure that no socket is already using that local port. A Socket () constructor will throw an exception if any socket matches the local port and local IP address (if any) specified in the constructor. For a ServerSocket, all constructors require the local port. The local address may be specified to the constructor; otherwise, the local address is the wildcard (*) address. The foreign address and port for a ServerSocket are always wildcards. For a Socket, all constructors require specification of the foreign address and port. For a Socket instance returned by accept(), the local address is the destination address from the initial handshake message from the client, the local port is the local port of the ServerSocket, and the foreign address/port is the local address/port of the client.","venue":"","listofauthors":"K. Calvert, M. J. Donahoo","citations":100,"year":2009,"publisher":"Elsevier","pages":"143-160","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1016/b978-0-12-374540-8.00009-2},\n\turl = {https://doi.org/10.1016%2Fb978-0-12-374540-8.00009-2},\n\tyear = 2009,\n\tpublisher = {Elsevier},\n\tpages = {143--160},\n\tauthor = {Donahoo Michael J. and Calvert Kenneth L.},\n\ttitle = {Under the Hood}\n}","authorsSemantic":[2]},{"id":165,"title":"Using statistical sampling for query optimization in heterogeneous library information systems","doi":"10.1145/170791.170904","description":"Query optimization in heterogeneous database systems is not always possible since the component DBMS may not have the ability to transmit necessary information. However, these systems need query optimization because the cost of transmitting large quantities of data across diverse databases is very high. We propose a query strategy which uses hypothesis testing to determine which of two sets of data are larger. Our experiments show that this strategy is very likely to select the smaller set when the sampling results fall outside a region of uncertainty we call the “grey zone.” This provides query optimization without transmission of database statistics.","venue":"CSC '93","listofauthors":"G. Speegle, M. J. Donahoo","citations":1,"year":1993,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1993,\n\tdoi = {10.1145/170791.170904},\n\turl = {https://doi.org/10.1145%2F170791.170904},\n\tyear = 1993,\n\tpublisher = {{ACM} Press},\n\tauthor = {Gregory D. Speegle and Michael J. Donahoo},\n\ttitle = {Using statistical sampling for query optimization in heterogeneous library information systems}\n}","authorsSemantic":[2,10]},{"id":121,"title":"TCP/IP Sockets in Java, Second Edition: Practical Guide for Programmers","doi":null,"description":"The networking capabilities of the Java platform have been extended considerably since the first edition of the book. This new edition covers version 1.5-1.7, the most current iterations, as well as making the following improvements:The API (application programming interface) reference sections in each chapter, which describe the relevant parts of each class, have been replaced with (i) a summary section that lists the classes and methods used in the code, and (ii) a ?gotchas? section that mentions nonobvious or poorly-documented aspects of the objects. In addition, the book covers several new classes and capabilities introduced in the last few revisions of the Java platform. New abstractions to be covered include NetworkInterface, InterfaceAddress, Inet4/6Address, SocketAddress/InetSocketAddress, Executor, and others; extended access to low-level network information; support for IPv6; more complete access to socket options; and scalable I/O. The example code is also modified to take advantage of new language features such as annotations, enumerations, as well as generics and implicit iterators where appropriate.Most Internet applications use sockets to implement network communication protocols. This book's focused, tutorial-based approach helps the reader master the tasks and techniques essential to virtually all client-server projects using sockets in Java. Chapter 1 provides a genral overview of networking concepts to allow readers to synchronize the concepts with terminology. Chapter 2 introduces the mechanics of simple clients and servers. Chapter 3 covers basic message construction and parsing. Chapter 4 then deals with techniques used to build more robust clients and servers. Chapter 5 (NEW) introduces the scalable interface facilities which were introduced in Java 1.5, including the buffer and channel abstractions. Chapter 6 discusses the relationship between the programming constructs and the underlying protocol implementations in more detail. Programming concepts are introduced through simple program examples accompanied by line-by-line code commentary that describes the purpose of every part of the program. The book's Web site contains many examples of command-based sockets-related code discussed throughout the book. No other resource presents so concisely or so effectively the material necessary to get up and running with Java sockets programming. KEY FEATURES* Focused, tutorial-based instruction in key sockets programming techniques allows reader to quickly come up to speed on Java applications. * Concise and up-to-date coverage of the most recent platform (1.7) for Java applications in networking technology* Provides code for all example programs via a companion Web site to let the reader see the important objects and methods in context and to understand the purpose of each line of code.","venue":"","listofauthors":"K. Calvert, M. J. Donahoo","citations":16,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":122,"title":"chapter 3 – Reshaping Results","doi":"10.1016/B978-012220531-6/50003-8","description":"Publisher Summary \nThe result table's columns are determined by the column list of the SELECT statement, and the WHERE expression determines the result table rows. This chapter explores the various structured query language (SQL) capabilities for manipulating result tables. If a SELECT statement generates a new table by default, result table columns have the same name as attributes in the original table; however, one can change the result table column name using a column alias. One can assign a column alias in the column list of the SELECT statement using AS. The AS keyword is optional. To specify a column alias with a space or other special character, one should enclose the alias name in quotes, otherwise the quotes are optional. Furthermore, duplicate rows are okay in SQL. One can eliminate duplicate rows by prefixing the SELECT column list with the DISTINCT keyword. DISTINCT only appears once in the SELECT statement at the head of the attribute list, and it applies to all attributes in the attribute list. SQL uses the values for both attributes to determine if each row value is unique.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"47-73","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50003-8},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50003-8},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {47--73},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Reshaping Results}\n}","authorsSemantic":[2,10]},{"id":123,"title":"chapter 5 – Joins","doi":"10.1016/B978-012220531-6/50005-1","description":"Publisher Summary \nStructured query language (SQL) allows deriving a single table that contains all of the desired data. One can then query that table using the various SQL techniques already known. Combining tables to derive a new table is called a join. This chapter provides numerous examples of join queries (how to join two tables). The FROM clause creates the new table that has the combined attributes of all tables in the list. The table name prefix is not required if the attribute name is unambiguous, as with name in the SELECT clause. The WHERE clause describes how to connect the tables. After executing the FROM and WHERE clauses, a table is formed with the attributes of both the tables and the rows with matched data. Finally, SQL executes the SELECT clause, keeping only the attributes in the SELECT list. Once the new table is created, one can use it just like any other table.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"91-115","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50005-1},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50005-1},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {91--115},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Joins}\n}","authorsSemantic":[2,10]},{"id":124,"title":"Creating, Deleting, and Altering Tables","doi":"10.1016/B978-012220531-6/50009-9","description":"Tables and all other parts of a database that are not data are collectively known as metadata. Metadata is data about data. Structured query language (SQL) is also known as metadata. The basic table has a name and a set of columns, each with its own data type. One can create a table in SQL using CREATE TABLE. This creates a table named . The columns of the table are specified in a comma-delimited list of name/data type pairs. Before creating a table, most database management systems (DBMSs) require the creation of a database to hold the new table. When a new row is created, any columns without a specified value are assigned the default value. Unless otherwise specified, the default value of a column is NULL. SQL allows specifying a default value for a column using the DEFAULT clause. DBMS can do much more than just store and access data. It can also enforce rules (called constraints) on what data are allowed in the database. Such constraints are important because they help maintain data integrity. SQL enforces constraints by prohibiting any data in the database that violate any constraint. Any insert, update, or delete that would result in a constraint violation is rejected without changing the database.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"179-204","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50009-9},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50009-9},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {179--204},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Creating, Deleting, and Altering Tables}\n}","authorsSemantic":[2,10]},{"id":125,"title":"chapter 4 – Aggregating Results","doi":"10.1016/B978-012220531-6/50004-X","description":"Publisher Summary \nDatabases are designed to hold many data. Given a set of data, structured query language (SQL) can provide with a single, aggregate value over that data. SQL provides several basic aggregation functions. These functions take an expression and return an aggregate value over all rows in the specified table. It includes information about each function, including what it computes, the acceptable data types, and how NULLs and repeated values are handled. There are five basic aggregate functions in SQL: AVG, SUM, MIN, MAX, and COUNT. AVG computes the average of an expression over all rows in the result table. Similarly, SUM computes the sum. The expression parameter can be as simple as a single column name. MIN and MAX find the minimum and maximum value, respectively, of the given expression over all rows in the given table. Because NULL is not comparable, it can be neither the minimum nor the maximum value; consequently, MIN and MAX ignore NULL values, just like AVG and SUM. Unlike AVG and SUM, MIN and MAX are not limited to numeric values. Further, COUNT returns the number of rows, and it comes in two flavors. COUNT (*) computes the number of rows in a table, including NULLs.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"75-89","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50004-x},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50004-x},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {75--89},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Aggregating Results}\n}","authorsSemantic":[2,10]},{"id":126,"title":"chapter 12 – Database Privileges","doi":"10.1016/B978-012220531-6/50012-9","description":"Publisher Summary \nDatabases are all about sharing data; it is accessible to many users. SQL allows one to assign different types of privileges to different users. A user identifier specifies a user. The creation and maintenance of users is DBMS specific. Some DBMSs use identifiers from the underlying operating system; others maintain their own set of users. Many DBMSs have a CREATE USER command for creating new users. The user that creates a database object, such as a table, is called the owner. The owner of an object can do just about anything he or she wants with that object, including determining the privileges of other users for that object. GRANT gives the specified privilege(s) on the named object to the list of identified users. Specifying the WITH GRANT OPTION allows the identified users to grant their privileges to other users. Many people need different access to tables. It is important that one grants exactly the privileges needed, no more, no less. Granting too many privileges opens the door for security holes. Being too restrictive prevents a user from doing their job.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"227-235","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50012-9},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50012-9},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {227--235},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Database Privileges}\n}","authorsSemantic":[2,10]},{"id":127,"title":"SQL: Practical Guide for Developers","doi":null,"description":"Chapter 1: Databasics Chapter 2: Single Table Retrieval Chapter 3: Taming Tables Chapter 4: Aggregating Results Chapter 5: Multiple Table Queries using Simple Subqueries Chapter 6: Multiple Table Queries Using Joins Chapter 7: Set Based Queries Chapter 8: Advanced Subqueries Chapter 9: Creating a Database Chapter 10: Database Data Chapter 11: Transaction Management Chapter 12: Authorization Chapter 13: Advanced Topics Chapter 14: SQL Programming","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":6,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":128,"title":"chapter 7 – Subqueries","doi":"10.1016/B978-012220531-6/50007-5","description":"Publisher Summary \nStructured query language (SQL) provides another way to combine tables. One can nest queries within queries. Such an embedded query is called a subquery. A subquery computes results that are then used by an outer query. A subquery can be nested inside the SELECT, FROM, WHERE, and HAVING clauses. SQL marks the boundaries of a subquery with parentheses. When using subqueries, only the columns of the outermost query can appear in the result table. When creating a new query, the outermost query must contain all of the attributes needed in the answer. There are some ways of connecting the outer query to the inner query. All SQL comparison operators work with subqueries. Subqueries are restricted in what they can return. First, the row, and column count must match the comparison operator. Second, the data types must be compatible. The type of the results of a subquery is determined by the context of the query. The standard comparison operators, BETWEEN, IS NULL, and LIKE assume scalar values. When these operators are used, SQL assumes a scalar context, and the result of the subquery is converted into a scalar value.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"127-164","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50007-5},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50007-5},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {127--164},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Subqueries}\n}","authorsSemantic":[2,10]},{"id":129,"title":"chapter 10 – Views","doi":"10.1016/B978-012220531-6/50010-5","description":"Publisher Summary \nA view is a virtual table defined by a query. It provides a mechanism to create alternate ways of working with the data in a database. A view acts much like a table. One can query it with a SELECT, and some views even allow INSERT, UPDATE, and DELETE. However, a view doesn't have any data. All of its data are ultimately derived from tables. A CREATE VIEW command creates a view named . The column names/types and data for the view are determined by the result table derived by executing . Optionally, one can specify the column names of the view in . The number of columns in must match the number of columns in the . There are several uses of views, such as usability, security, and reduced dependency. One can use a view as a wrapper around very complex SELECT statements to make the system more usable. If one requires restricting the data a user can access, one can create a view containing only the permitted data. The user is given access to the view instead of the base table(s).","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"205-213","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50010-5},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50010-5},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {205--213},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Views}\n}","authorsSemantic":[2,10]},{"id":130,"title":"chapter 1 – Databasics","doi":"10.1016/B978-012220531-6/50001-4","description":"Publisher Summary \nA database is a repository designed for organizing and accessing information. For simple data, management is easy. Basic lists work for very simple databases. However, the limitations of this approach can make even simple tasks difficult. Computers are especially adept at managing and quickly accessing information. Software designed to store, manipulate, and retrieve data in a database is called a database management system (DBMS). There are many types of DBMSs. Relational DBMSs speak a common language called structured query language (SQL). Using SQL, one can define, manipulate, and query the data. SQL is divided into three major parts. Data manipulation language (DML) is used to store and retrieve data from the database. Data description language (DDL) is used to define the structure of the data. Data control language (DCL) is used to restrict access to data by certain users. Furthermore, in the relational model, a database contains a set of tables. A table is made up of rows and columns; each table has a name, which is unique within the database. Each column has a name and a data type; the name of a column need only be unique within a table so other tables in the same database can have columns of the same name. Each row constitutes one record in the table. A table may contain zero or more rows. A row is subdivided into fields, one per column. Tables may be used to model real-world objects and relationships.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"1-26","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50001-4},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50001-4},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {1--26},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Databasics}\n}","authorsSemantic":[2,10]},{"id":191,"title":"Cross-Species Integrative Functional Genomics in GeneWeaver Reveals a Role for Pafah1b1 in Altered Response to Alcohol","doi":"10.3389/fnbeh.2016.00001","description":"Identifying the biological substrates of complex neurobehavioral traits such as alcohol dependency pose a tremendous challenge given the diverse model systems and phenotypic assessments used. To address this problem we have developed a platform for integrated analysis of high-throughput or genome-wide functional genomics studies. A wealth of such data exists, but it is often found in disparate, non-computable forms. Our interactive web-based software system, Gene Weaver (http://www.geneweaver.org), couples curated results from genomic studies to graph-theoretical tools for combinatorial analysis. Using this system we identified a gene underlying multiple alcohol-related phenotypes in four species. A search of over 60,000 gene sets in GeneWeaver's database revealed alcohol-related experimental results including genes identified in mouse genetic mapping studies, alcohol selected Drosophila lines, Rattus differential expression, and human alcoholic brains. We identified highly connected genes and compared these to genes currently annotated to alcohol-related behaviors and processes. The most highly connected gene not annotated to alcohol was Pafah1b1. Experimental validation using a Pafah1b1 conditional knock-out mouse confirmed that this gene is associated with an increased preference for alcohol and an altered thermoregulatory response to alcohol. Although this gene has not been previously implicated in alcohol-related behaviors, its function in various neural mechanisms makes a role in alcohol-related phenomena plausible. By making diverse cross-species functional genomics data readily computable, we were able to identify and confirm a novel alcohol-related gene that may have implications for alcohol use disorders and other effects of alcohol.","venue":"Front. Behav. Neurosci.","listofauthors":"J. Bubier, T. Wilcox, J. Jay, M. Langston, E. Baker, E. Chesler","citations":83,"year":2016,"publisher":"Frontiers Media SA","pages":null,"volume":"10","number":null,"bibtex":"@article{2016,\n\tdoi = {10.3389/fnbeh.2016.00001},\n\turl = {https://doi.org/10.3389%2Ffnbeh.2016.00001},\n\tyear = 2016,\n\tmonth = {jan},\n\tpublisher = {Frontiers Media {SA}},\n\tvolume = {10},\n\tauthor = {Jason A. Bubier and Troy D. Wilcox and Jeremy J. Jay and Michael A. Langston and Erich J. Baker and Elissa J. Chesler},\n\ttitle = {Cross-Species Integrative Functional Genomics in {GeneWeaver} Reveals a Role for Pafah1b1 in Altered Response to Alcohol}\n}","authorsSemantic":[3]},{"id":131,"title":"chapter 11 – Transactions","doi":"10.1016/B978-012220531-6/50011-7","description":"Publisher Summary \nDatabases are all about sharing data, so it is common for multiple users to be accessing and even changing the same data at the same time. The simultaneous execution of operations is called concurrency. Sometimes concurrency can get into trouble if the changes require multiple SQL statements. In addition, databases also allow the grouping of a sequence of SQL statements into an indivisible unit of work called a transaction. A transaction ends with either a commit or a rollback. A commit permanently stores all of the changes performed by the transaction. A rollback removes all of the updates performed by the transaction, no matter how many rows have been changed. A rollback can be executed either by the DBMS to prevent incorrect actions or explicitly by the user. The DBMS provides the following guarantees for a transaction, called the ACID properties: atomicity, consistency, isolation, and durability. COMMIT attempts to commit all of the changes made since the beginning of the transaction. If a problem is detected, COMMIT signals an error, and the transaction is rolled back. Once a commit successfully completes, the changes are permanent. ROLLBACK undoes all of the changes made since the beginning of the current transaction.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"215-226","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50011-7},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50011-7},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {215--226},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Transactions}\n}","authorsSemantic":[2,10]},{"id":132,"title":"Retrieval: Basic SELECTion","doi":"10.1016/B978-012220531-6/50002-6","description":"This chapter discusses structured query language (SQL) with the most basic operation: retrieving information. A database of interest already exists, complete with data, and one wants to query that data. The best way to learn SQL is by doing it. At its simplest, one only needs to tell SELECT two things: the data attributes one wants and the table, where it should get it. The SQL statement fetches the values from a single column, name, from all of the rows in the items table, and displays the results. To retrieve multiple columns, one should specify a comma-delimited list of column names. In the table, there is an order. The order in which the attributes are specified in the column name list determines the order of attributes in the result. The presentation of a NULL value in a result is database management systems specific. Furthermore, SQL has two types of comments. The first type begins with two minus signs (\"–\") and ends with the end of the line. The second type begins with /*, ends with */, and can span multiple lines. The chapter demonstrates how to extract data from a single table. It specifies the attributes one wants in the SELECT list.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"27-45","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50002-6},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50002-6},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {27--45},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Retrieval}\n}","authorsSemantic":[2,10]},{"id":133,"title":"Set Queries: UNION, INTERSECT, and EXCEPT","doi":"10.1016/B978-012220531-6/50006-3","description":"A result table is thought of as a set of rows. There are various operators in structured query language (SQL) that perform certain operations, such as UNION, INTERSECT, and EXCEPT. The SQL set operators combine query results to create new results. The UNION operator corresponds to U in sets. Where U combines two sets, UNION combines two query results. To keep duplicates, one should use UNION ALL. One can use DISTINCT instead of ALL to eliminate duplicate values; however, duplicate elimination is the default behavior so specifying DISTINCT is optional. UNION combines results in a very different way from joins. Furthermore, the intersections of two sets are all elements that are common to both sets. In SQL, the INTERSECT operator returns all rows that are the same in both results. The ALL modifier can be added to INTERSECT to require SQL to keep duplicates. Another common set operation is set difference. If R and S are sets, then R–S contains all of the elements in R that are not in S. The EXCEPT operator in SQL is similar, in that it returns the rows in the first result that are not in the second one. Like UNION and INTERSECT, EXCEPT eliminates duplicates. To keep duplicates, one should use EXCEPT ALL.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"117-126","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50006-3},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50006-3},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {117--126},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Set Queries}\n}","authorsSemantic":[2,10]},{"id":134,"title":"Introduction to Cursors, Embedded SQL, Stored Procedures, and Triggers","doi":"10.1016/B978-012220531-6/50013-0","description":"SQL is powerful, but it also requires more capabilities to make it truly useful. Such capabilities include executing SQL and accessing results in other programming languages, scripting procedures, and reacting to changes in the database. An SQL query returns an entire set of rows. In some instances, one may wish to process a result one row at a time instead of all at once. This can be done through a cursor. A cursor is basically a pointer to some position within the rows of a result set. Sometimes single SQL statements are not sufficient for what one needs to do. There are several ways to create a sequence of SQL statements to solve a problem, such as stored procedures. SQL allows the creation of scripts, called stored procedures, within the database. These scripts may contain one or more SQL statements. Stored procedures may take parameter values as input and even return results. The scripting language also includes loops, conditionals, and variables.","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"237-247","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50013-0},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50013-0},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {237--247},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Introduction to Cursors, Embedded {SQL}, Stored Procedures, and Triggers}\n}","authorsSemantic":[2,10]},{"id":140,"title":"Method-specific knowledge compilation","doi":"10.1007/978-1-4757-4911-3_19","description":"Generality and scale are important but difficult issues in knowledge engineering. At the root of the difficulty lie two challenging issues: how to accumulate huge volumes of knowledge and how to support heterogeneous knowledge and processing. One approach to the first issue is to reuse legacy knowledge systems, integrate knowledge systems with legacy databases, and enable sharing of the databases by multiple knowledge systems. We present an architecture called HIPED for realizing this approach. HIPED converts the second issue above into a new form: how to convert data accessed from a legacy database into a form appropriate to the processing method used in a legacy knowledge system. One approach to this reformed issue is to use method-specific compilation of data into knowledge. We describe an experiment in which a legacy knowledge system called Interactive Kritik is integrated with an ORACLE database. The experiment indicates the computational feasibility of method-specific data-to-knowledge compilation.","venue":"","listofauthors":"J. William Murdock, Ashok K. Goel, M. J. Donahoo, S. Navathe","citations":0,"year":2001,"publisher":"Springer US","pages":"443-463","volume":null,"number":null,"bibtex":"@incollection{2001,\n\tdoi = {10.1007/978-1-4757-4911-3_19},\n\turl = {https://doi.org/10.1007%2F978-1-4757-4911-3_19},\n\tyear = 2001,\n\tpublisher = {Springer {US}},\n\tpages = {443--463},\n\tauthor = {J. William Murdock and Ashok K. Goel and Michael J. Donahoo and Shamkant Navathe},\n\ttitle = {Method-Specific Knowledge Compilation}\n}","authorsSemantic":[2]},{"id":135,"title":"chapter 8 – Modifying Data","doi":"10.1016/B978-012220531-6/50008-7","description":"Publisher Summary \nThere are ways to add, modify, and update data. Structured query language (SQL) provides three statements for modifying data: INSERT, UPDATE, and DELETE. The INSERT statement adds new rows to a specified table. There are two variants of the INSERT statement. One inserts a single row of values into the database, whereas the other inserts multiple rows returned from a SELECT. The most basic form of INSERT creates a single, new row with either user-specified or default values. The values of the row are specified by the value list. The items in the VALUES list are matched one-by-one, in order of the attributes in the table. NULL attribute value can be specified with the NULL keyword. All data types also have a default value. The default value for any attribute is NULL unless otherwise specified. Fields that are defined as NOT NULL do not have a default value unless one is explicitly specified. Whereas, DELETE removes all rows from the table where condition evaluates to true. DELETE may also use correlated subqueries. When executing DELETE, one needs to be careful with the WHERE clause. Using substring matching with LIKE or an inequality comparator can cause unexpected consequences. UPDATE changes all the rows in the table where condition evaluates to true","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":0,"year":2005,"publisher":"Elsevier","pages":"165-178","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1016/b978-012220531-6/50008-7},\n\turl = {https://doi.org/10.1016%2Fb978-012220531-6%2F50008-7},\n\tyear = 2005,\n\tpublisher = {Elsevier},\n\tpages = {165--178},\n\tauthor = {Michael J. Donahoo and Gregory D. Speegle},\n\ttitle = {Modifying Data}\n}","authorsSemantic":[2,10]},{"id":136,"title":"A Framework for Method-Specific Knowledge Compilation from Databases","doi":"10.1023/A:1012545615073","description":"Generality and scale are important but difficult issues in knowledge engineering. At the root of the difficulty lie two challenging issues: how to accumulate huge volumes of knowledge and how to support heterogeneous knowledge and processing. One approach to the first issue is to reuse legacy knowledge systems, integrate knowledge systems with legacy databases, and enable sharing of the databases by multiple knowledge systems. We present an architecture called HIPED for realizing this approach. HIPED converts the second issue above into a new form: how to convert data accessed from a legacy database into a form appropriate to the processing method used in a legacy knowledge system. One approach to this reformed issue is to use method-specific compilation of data into knowledge. We describe an experiment in which a legacy knowledge system called INTERACTIVE KRITIK is integrated with an ORACLE database. The experiment indicates the computational feasibility of method-specific data-to-knowledge compilation.","venue":"Journal of Intelligent Information Systems","listofauthors":"J. William Murdock, Ashok K. Goel, M. J. Donahoo, S. Navathe","citations":3,"year":2001,"publisher":"Springer Science and Business Media LLC","pages":"5-21","volume":"17","number":"1","bibtex":"@article{2001,\n\tdoi = {10.1023/a:1012545615073},\n\turl = {https://doi.org/10.1023%2Fa%3A1012545615073},\n\tyear = 2001,\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {17},\n\tnumber = {1},\n\tpages = {5--21},\n\tauthor = {J. William Murdock and Ashok K. Goel and Michael J. Donahoo and Shamkant Navathe}\n}","authorsSemantic":[2]},{"id":137,"title":"TCP / IP sockets in C# - practical guide for programmers","doi":null,"description":"Most Internet applications use sockets to implement network communication protocols. TCP/IP Sockets in Java: Practical Guide for Programmers, with its focused, tutorial-based coverage, helps you master the tasks and techniques essential to virtually all client-server projects using sockets in Java. Later chapters teach you to implement more specialized functionality; incisive discussions of programming constructs and protocol implementations equip you with a deeper understanding that is invaluable for meeting future challenges. No other resource presents so concisely or so effectively the exact material you need to get up and running with Java sockets programming.","venue":"The Morgan Kaufmann practical guides series","listofauthors":"David B. Makofske, M. J. Donahoo, K. Calvert","citations":90,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":138,"title":"Sending and Receiving Messages","doi":"10.1016/B978-012466051-9/50003-2","description":"This chapter covers the basics of message construction and parsing. The TCP/IP protocols transport bytes of user data without examining or modifying them. This allows applications great flexibility in how they encode their information for transmission. For various reasons, most application protocols are defined in terms of discrete messages made up of sequences of fields. Each field contains a specific piece of information encoded as a sequence of bits. The application protocol specifies how these sequences of bits are to be formatted by the sender and interpreted, or parsed, by the receiver so that the latter can extract the meaning of each field. About the only constraint imposed by TCP/IP is that information must be sent and received in chunks whose length in bits is a multiple of eight. As a strongly typed language, Java requires that other types―String, int, and so on―be explicitly converted to these transmittable types. Fortunately, the language has a number of built-in facilities that make such conversions more convenient. It is explored how Java data types can be encoded in different ways, and how messages can be constructed from various types of information. Recent versions of Java include serialization capabilities―the Serializable and Externalizable interfaces—which provide for instances of supporting Java classes to be converted to and from byte sequences very easily. A basic tenet of good protocol design is that the protocol should constrain the implementor as little as possible and should minimize assumptions about the platform on which the protocol will be implemented.","venue":"","listofauthors":"David B. Makofske, M. J. Donahoo, K. Calvert","citations":0,"year":2004,"publisher":"Elsevier","pages":"59-84","volume":null,"number":null,"bibtex":"@incollection{2004,\n\tdoi = {10.1016/b978-012466051-9/50003-2},\n\turl = {https://doi.org/10.1016%2Fb978-012466051-9%2F50003-2},\n\tyear = 2004,\n\tpublisher = {Elsevier},\n\tpages = {59--84},\n\tauthor = {David B. Makofske and Michael J. Donahoo and Kenneth L. Calvert},\n\ttitle = {Sending and Receiving Messages}\n}","authorsSemantic":[2]},{"id":139,"title":"TCP / IP sockets in Java - practical guide for programmers","doi":null,"description":"The networking capabilities of the Java platform have been extended considerably since the first edition of the book. This new edition covers version 1.5-1.7, the most current iterations, as well as making the following improvements:The API (application programming interface) reference sections in each chapter, which describe the relevant parts of each class, have been replaced with (i) a summary section that lists the classes and methods used in the code, and (ii) a ?gotchas? section that mentions nonobvious or poorly-documented aspects of the objects. In addition, the book covers several new classes and capabilities introduced in the last few revisions of the Java platform. New abstractions to be covered include NetworkInterface, InterfaceAddress, Inet4/6Address, SocketAddress/InetSocketAddress, Executor, and others; extended access to low-level network information; support for IPv6; more complete access to socket options; and scalable I/O. The example code is also modified to take advantage of new language features such as annotations, enumerations, as well as generics and implicit iterators where appropriate.Most Internet applications use sockets to implement network communication protocols. This book's focused, tutorial-based approach helps the reader master the tasks and techniques essential to virtually all client-server projects using sockets in Java. Chapter 1 provides a genral overview of networking concepts to allow readers to synchronize the concepts with terminology. Chapter 2 introduces the mechanics of simple clients and servers. Chapter 3 covers basic message construction and parsing. Chapter 4 then deals with techniques used to build more robust clients and servers. Chapter 5 (NEW) introduces the scalable interface facilities which were introduced in Java 1.5, including the buffer and channel abstractions. Chapter 6 discusses the relationship between the programming constructs and the underlying protocol implementations in more detail. Programming concepts are introduced through simple program examples accompanied by line-by-line code commentary that describes the purpose of every part of the program. The book's Web site contains many examples of command-based sockets-related code discussed throughout the book. No other resource presents so concisely or so effectively the material necessary to get up and running with Java sockets programming.KEY FEATURES* Focused, tutorial-based instruction in key sockets programming techniques allows reader to quickly come up to speed on Java applications. * Concise and up-to-date coverage of the most recent platform (1.7) for Java applications in networking technology* Provides code for all example programs via a companion Web site to let the reader see the important objects and methods in context and to understand the purpose of each line of code.","venue":"The Morgan Kaufmann practical guides series","listofauthors":"K. Calvert, M. J. Donahoo","citations":26,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":141,"title":"Chapter 5 – Socket Programming","doi":"10.1016/B978-155860826-9/50016-1","description":"Publisher Summary \nThis chapter presents client and server examples to demonstrate the basic model for socket programming. The ideas need to be integrated into various programming models such as multitasking, signaling, and broadcasting. These principles are shown in the context of standard UNIX programming; however, most modern operating systems support similar features (processes and threads). The Transmission Control Protocol (TCP)/Internet Protocol developers spent a good deal of time thinking about the default behaviors that would satisfy most applications. Signals provide a mechanism for notifying programs that certain events have occurred. This chapter explores several models of such concurrent servers, including perclient processes, perclient threads, and constrained multitasking. The decision of using broadcast or multicast in an application depends on several issues, including the portion of network hosts interested in receiving the data and the knowledge of the communicating parties. Broadcast works well if a large percentage of the network hosts wish to receive the message; however, if there are many more hosts than receivers, broadcast is very inefficient.","venue":"","listofauthors":"M. J. Donahoo","citations":1,"year":2001,"publisher":"Elsevier","pages":"43-85","volume":null,"number":null,"bibtex":"@incollection{2001,\n\tdoi = {10.1016/b978-155860826-9/50016-1},\n\turl = {https://doi.org/10.1016%2Fb978-155860826-9%2F50016-1},\n\tyear = 2001,\n\tpublisher = {Elsevier},\n\tpages = {43--85},\n\tauthor = {Michael J. Donahoo and Kenneth L. Calvert},\n\ttitle = {Socket Programming}\n}","authorsSemantic":[2]},{"id":142,"title":"Chapter 2 – Basic Sockets","doi":"10.1016/B978-155860826-9/50013-6","description":"Publisher Summary \nAlthough clients and servers differ in some aspects of their use of the application programming interface (API), other aspects are common across clients, and servers and across Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) sockets. This chapter deals with these common aspects and presents the details through an example. A program begins by asking the operating system to create an instance of the socket abstraction. The function that accomplishes this is a socket; its parameters specify the flavor of socket needed by the program. When an application is finished with a socket, it calls close, giving the descriptor for the socket that is no longer needed. Applications using sockets need to be able to specify Internet addresses and ports to the kernel. The distinction between client and server is important because each uses the socket interface differently at certain steps in the communication. The job of client is to initiate communications with a server that is passively waiting to be contacted. The server's job is to set up a communication endpoint and passively wait for a connection from the client. As with clients, the setup for a TCP and UDP server is similar. Creating the socket, sending, receiving, and closing are the same as in the client. The differences in the server have to do with binding an address to the socket and then using the socket as a channel to “receive” other sockets that are connected to clients.","venue":"","listofauthors":"M. J. Donahoo","citations":0,"year":2001,"publisher":"Elsevier","pages":"9-23","volume":null,"number":null,"bibtex":"@incollection{2001,\n\tdoi = {10.1016/b978-155860826-9/50013-6},\n\turl = {https://doi.org/10.1016%2Fb978-155860826-9%2F50013-6},\n\tyear = 2001,\n\tpublisher = {Elsevier},\n\tpages = {9--23},\n\tauthor = {Michael J. Donahoo and Kenneth L. Calvert},\n\ttitle = {Basic Sockets}\n}","authorsSemantic":[2]},{"id":143,"title":"Chapter 4 – Using UDP Sockets","doi":"10.1016/B978-155860826-9/50015-X","description":"Publisher Summary \nThe User Datagram Protocol (UDP) provides an end-to-end service different from that of the Transmission Control Protocol (TCP). UDP performs only two functions. It adds another layer of addressing (ports) to that of the Internet Protocol (IP), and it detects data corruption that may occur in transit and discards any corrupted datagrams. Because of this simplicity, UDP (datagram) sockets have some different characteristics from the TCP (stream) sockets. As soon as it is created, a UDP socket can be used to send/receive messages to/from any address and to/from many different addresses in succession. To allow the destination address to be specified for each message, the socket application programming interface provides a different sending routine that is generally used with UDP sockets. Another difference between UDP sockets and TCP sockets is the way they deal with message boundaries: UDP sockets preserve them. This makes receiving an application message simpler, in some ways, than with TCP sockets. A subtle, but important difference, between TCP and UDP is that UDP preserves message boundaries.","venue":"","listofauthors":"M. J. Donahoo","citations":1,"year":2001,"publisher":"Elsevier","pages":"35-42","volume":null,"number":null,"bibtex":"@incollection{2001,\n\tdoi = {10.1016/b978-155860826-9/50015-x},\n\turl = {https://doi.org/10.1016%2Fb978-155860826-9%2F50015-x},\n\tyear = 2001,\n\tpublisher = {Elsevier},\n\tpages = {35--42},\n\tauthor = {Michael J. Donahoo and Kenneth L. Calvert},\n\ttitle = {Using {UDP} Sockets}\n}","authorsSemantic":[2]},{"id":144,"title":"Scalable multicast representative member selection","doi":"10.1109/INFCOM.2001.916708","description":"In multicast applications requiring receiver feedback, the primary impediment to receiver-set scalability stems from the feedback implosion problem. Some applications only need feedback from a single, representative receiver. For example, an adaptive video server may only need to know the video quality for the worst receiver. Soliciting a response from a receiver with the specific metric value is difficult because a receiver does not know the metric value at other receivers and, therefore, cannot assess if it should report its metric value. Previous work in feedback aggregation solicits a response from any single receiver, not a receiver with some particular metric characteristic. In addition, existing solutions exhibit high application specificity, allowing very little protocol control. We propose adaptation of two existing approaches to representative selection: backoff timers and probabilistic polls. Both adapted approaches are application independent protocols and allow control over the definition of implosion, probability of implosion, and target metric values. We explicitly consider the inherent tradeoff between implosion and delay.","venue":"Proceedings IEEE INFOCOM 2001. Conference on Computer Communications. Twentieth Annual Joint Conference of the IEEE Computer and Communications Society (Cat. No.01CH37213)","listofauthors":"M. J. Donahoo, Sunila R. Ainapure","citations":19,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/infcom.2001.916708},\n\turl = {https://doi.org/10.1109%2Finfcom.2001.916708},\n\tpublisher = {{IEEE}},\n\tauthor = {M.J. Donahoo and S.R. Ainapure},\n\ttitle = {Scalable multicast representative member selection}\n}","authorsSemantic":[2]},{"id":150,"title":"The pocket guide to TCP/IP sockets","doi":null,"description":"\"The Pocket Guide to TCP/IP Sockets is a quick and affordable way to gain the knowledge and skills you need to develop sophisticated and powerful networked-based programs using sockets. Written by two experienced networking instructors, this book provides a series of examples that demonstrate basic sockets techniques for clients and servers. Using plenty of real-world examples, this book is a complete beginner's guide to socket programming and a springboard to more advanced networking topics, including multimedia protocols. * Concise, no-nonsense explanations of issues often troublesome for beginners, including message construction and parsing. * Comprehensive example-based coverage of the most important TCP/IP techniques-including iterative and concurrent servers, timeouts, and asynchronous message processing. * Includes a detailed, easy-to-use reference to the system calls and auxiliary routines that comprise the sockets interface. * A companion Web site provides source code for all example programs in both C and WinSock versions, as well as guidance on running the code on various platforms.","venue":"","listofauthors":"M. J. Donahoo, K. Calvert","citations":9,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":145,"title":"Chapter 3 – Constructing Messages","doi":"10.1016/B978-155860826-9/50014-8","description":"Publisher Summary \nThis chapter discusses some basics of data encoding. It is intended to help avoid some of the common pitfalls of formatting and parsing messages. The Transmission Control Protocol/Internet protocol transports bytes of user data without examining or modifying them. This service allows great flexibility in how they encode their information for transmission. For various reasons, most application protocols are defined in terms of discrete messages made up of fields; each field contains a specific piece of information. The application protocol needs to specify how the data are formatted by the sender, so that the receiver can parse it into its components and extract the meaning of those components. One way to represent the withdrawal and deposit information is to encode the numbers as strings of printable decimal digits, that is, sequences of bytes whose values are determined according to a character encoding such as ASCII. Application protocols typically deal with discrete messages, which are viewed as collections of fields.","venue":"","listofauthors":"M. J. Donahoo","citations":0,"year":2001,"publisher":"Elsevier","pages":"25-33","volume":null,"number":null,"bibtex":"@incollection{2001,\n\tdoi = {10.1016/b978-155860826-9/50014-8},\n\turl = {https://doi.org/10.1016%2Fb978-155860826-9%2F50014-8},\n\tyear = 2001,\n\tpublisher = {Elsevier},\n\tpages = {25--33},\n\tauthor = {Michael J. Donahoo and Kenneth L. Calvert},\n\ttitle = {Constructing Messages}\n}","authorsSemantic":[2]},{"id":146,"title":"Domain Name Service","doi":"10.1016/B978-155860826-9/50018-5","description":"A Transmission Control Protocol (TCP)/Internet Protocol family uses numeric Internet addresses and numeric ports to describe communication endpoints. The host naming service can access information from a wide variety of sources. This makes the protocol implementations efficient, but has at least two drawbacks. First, strings of numbers do not mean much to humans and are therefore, hard to remember. And second, a host's Internet address is tied to the part of the network to which it is connected. This fosters inflexibility in their use. If a host moves to another network or changes Internet service providers (ISPs), in general, its Internet address must change. Two of the primary sources are the domain name system (DNS) and local configuration databases. Local configuration databases are generally operating-system-specific mechanisms for name-to-Internet-address mappings. The source of the information is implementation dependent and may be the DNS, a local configuration database, or some combination of the two. The application programming interface provides a way to obtain information about an application (server), including the port number it uses, by name.","venue":"","listofauthors":"M. J. Donahoo, K. Calvert","citations":26,"year":2001,"publisher":"Elsevier","pages":"103-108","volume":null,"number":null,"bibtex":"@incollection{2001,\n\tdoi = {10.1016/b978-155860826-9/50018-5},\n\turl = {https://doi.org/10.1016%2Fb978-155860826-9%2F50018-5},\n\tyear = 2001,\n\tpublisher = {Elsevier},\n\tpages = {103--108},\n\tauthor = {Michael J. Donahoo and Kenneth L. Calvert},\n\ttitle = {Domain Name Service}\n}","authorsSemantic":[2]},{"id":147,"title":"Chapter 6 – Under the Hood","doi":"10.1016/B978-155860826-9/50017-3","description":"Publisher Summary \nSome of the subtleties of the sockets programming interface are difficult to grasp without some understanding of the data structures associated with each socket in the implementation and some details of how the underlying protocols work. This is especially true of Transmission Control Protocol (TCP) sockets. This chapter describes some of what goes on “under the hood” when creating and using a socket. Knowing about the existence of these data structures and how they are affected by the underlying protocols is useful because they control various aspects of the behavior of the sockets application programming interface (API) functions. The need to copy user data into SendQ for potential retransmission also has implications for performance. The socket interface permits sending and receiving data on a TCP socket only when it is in the connected or “Established” state, that is, only when it has completed the opening handshake message exchange required to establish a TCP connection. The process of matching an incoming packet to a socket is actually the same for both TCP and User Datagram Protocol (UDP).","venue":"","listofauthors":"M. J. Donahoo","citations":0,"year":2001,"publisher":"Elsevier","pages":"87-102","volume":null,"number":null,"bibtex":"@incollection{2001,\n\tdoi = {10.1016/b978-155860826-9/50017-3},\n\turl = {https://doi.org/10.1016%2Fb978-155860826-9%2F50017-3},\n\tyear = 2001,\n\tpublisher = {Elsevier},\n\tpages = {87--102},\n\tauthor = {Michael J. Donahoo and Kenneth L. Calvert},\n\ttitle = {Under the Hood}\n}","authorsSemantic":[2]},{"id":148,"title":"Scaling replica maintenance in intermittently synchronized mobile databases","doi":"10.1145/502585.502661","description":"To avoid the high cost of continuous connectivity, a class of mobile applications employs replicas of shared data that are periodically updated. Updates to these replicas are typically performed on a client-by-client basis--that is, the server individually computes and transmits updates to each client--limiting scalability. By basing updates on replica groups (instead of clients), however, update generation complexity is no longer bound by client population size. Clients then download updates of pertinent groups. Proper group design reduces redundancies in server processing, disk usage and bandwidth usage, and dimininishes the tie between the complexity of updating replicas and the size of the client population. In this paper, we expand on previous work done on group design, include a detailed I/O cost model for update generation, and propose a heuristic-based greedy algorithm for group computation. Experimental results with an adapted commercial replication system demonstrate a significant increase in overall scalability over the client-centric approach.","venue":"CIKM '01","listofauthors":"W. Yee, E. Omiecinski, M. J. Donahoo, S. Navathe","citations":13,"year":2001,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2001,\n\tdoi = {10.1145/502585.502661},\n\turl = {https://doi.org/10.1145%2F502585.502661},\n\tyear = 2001,\n\tpublisher = {{ACM} Press},\n\tauthor = {Wai Gen Yee and Michael J. Donahoo and Edward Omiecinski and Shamkant B. Navathe},\n\ttitle = {Scaling replica maintenance in intermittently synchronized mobile databases}\n}","authorsSemantic":[2]},{"id":149,"title":"A framework for designing update objects to improve server scalability in intermittently synchronized databases","doi":"10.1145/354756.354787","description":"We consider the class of mobile computing applications in which clients naturally operate on shared data without a connection to the server. When appropriate, a server connection is made in order to exchange updates. The update server computes and retransmits these updates on a clientby-client basis; consequently, the complexity of these operations is on the order of the number of clients, limiting scalability. We recently proposed exploiting overlap in client data subscriptions by organizing updates to these subscriptions into groups (with less overlap) instead of on a clientby-client basis. By grouping updates, update processing is performed only once per group, irrespective of the number of clients. Additionally, we may gain bandwidth scalability by employing broadcast delivery since, unlike in the case of the per-client approach, multiple clients may be interested in a group's updates. In this work, we model the operations of such database systems and introduce a framework for evaluation of group design. Since such fragmentation algorithms are computationally intractable, a heuristic graphbased group generation algorithm is speci ed. Performance results executed on a prototype developed using commercially available software are presented.","venue":"CIKM '00","listofauthors":"W. Yee, M. J. Donahoo, S. Navathe","citations":5,"year":2000,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2000,\n\tdoi = {10.1145/354756.354787},\n\turl = {https://doi.org/10.1145%2F354756.354787},\n\tyear = 2000,\n\tpublisher = {{ACM} Press},\n\tauthor = {Wai Gen Yee and Michael J. Donahoo and Shamkant B. Navathe},\n\ttitle = {A framework for designing update objects to improve server scalability in intermittently synchronized databases}\n}","authorsSemantic":[2]},{"id":153,"title":"Multiple-channel multicast scheduling for scalable bulk-data transport","doi":"10.1109/INFCOM.1999.751473","description":"A key technique for allowing servers to handle a large volume of requests for file transfers is to multicast the data to the set of requesting clients. Typically the paths from the server to the clients will be heterogeneous in bandwidth availability. Multiple-channel multicast (MCM) is an approach that can be used to handle this heterogeneity. In this approach the data is multicast over multiple channels, each addressed as a separate multicast group. Each receiver subscribes to a set of channels (i.e., joins the corresponding multicast groups) commensurate with its own rate capabilities. Of particular interest in the design of MCM schemes is the scheduling of data transmission across the multiple channels to accommodate asynchronous requests from clients. In this paper we present and analyze a new multiple-channel multicast approach called partition organization scheduling. The scheme is designed to result in good reception efficiency when compared to existing proposals while improving on their performance when other measures of interest are considered.","venue":"IEEE INFOCOM '99. Conference on Computer Communications. Proceedings. Eighteenth Annual Joint Conference of the IEEE Computer and Communications Societies. The Future is Now (Cat. No.99CH36320)","listofauthors":"M. J. Donahoo, M. Ammar, E. Zegura","citations":28,"year":1999,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1999,\n\tdoi = {10.1109/infcom.1999.751473},\n\turl = {https://doi.org/10.1109%2Finfcom.1999.751473},\n\tyear = 1999,\n\tpublisher = {{IEEE}},\n\tauthor = {M.J. Donahoo and M.H. Ammar and E.W. Zegura},\n\ttitle = {Multiple-channel multicast scheduling for scalable bulk-data transport}\n}","authorsSemantic":[2]},{"id":154,"title":"Toward A Method of Grouping Server Data Fragments for Improving Scalability in Intermittently Synchronized Databases","doi":null,"description":"null","venue":"","listofauthors":"W. Yee, M. J. Donahoo, S. Navathe","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":155,"title":"Method-Specific Knowledge Compilation: Towards Practical Design Support Systems","doi":"10.1007/978-94-011-5121-4_22","description":"Modern knowledge systems for design typically employ multiple problem-solving methods which in turn use different kinds of knowledge. The construction of a heterogeneous knowledge system that can support practical design thus raises two fundamental questions: how to accumulate huge volumes of design information, and how to support heterogeneous design processing? Fortunately, partial answers to both questions exist separately. Legacy databases already contain huge amounts of general-purpose design information. In addition, modern knowledge systems typically characterize the kinds of knowledge needed by specific problem-solving methods quite precisely. This leads us to hypothesize method-specific data-to-knowledge compilation as a potential mechanism for integrating heterogeneous knowledge systems and legacy databases for design. In this paper, first we outline a general computational architecture called HIPED for this integration. Then, we focus on the specific issue of how to convert data accessed from a legacy database into a form appropriate to the problem-solving method used in a heterogeneous knowledge system. We describe an experiment in which a legacy knowledge system called Interactive Kritik is integrated with an ORACLE database using IDI as the communication tool. The limited experiment indicates the computational feasibility of method-specific data-to-knowledge compilation, but also raises additional research issues.","venue":"AID","listofauthors":"J. William Murdock, Ashok K. Goel, M. J. Donahoo, S. Navathe","citations":0,"year":1998,"publisher":"Springer Netherlands","pages":"427-444","volume":null,"number":null,"bibtex":"@incollection{1998,\n\tdoi = {10.1007/978-94-011-5121-4_22},\n\turl = {https://doi.org/10.1007%2F978-94-011-5121-4_22},\n\tyear = 1998,\n\tpublisher = {Springer Netherlands},\n\tpages = {427--444},\n\tauthor = {J. William Murdock and Ashok K. Goel and Michael J. Donahoo and Shamkant Navathe},\n\ttitle = {Method-Specific Knowledge Compilation: Towards Practical Design Support Systems}\n}","authorsSemantic":[2]},{"id":156,"title":"Grouping techniques for update propagation in intermittently connected databases","doi":"10.1109/ICDE.1998.655756","description":"We consider an environment where one or more servers carry databases that are of interest to a community of clients. The clients are only intermittently connected to the server for brief periods of time. Clients carry a part of the database for their own processing and accumulate local updates while disconnected. We call this the Intermittently Connected Database (ICDB) environment. ICDBs have a wide variety of applications including sales force automation, insurance claim processing, and mobile workforces. Our focus is on the problem of update propagation at the server in ICDBs and the associated processing at the clients. The typical client-centric approach involves the communication and processing of updates and transactions on a per-client basis, ignoring the overlap of data between clients. The complexity of this approach is in the order of the number of connecting clients, thereby limiting the scalability of the server. We propose a data-centric approach which clusters data into groups and assigns to each client one or more of these groups. The proposed scheme results in server processing complexity on the order of the number of groups, which we control. We propose various techniques for grouping and discuss the processing required at the clients to enable the grouping approach. While the client-centric approach is expected to significantly degrade with the increasing number of clients, we expect that a properly designed grouping scheme will sustain a number of clients that is significantly larger. A prototype has been developed and performance studies are in progress.","venue":"Proceedings 14th International Conference on Data Engineering","listofauthors":"S. Mahajan, M. J. Donahoo, S. Navathe, M. Ammar, S. Malik","citations":29,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/icde.1998.655756},\n\turl = {https://doi.org/10.1109%2Ficde.1998.655756},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {S. Mahajan and M. Donahoo and S. Navathe and M. Ammar and S. Malik},\n\ttitle = {Grouping techniques for update propagation in intermittently connected databases}\n}","authorsSemantic":[2]},{"id":157,"title":"Application-based enhancement to network-layer multicast","doi":null,"description":"The capabilities of the Internet have increased at a phenomenal rate since its inception; however, the user base and application requirements have increased at a much greater rate. It is clear that simply increasing network and processor bandwidth is not sufficient for handling growth in Internet resource demands. Instead, network and application designers must develop and utilize mechanisms to foster efficient use of the existing resources. One such mechanism is multicast. However, IP multicast is a fairly rudimentary tool providing only low-level, best-effort data-gram delivery service; consequently, simply having multicast as a network service does not guarantee increased efficiency. \nOur work bridges the gap between this rudimentary IP multicast service provided by the Internet and applications needing improved scalability. We accomplish this by proposing application-specific adaptations for the use of multicast at the network, transport, and application layers. In the network layer, we examine the consequences of center placement in center-based routing schemes and provide mechanisms for placement and migration. At the application layer, the existence of network multicast does not guarantee that an application can take advantage of it. We demonstrate the adaptation of a traditionally unicast application—database sharing—to multicast, thus achieving scaling in network and processor performance. For the transport layer, there is a growing consensus in the research community that relying on generic protocols can significantly reduce the efficiency of many applications. This effect seems to be even more pronounced for multicast transport protocols; consequently, protocols may need to be constructed for different types of applications to achieve increased efficiency. We propose a reliable multicast transport protocol tuned specifically for bulk-data applications, such as our multicast-adapted database sharing application. In developing this new transport protocol, we employ a synergy of new and existing techniques in multicast transport. We propose a new sender scheduling approach to deal with heterogeneity in receiver bandwidth capabilities and increase server scalability.","venue":"","listofauthors":"M. J. Donahoo, E. Zegura","citations":4,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":158,"title":"Center selection and migration for wide-area multicast routing","doi":"10.3233/JHS-1997-6205","description":"Interdomain multicast routing is a challenge, requiring a scalable solution that is suitable for diverse application characteristics. Center-based routing trees overcome some of the limitations associated with traditional shortest-path tree routing, however they present a new set of routing problems. Speciically, the choice of the center router can be critical in determining the multicast routing performance. We present and assess algorithms to solve two problems: selection of an initial center router, and migration of the center router in response to dynamic changes in the group membership during the application lifetime. These two problems, and our proposed solutions, may have more general applicability in wide-area networking, e.g., in placing shared resources in the network or estimating bandwidth utilization for multicast distribution.","venue":"J. High Speed Networks","listofauthors":"M. J. Donahoo, K. Calvert, E. Zegura","citations":12,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":" @article{Donahoo_Calvert_Zegura_1997, place={NL}, title={Center selection and migration for wide-area multicast routing}, volume={6}, ISSN={09266801}, url={https://doi.org/10.3233/JHS-1997-6205}, DOI={10.3233/JHS-1997-6205}, abstractNote={Interdomain multicast routing is a challenge, requiring a scalable solution that is suitable for diverse application characteristics. Center-based routing trees overcome some of the limitations associated with traditional shortest-path tree routing, however, they present a new set of routing problems. Specifically, the choice of the center router can be critical in determining the multicast routing performance. We present and assess algorithms to solve two problems: selection of an initial center router, and migration of the center router in response to dynamic chauges in the group membership during the application lifetime. These two problems, and our proposed solutions, may have more general applicability in wide-area networking, e.g., in placing shared resources in the network or estimating bandwidth utilization for multicast distribution.}, number={2}, journal={Journal of High Speed Networks}, publisher={IOS Press}, author={Donahoo, Michael J. and Calvert, Kenneth L. and Zegura, Ellen W.}, year={1997}, pages={141–164} }\n","authorsSemantic":[2]},{"id":159,"title":"From Data to Knowledge: Method-Specific Transformations","doi":"10.1007/3-540-63614-5_40","description":"Generality and scale are important but difficult issues in knowledge engineering. At the root of the difficulty lie two hard questions: how to accumulate huge volumes of knowledge, and how to support heterogeneous knowledge and processing? One answer to the first question is to reuse legacy knowledge systems, integrate knowledge systems with legacy databases, and enable sharing of the databases by multiple knowledge systems. We present an architecture called HIPED for realizing this answer. HIPED converts the second question above into a new form: how to convert data accessed from a legacy database into a form appropriate to the processing method used in a legacy knowledge system? One answer to this reformed question is to use method-specific transformation of data into knowledge. We describe an experiment in which a legacy knowledge system called Interactive Kritik is integrated with an ORACLE database using IDI as the communication tool. The experiment indicates the computational feasibility of method-specific data-to-knowledge transformations.","venue":"ISMIS","listofauthors":"M. J. Donahoo, J. William Murdock, Ashok K. Goel, S. Navathe, E. Omiecinski","citations":2,"year":1997,"publisher":"Springer Berlin Heidelberg","pages":"411-420","volume":null,"number":null,"bibtex":"@incollection{1997,\n\tdoi = {10.1007/3-540-63614-5_40},\n\turl = {https://doi.org/10.1007%2F3-540-63614-5_40},\n\tyear = 1997,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {411--420},\n\tauthor = {Michael J. Donahoo and J. William Murdock and Ashok K. Goel and Shamkant Navathe and Edward Omiecinski},\n\ttitle = {From data to knowledge: method-specific transformations}\n}","authorsSemantic":[2]},{"id":160,"title":"A quantitative comparison of graph-based models for Internet topology","doi":"10.1109/90.650138","description":"Graphs are commonly used to model the topological structure of internetworks in order to study problems ranging from routing to resource reservation. A variety of graphs are found in the literature, including fixed topologies such as rings or stars, \"well-known\" topologies such as the ARPAnet, and randomly generated topologies. While many researchers rely upon graphs for analytic and simulation studies, there has been little analysis of the implications of using a particular model or how the graph generation method may affect the results of such studies. Further, the selection of one generation method over another is often arbitrary, since the differences and similarities between methods are not well understood. This paper considers the problem of generating and selecting graphs that reflect the properties of real internetworks. We review generation methods in common use and also propose several new methods. We consider a set of metrics that characterize the graphs produced by a method, and we quantify similarities and differences among several generation methods with respect to these metrics. We also consider the effect of the graph model in the context of a specific problem, namely multicast routing.","venue":"TNET","listofauthors":"E. Zegura, K. Calvert, M. J. Donahoo","citations":545,"year":1997,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"770-783","volume":"5","number":"6","bibtex":"@article{1997,\n\tdoi = {10.1109/90.650138},\n\turl = {https://doi.org/10.1109%2F90.650138},\n\tyear = 1997,\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {5},\n\tnumber = {6},\n\tpages = {770--783},\n\tauthor = {E.W. Zegura and K.L. Calvert and M.J. Donahoo},\n\ttitle = {A quantitative comparison of graph-based models for Internet topology}\n}","authorsSemantic":[2]},{"id":161,"title":"Towards intelligent integration of heterogeneous information sources","doi":"10.1007/978-1-4615-1803-7_20","description":"Current methodologies for information integration are inadequate for solving the problem of integration of large scale, distributed information sources (e.g. databases, free-form text, simulation etc.). The existing approaches are either too restrictive and complicated as in the “federated” (global model) approach or do not provide the necessary functionality as in the “multidatabase” approach. We propose a hybrid approach combining the advantages of both the federated and multidatabase techniques which we believe provide the most feasible avenue for large scale integration. Under our architecture, the individual data site administrators provide anaugmented export schemaspecifying knowledge about the sources of data (where data exists), their structure (underlying data model or file structure), their content (what data exists), and their relationships (how the data relates to other information in its domain). The augmented export schema from each information source provides an intelligent agent, called the “mediator”, knowledge which can be used to infer information on some of the existing inter-system relationships. This knowledge can then be used to generate a partially integrated, global view of the data.","venue":"","listofauthors":"S. Navathe, M. J. Donahoo","citations":22,"year":1996,"publisher":"Springer US","pages":"275-282","volume":null,"number":null,"bibtex":"@incollection{1996,\n\tdoi = {10.1007/978-1-4615-1803-7_20},\n\turl = {https://doi.org/10.1007%2F978-1-4615-1803-7_20},\n\tyear = 1996,\n\tpublisher = {Springer {US}},\n\tpages = {275--282},\n\tauthor = {Shamkant B. Navathe and Michael J. Donahoo},\n\ttitle = {Towards Intelligent Integration of Heterogeneous Information Sources}\n}","authorsSemantic":[2]},{"id":162,"title":"Core Migration for Dynamic Multicast Routing","doi":null,"description":"null","venue":"","listofauthors":"M. J. Donahoo, E. Zegura","citations":50,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":163,"title":"Core selection methods for multicast routing","doi":"10.1109/ICCCN.1995.540184","description":"Multicast routing is an important topic of both theoretical and practical interest. Several proposed multicast routing algorithms involve the designation of one or more network nodes as the \"center\" of the routing tree for each multicast group. The choice of this designated router (which we refer to as the \"core\") influences the shape of the multicast routing tree, and thus influences the performance of the routing scheme. We investigate the relationship between the choice of core and three performance measures. Specifically, we compare various methods of selecting a core with respect to their effect on the bandwidth, delay, and traffic concentration. We conclude that simple methods are adequate for widely distributed groups, but that the addition of group information can be leveraged to improve performance especially when the group is small or exhibits a high degree of locality. We also conclude that core choice can be used to control traffic concentration, in fact traffic concentration effects can be ameliorated by appropriate core choice policies.","venue":"Proceedings of Fourth International Conference on Computer Communications and Networks - IC3N'95","listofauthors":"K. Calvert, E. Zegura, M. J. Donahoo","citations":110,"year":0,"publisher":"IEEE Comput. Soc. Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/icccn.1995.540184},\n\turl = {https://doi.org/10.1109%2Ficccn.1995.540184},\n\tpublisher = {{IEEE} Comput. Soc. Press},\n\tauthor = {K.L. Calvert and E.W. Zegura and M.J. Donahoo},\n\ttitle = {Core selection methods for multicast routing}\n}","authorsSemantic":[2]},{"id":164,"title":"Resolving Result Set Contention in Heterogeneous Library Information Systems","doi":null,"description":"Federated databases are one of the challenges for database system builders. In an e ort to create these systems, current implementations require every member of the federation to agree to a particular standard. We examine query optimization under the Z39.50 standard for heterogeneous library information systems, looking speci cally at the problem of managing temporary relations within a library system limiting the number of these relations held by a single user. We propose a strategy which uses knowledge of the current user query and an approximation of a set of future user queries in order to choose which temporary relation is the least likely to be useful in answer the current and future queries. Simulation results indicate that this strategy, called Look-Ahead, performs better than strategies based on operating system concepts. Work supported in part by a summer sabbatical from Baylor University","venue":"","listofauthors":"G. Speegle, M. J. Donahoo","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[2]},{"id":166,"title":"Using hierarchical similarity to examine the genetics of Behçet’s disease","doi":"10.1186/s13104-021-05767-6","description":"Objective Behçet’s disease (BD) is a multisystem inflammatory disease that affects patients along the historic silk road. Thus far, the pathogenesis of the disease has proved elusive due to the complex genetic interactions of the disease. In this paper, we seek to clarify the genetic factors of the disease while also uncovering other diseases of interest that present with a similar genotype as BD. Results To do this, we employ a convergent functional genomics approach by leveraging the hierarchical similarity tool available in Geneweaver. Through our analysis, we were able to ascertain 7 BD consensus genes and 16 autoimmune diseases with genetic overlap with BD. The results of our study will inform further research into the pathogenesis of Behçet’s disease.","venue":"BMC research notes","listofauthors":"S. Shenoi, E. Baker","citations":0,"year":2021,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"14","number":"1","bibtex":"@article{2021,\n\tdoi = {10.1186/s13104-021-05767-6},\n\turl = {https://doi.org/10.1186%2Fs13104-021-05767-6},\n\tyear = 2021,\n\tmonth = {sep},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {14},\n\tnumber = {1},\n\tauthor = {Samuel J. Shenoi and Erich J. Baker},\n\ttitle = {Using hierarchical similarity to examine the genetics of Beh{\\c{c}}et's disease}\n}","authorsSemantic":[3]},{"id":167,"title":"Behçet’s Disease Network Analysis: A Convergent Functional Genomics Approach","doi":"10.1101/2021.04.06.438717","description":"Behcet9s disease (BD) is a multisystem inflammatory disease that effects patients along the historic silk road. Thus far, the patheogensis of the disease has proved elusive due to the complex genetic interactions and unknown environmental or viral triggering factors of the disease. In this paper, we seek to clarify the gentic factors of the disease while also uncovering other diseases of interest that present with a similar genotype as BD. To do this, we employ a computational functional genomics approach by leveraging the hierarchical similarity tool available in Geneweaver. Through our analysis, we were able to ascertain 7 BD consensus genes and 16 autoimmune diseases with genetic overlap with BD. The results of our study will inform further research into the patheogenesis of Behcet9s Disease.","venue":"","listofauthors":"Shenoi Sj, E. Baker","citations":0,"year":2021,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2021,\n\tdoi = {10.1101/2021.04.06.438717},\n\turl = {https://doi.org/10.1101%2F2021.04.06.438717},\n\tyear = 2021,\n\tmonth = {apr},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {Samuel J Shenoi and Erich J Baker},\n\ttitle = {Using Hierarchical Similarity To Examine The Genetics of Beh{\\c{c}}et's Disease}\n}","authorsSemantic":[3]},{"id":168,"title":"Integration of evidence across human and model organism studies: A meeting report","doi":"10.1111/gbb.12738","description":"The National Institute on Drug Abuse and Joint Institute for Biological Sciences at the Oak Ridge National Laboratory hosted a meeting attended by a diverse group of scientists with expertise in substance use disorders (SUDs), computational biology, and FAIR (Findability, Accessibility, Interoperability, and Reusability) data sharing. The meeting's objective was to discuss and evaluate better strategies to integrate genetic, epigenetic, and 'omics data across human and model organisms to achieve deeper mechanistic insight into SUDs. Specific topics were to (a) evaluate the current state of substance use genetics and genomics research and fundamental gaps, (b) identify opportunities and challenges of integration and sharing across species and data types, (c) identify current tools and resources for integration of genetic, epigenetic, and phenotypic data, (d) discuss steps and impediment related to data integration, and (e) outline future steps to support more effective collaboration—particularly between animal model research communities and human genetics and clinical research teams. This review summarizes key facets of this catalytic discussion with a focus on new opportunities and gaps in resources and knowledge on SUDs.","venue":"Genes, brain, and behavior","listofauthors":"R. Palmer, E. Johnson, H. Won, R. Polimanti, M. Kapoor, Apurva S. Chitre, M. Bogue, Chelsie E. Benca-Bachman, C. Parker, Oana Ursu, A. Verma, T. Reynolds, J. Ernst, M. Bray, Soo Bin Kwon, D. Lai, B. Quach, N. Gaddis, L. Saba, Hao Chen, M. Hawrylycz, Shan Zhang, Yuan Zhou, Spencer B Mahaffey, Christian Fischer, S. Sanchez-Roige, A. Bandrowski, Lu Qing, Li Shen, V. Philip, J. Gelernter, L. Bierut, D. Hancock, H. Edenberg, E. Johnson, E. Nestler, Peter B. Barr, P. Prins, Desmond J. Smith, S. Akbarian, T. Thorgeirsson, Dave Walton, E. Baker, Daniel Jacobson, A. Palmer, M. Miles, E. Chesler, Jake Emerson, A. Agrawal, M. Martone, Robert W. Williams","citations":0,"year":2021,"publisher":"Wiley","pages":null,"volume":null,"number":null,"bibtex":"@article{2021,\n\tdoi = {10.1111/gbb.12738},\n\turl = {https://doi.org/10.1111%2Fgbb.12738},\n\tyear = 2021,\n\tmonth = {jun},\n\tpublisher = {Wiley},\n\tauthor = {Rohan H. C. Palmer and Emma C. Johnson and Hyejung Won and Renato Polimanti and Manav Kapoor and Apurva Chitre and Molly A. Bogue and Chelsie E. Benca-Bachman and Clarissa C. Parker and Anurag Verma and Timothy Reynolds and Jason Ernst and Michael Bray and Soo Bin Kwon and Dongbing Lai and Bryan C. Quach and Nathan C. Gaddis and Laura Saba and Hao Chen and Michael Hawrylycz and Shan Zhang and Yuan Zhou and Spencer Mahaffey and Christian Fischer and Sandra Sanchez-Roige and Anita Bandrowski and Qing Lu and Li Shen and Vivek Philip and Joel Gelernter and Laura J. Bierut and Dana B. Hancock and Howard J. Edenberg and Eric O. Johnson and Eric J. Nestler and Peter B. Barr and Pjotr Prins and Desmond J. Smith and Schahram Akbarian and Thorgeir Thorgeirsson and Dave Walton and Erich Baker and Daniel Jacobson and Abraham A. Palmer and Michael Miles and Elissa J. Chesler and Jake Emerson and Arpana Agrawal and Maryann Martone and Robert W. Williams},\n\ttitle = {Integration of evidence across human and model organism studies: A meeting report}\n}","authorsSemantic":[3]},{"id":169,"title":"Interpretation of psychiatric genome-wide association studies with multispecies heterogeneous functional genomic data integration","doi":"10.1038/s41386-020-00795-5","description":"Genome-wide association studies and other discovery genetics methods provide a means to identify previously unknown biological mechanisms underlying behavioral disorders that may point to new therapeutic avenues, augment diagnostic tools, and yield a deeper understanding of the biology of psychiatric conditions. Recent advances in psychiatric genetics have been made possible through large-scale collaborative efforts. These studies have begun to unearth many novel genetic variants associated with psychiatric disorders and behavioral traits in human populations. Significant challenges remain in characterizing the resulting disease-associated genetic variants and prioritizing functional follow-up to make them useful for mechanistic understanding and development of therapeutics. Model organism research has generated extensive genomic data that can provide insight into the neurobiological mechanisms of variant action, but a cohesive effort must be made to establish which aspects of the biological modulation of behavioral traits are evolutionarily conserved across species. Scalable computing, new data integration strategies, and advanced analysis methods outlined in this review provide a framework to efficiently harness model organism data in support of clinically relevant psychiatric phenotypes.","venue":"Neuropsychopharmacology","listofauthors":"T. Reynolds, E. Johnson, S. Huggett, J. Bubier, R. Palmer, A. Agrawal, E. Baker, E. Chesler","citations":4,"year":2020,"publisher":"Springer Science and Business Media LLC","pages":"86-97","volume":"46","number":"1","bibtex":"@article{2020,\n\tdoi = {10.1038/s41386-020-00795-5},\n\turl = {https://doi.org/10.1038%2Fs41386-020-00795-5},\n\tyear = 2020,\n\tmonth = {aug},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {46},\n\tnumber = {1},\n\tpages = {86--97},\n\tauthor = {Timothy Reynolds and Emma C. Johnson and Spencer B. Huggett and Jason A. Bubier and Rohan H. C. Palmer and Arpana Agrawal and Erich J. Baker and Elissa J. Chesler},\n\ttitle = {Interpretation of psychiatric genome-wide association studies with multispecies heterogeneous functional genomic data integration}\n}","authorsSemantic":[3]},{"id":170,"title":"Finding human gene-disease associations using a Network Enhanced Similarity Search (NESS) of multi-species heterogeneous functional genomics data","doi":"10.1101/2020.03.11.987552","description":"Disease diagnosis and treatment is challenging in part due to the misalignment of diagnostic categories with the underlying biology of disease. The evaluation of large-scale genomic experimental datasets is a compelling approach to refining the classification of biological concepts, such as disease. Well-established approaches, some of which rely on information theory or network analysis, quantitatively assess relationships among biological entities using gene annotations, structured vocabularies, and curated data sources. However, the gene annotations used in these evaluations are often sparse, potentially biased due to uneven study and representation in the literature, and constrained to the single species from which they were derived. In order to overcome these deficiencies inherent in the structure and sparsity of these annotated datasets, we developed a novel Network Enhanced Similarity Search (NESS) tool which takes advantage of multi-species networks of heterogeneous data to bridge sparsely populated datasets. NESS employs a random walk with restart algorithm across harmonized multi-species data, effectively compensating for sparsely populated and noisy genomic studies. We further demonstrate that it is highly resistant to spurious or sparse datasets and generates significantly better recapitulation of ground truth biological pathways than other similarity metrics alone. Furthermore, since NESS has been deployed as an embedded tool in the GeneWeaver environment, it can rapidly take advantage of curated multi-species networks to provide informative assertions of relatedness of any pair of biological entities or concepts, e.g., gene-gene, gene-disease, or phenotype-disease associations. NESS ultimately enables multi-species analysis applications to leverage model organism data to overcome the challenge of data sparsity in the study of human disease. Availability and Implementation Implementation available at https://geneweaver.org/ness. Source code freely available at https://github.com/treynr/ness. Author summary Finding consensus among large-scale genomic datasets is an ongoing challenge in the biomedical sciences. Harmonizing and analyzing such data is important because it allows researchers to mitigate the idiosyncrasies of experimental systems, alleviate study biases, and augment sparse datasets. Additionally, it allows researchers to utilize animal model studies and cross-species experiments to better understand biological function in health and disease. Here we provide a tool for integrating and analyzing heterogeneous functional genomics data using a graph-based model. We show how this type of analysis can be used to identify similar relationships among biological entities such as genes, processes, and disease through shared genomic associations. Our results indicate this approach is effective at reducing biases caused by sparse and noisy datasets. We show how this type of analysis can be used to aid the classification gene function and prioritization of genes involved in substance use disorders. In addition, our analysis reveals genes and biological pathways with shared association to multiple, co-occurring substance use disorders.","venue":"","listofauthors":"T. Reynolds, J. Bubier, M. Langston, E. Chesler, E. Baker","citations":1,"year":2020,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2020,\n\tdoi = {10.1101/2020.03.11.987552},\n\turl = {https://doi.org/10.1101%2F2020.03.11.987552},\n\tyear = 2020,\n\tmonth = {mar},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {Timothy Reynolds and Jason A. Bubier and Michael A. Langston and Elissa J. Chesler and Erich J. Baker},\n\ttitle = {Finding human gene-disease associations using a Network Enhanced Similarity Search ({NESS}) of multi-species heterogeneous functional genomics data}\n}","authorsSemantic":[3]},{"id":171,"title":"Integration of heterogeneous functional genomics data in gerontology research to find genes and pathway underlying aging across species","doi":"10.1371/journal.pone.0214523","description":"Understanding the biological mechanisms behind aging, lifespan and healthspan is becoming increasingly important as the proportion of the world's population over the age of 65 grows, along with the cost and complexity of their care. BigData oriented approaches and analysis methods enable current and future bio-gerontologists to synthesize, distill and interpret vast, heterogeneous data from functional genomics studies of aging. GeneWeaver is an analysis system for integration of data that allows investigators to store, search, and analyze immense amounts of data including user-submitted experimental data, data from primary publications, and data in other databases. Aging related genome-wide gene sets from primary publications were curated into this system in concert with data from other model-organism and aging-specific databases, and applied to several questions in genrontology using. For example, we identified Cd63 as a frequently represented gene among aging-related genome-wide results. To evaluate the role of Cd63 in aging, we performed RNAi knockdown of the C. elegans ortholog, tsp-7, demonstrating that this manipulation is capable of extending lifespan. The tools in GeneWeaver enable aging researchers to make new discoveries into the associations between the genes, normal biological processes, and diseases that affect aging, healthspan, and lifespan.","venue":"PloS one","listofauthors":"J. Bubier, G. Sutphin, T. Reynolds, R. Korstanje, A. Fuksman-Kumpa, E. Baker, M. Langston, E. Chesler","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[3]},{"id":192,"title":"Increased levels of the acetaldehyde-derived DNA adduct N 2-ethyldeoxyguanosine in oral mucosa DNA from Rhesus monkeys exposed to alcohol.","doi":"10.1093/mutage/gew016","description":"Alcohol is a human carcinogen. A causal link has been established between alcohol drinking and cancers of the upper aerodigestive tract, colon, liver and breast. Despite this established association, the underlying mechanisms of alcohol-induced carcinogenesis remain unclear. Various mechanisms may come into play depending on the type of cancer; however, convincing evidence supports the concept that ethanol's major metabolite acetaldehyde may play a major role. Acetaldehyde can react with DNA forming adducts which can serve as biomarkers of carcinogen exposure and potentially of cancer risk. The major DNA adduct formed from this reaction is N (2)-ethylidenedeoxyguanosine, which can be quantified as its reduced form N (2)-ethyl-dG by LC-ESI-MS/MS. To investigate the potential use of N (2)-ethyl-dG as a biomarker of alcohol-induced DNA damage, we quantified this adduct in DNA from the oral, oesophageal and mammary gland tissues from rhesus monkeys exposed to alcohol drinking over their lifetimes and compared it to controls. N (2)-Ethyl-dG levels were significantly higher in the oral mucosa DNA of the exposed animals. Levels of the DNA adduct measured in the oesophageal mucosa of exposed animals were not significantly different from controls. A correlation between the levels measured in the oral and oesophageal DNA, however, was observed, suggesting a common source of formation of the DNA adducts. N (2) -Ethyl-dG was measured in mammary gland DNA from a small cohort of female animals, but no difference was observed between exposed animals and controls. These results support the hypothesis that acetaldehyde induces DNA damage in the oral mucosa of alcohol-exposed animals and that it may play role in the alcohol-induced carcinogenic process. The decrease of N (2)-ethyl-dG levels in exposed tissues further removed from the mouth also suggests a role of alcohol metabolism in the oral cavity, which may be considered separately from ethanol liver metabolism in the investigation of ethanol-related cancer risk.","venue":"Mutagenesis","listofauthors":"S. Balbo, Rita Pilar Cervera Juanes, S. Khariwala, E. Baker, J. Daunais, K. Grant","citations":15,"year":2016,"publisher":"Oxford University Press (OUP)","pages":"553-558","volume":"31","number":"5","bibtex":"@article{2016,\n\tdoi = {10.1093/mutage/gew016},\n\turl = {https://doi.org/10.1093%2Fmutage%2Fgew016},\n\tyear = 2016,\n\tmonth = {apr},\n\tpublisher = {Oxford University Press ({OUP})},\n\tvolume = {31},\n\tnumber = {5},\n\tpages = {553--558},\n\tauthor = {Silvia Balbo and Rita Cervera Juanes and Samir Khariwala and Erich J. Baker and James B. Daunais and Kathleen A. Grant},\n\ttitle = {Increased levels of the acetaldehyde-derived {DNA} {adductN}2-ethyldeoxyguanosine in oral mucosa {DNA} from Rhesus monkeys exposed to alcohol}\n}","authorsSemantic":[3]},{"id":172,"title":"On Finding and Enumerating Maximal and Maximum k-Partite Cliques in k-Partite Graphs","doi":"10.3390/a12010023","description":"Let k denote an integer greater than 2, let G denote a k-partite graph, and let S denote the set of all maximal k-partite cliques in G. Several open questions concerning the computation of S are resolved. A straightforward and highly-scalable modification to the classic recursive backtracking approach of Bron and Kerbosch is first described and shown to run in O(3n/3) time. A series of novel graph constructions is then used to prove that this bound is best possible in the sense that it matches an asymptotically tight upper limit on |S|. The task of identifying a vertex-maximum element of S is also considered and, in contrast with the k = 2 case, shown to be NP-hard for every k ≥ 3. A special class of k-partite graphs that arises in the context of functional genomics and other problem domains is studied as well and shown to be more readily solvable via a polynomial-time transformation to bipartite graphs. Applications, limitations, potentials for faster methods, heuristic approaches, and alternate formulations are also addressed.","venue":"Algorithms","listofauthors":"C. Phillips, K. Wang, E. Baker, J. Bubier, E. Chesler, M. Langston","citations":6,"year":2019,"publisher":"MDPI AG","pages":"23","volume":"12","number":"1","bibtex":"@article{2019,\n\tdoi = {10.3390/a12010023},\n\turl = {https://doi.org/10.3390%2Fa12010023},\n\tyear = 2019,\n\tmonth = {jan},\n\tpublisher = {{MDPI} {AG}},\n\tvolume = {12},\n\tnumber = {1},\n\tpages = {23},\n\tauthor = {Charles Phillips and Kai Wang and Erich Baker and Jason Bubier and Elissa Chesler and Michael Langston},\n\ttitle = {On Finding and Enumerating Maximal and Maximum k-Partite Cliques in k-Partite Graphs}\n}","authorsSemantic":[3]},{"id":173,"title":"Time for a Drink? A Mathematical Model of Non-human Primate Alcohol Consumption","doi":"10.3389/fams.2019.00006","description":"We simulate a non-human primate’s alcohol drinking pattern in order to better understand temporal patterning of alcoholic drinks that can lead to the excessive intakes associated with alcohol use disorder. A stochastic mathematical model of alcohol consumption pattern is developed, where model parameters are calibrated to an individual monkey’s drinking history. The model predicts a time series that simulates a monkey’s alcohol intake in time, and we analyze this drinking pattern to understand the variations in day and night drinking, the lengths of drinks (intake in 5 or more consecutive secs), and lengths of bouts (1 or more drinks per 5 min occasion). This time series can predict a lifetime categorical drinking level (light, binge, heavy, or very heavy), thus correlating an individual monkey’s parameters with distinct long term drinking classifications.","venue":"Front. Appl. Math. Stat.","listofauthors":"Sharon Moore, A. Radunskaya, Elizabeth Zollinger, K. Grant, S. Gonzales, E. Baker","citations":0,"year":2019,"publisher":"Frontiers Media SA","pages":null,"volume":"5","number":null,"bibtex":"@article{2019,\n\tdoi = {10.3389/fams.2019.00006},\n\turl = {https://doi.org/10.3389%2Ffams.2019.00006},\n\tyear = 2019,\n\tmonth = {feb},\n\tpublisher = {Frontiers Media {SA}},\n\tvolume = {5},\n\tauthor = {Sharon Moore and Ami Radunskaya and Elizabeth Zollinger and Kathleen A. Grant and Steven Gonzales and Erich J. Baker},\n\ttitle = {Time for a Drink? A Mathematical Model of Non-human Primate Alcohol Consumption}\n}","authorsSemantic":[3]},{"id":174,"title":"Chronic ethanol drinking increases during the luteal menstrual cycle phase in rhesus monkeys: implication of progesterone and related neurosteroids","doi":"10.1007/s00213-019-5168-9","description":"RationaleSporadic reports of alcohol consumption being linked to menstrual cycle phase highlight the need to consider hormonally characterized menstrual cycle phase in understanding the sex-specific effects of risk for alcohol drinking in women.ObjectivesWe investigated the association between menstrual cycle phase, characterized by circulating progesterone and menses, with accurate daily alcohol intakes in rhesus monkeys, and the contribution of progesterone derived neuroactive steroids to cycle-related alcohol drinking.MethodsMenses (daily) and progesterone (2–3×/week) were obtained in female monkeys (n = 8, 5 ethanol, 3 control) for 12–18 months. Ethanol monkeys were then induced to drink ethanol (4% w/v; 3 months) and given 22 h/day access to ethanol and water for approximately 1 year. In selected cycles, a panel of neuroactive steroids were assayed during follicular and luteal phases from pre-ethanol and ethanol exposure.ResultsThere were minimal to no effects of ethanol on menstrual cycle length, progesterone levels, and follicular or luteal phase length. The monkeys drank more ethanol during the luteal phase, compared to the follicular phase, and ethanol intake was highest in the late luteal phase when progesterone declines rapidly. Two neuroactive steroids were higher during the luteal phase versus the follicular phase, and several neuroactive steroids were higher in the pre- vs. post-ethanol drinking menstrual cycles.ConclusionsThis is the first study to show that normal menstrual cycle fluctuations in progesterone, particularly during the late luteal phase, can modulate ethanol intake. Two of 11 neuroactive steroids were selectively associated with the effect of cycle progesterone on ethanol drinking, suggesting possible links to CNS mechanisms of ethanol intake control.","venue":"Psychopharmacology","listofauthors":"B. Dozier, C. Stull, E. Baker, M. M. Ford, J. Jensen, D. Finn, K. Grant","citations":7,"year":2019,"publisher":"Springer Science and Business Media LLC","pages":"1817-1828","volume":"236","number":"6","bibtex":"@article{2019,\n\tdoi = {10.1007/s00213-019-5168-9},\n\turl = {https://doi.org/10.1007%2Fs00213-019-5168-9},\n\tyear = 2019,\n\tmonth = {jan},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {236},\n\tnumber = {6},\n\tpages = {1817--1828},\n\tauthor = {Brandy L. Dozier and Cara A. Stull and Erich J. Baker and Matthew M. Ford and Jeremiah P. Jensen and Deborah A. Finn and Kathleen A. Grant},\n\ttitle = {Chronic ethanol drinking increases during the luteal menstrual cycle phase in rhesus monkeys: implication of progesterone and related neurosteroids}\n}","authorsSemantic":[3]},{"id":175,"title":"Curating gene sets: challenges and opportunities for integrative analysis","doi":"10.1093/database/baz036","description":"Abstract Genomic data interpretation often requires analyses that move from a gene-by-gene focus to a focus on sets of genes that are associated with biological phenomena such as molecular processes, phenotypes, diseases, drug interactions or environmental conditions. Unique challenges exist in the curation of gene sets beyond the challenges in curation of individual genes. Here we highlight a literature curation workflow whereby gene sets are curated from peer-reviewed published data into GeneWeaver (GW), a data repository and analysis platform. We describe the system features that allow for a flexible yet precise curation procedure. We illustrate the value of curation by gene sets through analysis of independently curated sets that relate to the integrated stress response, showing that sets curated from independent sources all share significant Jaccard similarity. A suite of reproducible analysis tools is provided in GW as services to carry out interactive functional investigation of user-submitted gene sets within the context of over 150 000 gene sets constructed from publicly available resources and published gene lists. A curation interface supports the ability of users to design and maintain curation workflows of gene sets, including assigning, reviewing and releasing gene sets within a curation project context.","venue":"Database J. Biol. Databases Curation","listofauthors":"J. Bubier, D. Hill, Gaurab Mukherjee, T. Reynolds, E. Baker, Alexander Berger, Jake Emerson, J. Blake, E. Chesler","citations":4,"year":2019,"publisher":"Oxford University Press (OUP)","pages":null,"volume":"2019","number":null,"bibtex":"@article{2019,\n\tdoi = {10.1093/database/baz036},\n\turl = {https://doi.org/10.1093%2Fdatabase%2Fbaz036},\n\tyear = 2019,\n\tmonth = {jan},\n\tpublisher = {Oxford University Press ({OUP})},\n\tvolume = {2019},\n\tauthor = {Jason Bubier and David Hill and Gaurab Mukherjee and Timothy Reynolds and Erich J Baker and Alexander Berger and Jake Emerson and Judith A Blake and Elissa J Chesler},\n\ttitle = {Curating gene sets: challenges and opportunities for integrative analysis}\n}","authorsSemantic":[3]},{"id":176,"title":"Cisplatin-resistant triple-negative breast cancer subtypes: multiple mechanisms of resistance","doi":"10.1186/s12885-019-6278-9","description":"Understanding mechanisms underlying specific chemotherapeutic responses in subtypes of cancer may improve identification of treatment strategies most likely to benefit particular patients. For example, triple-negative breast cancer (TNBC) patients have variable response to the chemotherapeutic agent cisplatin. Understanding the basis of treatment response in cancer subtypes will lead to more informed decisions about selection of treatment strategies. In this study we used an integrative functional genomics approach to investigate the molecular mechanisms underlying known cisplatin-response differences among subtypes of TNBC. To identify changes in gene expression that could explain mechanisms of resistance, we examined 102 evolutionarily conserved cisplatin-associated genes, evaluating their differential expression in the cisplatin-sensitive, basal-like 1 (BL1) and basal-like 2 (BL2) subtypes, and the two cisplatin-resistant, luminal androgen receptor (LAR) and mesenchymal (M) subtypes of TNBC. We found 20 genes that were differentially expressed in at least one subtype. Fifteen of the 20 genes are associated with cell death and are distributed among all TNBC subtypes. The less cisplatin-responsive LAR and M TNBC subtypes show different regulation of 13 genes compared to the more sensitive BL1 and BL2 subtypes. These 13 genes identify a variety of cisplatin-resistance mechanisms including increased transport and detoxification of cisplatin, and mis-regulation of the epithelial to mesenchymal transition. We identified gene signatures in resistant TNBC subtypes indicative of mechanisms of cisplatin. Our results indicate that response to cisplatin in TNBC has a complex foundation based on impact of treatment on distinct cellular pathways. We find that examination of expression data in the context of heterogeneous data such as drug-gene interactions leads to a better understanding of mechanisms at work in cancer therapy response.","venue":"BMC Cancer","listofauthors":"D. Hill, A. Harper, J. Malcolm, M. McAndrews, S. Mockus, S. Patterson, T. Reynolds, E. Baker, C. Bult, E. Chesler, J. Blake","citations":22,"year":2019,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"19","number":"1","bibtex":"@article{2019,\n\tdoi = {10.1186/s12885-019-6278-9},\n\turl = {https://doi.org/10.1186%2Fs12885-019-6278-9},\n\tyear = 2019,\n\tmonth = {nov},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {19},\n\tnumber = {1},\n\tauthor = {David P. Hill and Akeena Harper and Joan Malcolm and Monica S. McAndrews and Susan M. Mockus and Sara E. Patterson and Timothy Reynolds and Erich J. Baker and Carol J. Bult and Elissa J. Chesler and Judith A. Blake},\n\ttitle = {Cisplatin-resistant triple-negative breast cancer subtypes: multiple mechanisms of resistance}\n}","authorsSemantic":[3]},{"id":193,"title":"PredSTP: a highly accurate SVM based model to predict sequential cystine stabilized peptides","doi":"10.1186/s12859-015-0633-x","description":"BackgroundNumerous organisms have evolved a wide range of toxic peptides for self-defense and predation. Their effective interstitial and macro-environmental use requires energetic and structural stability. One successful group of these peptides includes a tri-disulfide domain arrangement that offers toxicity and high stability. Sequential tri-disulfide connectivity variants create highly compact disulfide folds capable of withstanding a variety of environmental stresses. Their combination of toxicity and stability make these peptides remarkably valuable for their potential as bio-insecticides, antimicrobial peptides and peptide drug candidates. However, the wide sequence variation, sources and modalities of group members impose serious limitations on our ability to rapidly identify potential members. As a result, there is a need for automated high-throughput member classification approaches that leverage their demonstrated tertiary and functional homology.ResultsWe developed an SVM-based model to predict sequential tri-disulfide peptide (STP) toxins from peptide sequences. One optimized model, called PredSTP, predicted STPs from training set with sensitivity, specificity, precision, accuracy and a Matthews correlation coefficient of 94.86 %, 94.11 %, 84.31 %, 94.30 % and 0.86, respectively, using 200 fold cross validation. The same model outperforms existing prediction approaches in three independent out of sample testsets derived from PDB.ConclusionPredSTP can accurately identify a wide range of cystine stabilized peptide toxins directly from sequences in a species-agnostic fashion. The ability to rapidly filter sequences for potential bioactive peptides can greatly compress the time between peptide identification and testing structural and functional properties for possible antimicrobial and insecticidal candidates. A web interface is freely available to predict STP toxins from http://crick.ecs.baylor.edu/.","venue":"BMC Bioinformatics","listofauthors":"S. Islam, Tanvir Sajed, C. Kearney, E. Baker","citations":16,"year":2015,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"16","number":"1","bibtex":"@article{2015,\n\tdoi = {10.1186/s12859-015-0633-x},\n\turl = {https://doi.org/10.1186%2Fs12859-015-0633-x},\n\tyear = 2015,\n\tmonth = {jul},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {16},\n\tnumber = {1},\n\tauthor = {S. M. Ashiqul Islam and Tanvir Sajed and Christopher Michel Kearney and Erich J Baker},\n\ttitle = {{PredSTP}: a highly accurate {SVM} based model to predict sequential cystine stabilized peptides}\n}","authorsSemantic":[3]},{"id":177,"title":"Systems genetic discovery of host-microbiome interactions reveals mechanisms of microbial involvement in disease","doi":"10.1101/349605","description":"The role of the microbiome in health and disease involves complex networks of host genetics, genomics, microbes and environment. Identifying the mechanisms of these interactions has remained challenging. Systems genetics in the laboratory mouse enables data-driven discovery of network components and mechanisms of host-microbial interactions underlying multiple disease phenotypes. To examine the interplay among the whole host genome, transcriptome and microbiome, we mapped quantitative trait loci and correlated the abundance of cecal mRNA, luminal microflora, physiology and behavior in incipient strains of the highly diverse Collaborative Cross mouse population. The relationships that are extracted can be tested experimentally to ascribe causality among host and microbe in behavior and physiology, providing insight into disease. Application of this strategy in the Collaborative Cross population revealed experimentally validated mechanisms of microbial involvement in models of autism, inflammatory bowel disease and sleep disorder. eTOC Blurb Host genetic diversity provides a variable selection environment and physiological context for microbiota and their interaction with host physiology. Using a highly diverse mouse population Bubier et al. identified a variety of host, microbe and potentially disease interactions. Highlights * 18 significant species-specific QTL regulating microbial abundance were identified * Cis and trans eQTL for 1,600 cecal transcripts were mapped in the Collaborative Cross * Sleep phenotypes were highly correlated with the abundance of B.P. Odoribacter * Elimination of sleep-associated microbes restored normal sleep patterns in mice.","venue":"","listofauthors":"J. Bubier, V. Philip, C. Quince, James H. Campbell, Yanjiao Zhou, T. Vishnivetskaya, S. Duvvuru, R. Blair, Juliet Ndukum, K. Donohue, C. Phillips, C. Foster, D. J. Mellert, G. Weinstock, C. Culiat, E. Baker, M. Langston, B. O’Hara, A. Palumbo, M. Podar, E. Chesler","citations":2,"year":2018,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2018,\n\tdoi = {10.1101/349605},\n\turl = {https://doi.org/10.1101%2F349605},\n\tyear = 2018,\n\tmonth = {jun},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {Jason A. Bubier and Vivek M. Philip and Christopher Quince and James Campbell and Yanjiao Zhou and Tatiana Vishnivetskaya and Suman Duvvuru and Rachel Hageman Blair and Juliet Ndukum and Kevin D. Donohue and Charles Phillips and Carmen M. Foster and David J. Mellert and George Weinstock and Cymbeline T. Culiat and Erich J. Baker and Michael A. Langston and Bruce O'Hara and Anthony V. Palumbo and Mircea Podar and Elissa J. Chesler},\n\ttitle = {Systems genetic discovery of host-microbiome interactions reveals mechanisms of microbial involvement in disease}\n}","authorsSemantic":[3]},{"id":178,"title":"Integration of heterogeneous functional genomics data in gerontology research identifies genes and pathway underlying aging across species","doi":"10.1101/451013","description":"Understanding the biological mechanisms behind aging, lifespan and healthspan is becoming increasingly important as the proportion of the world's population over the age of 65 grows, along with the cost and complexity of their care. BigData oriented approaches and analysis methods for integrative functional genomics enable current and future bio-gerontologists to synthesize, distill and interpret vast, heterogeneous data. GeneWeaver is an analysis system for integration of data that allows investigators to store, search, and analyze immense amounts of data including user-submitted experimental data, data from primary publications, and data in other databases. Aging related genome-wide gene sets from primary publications were curated into this system in concert with data from other model-organism and aging-specific databases, and used in several application using GeneWeavers analysis tools. For example, we identified Cd63 as a frequently represented gene among aging-related genome-wide results. To evaluate the role of Cd63 in aging, we performed RNAi knockdown of the C. elegans ortholog, tsp-7, demonstrating that this manipulation is capable of extending lifespan. The tools in GeneWeaver enable aging researchers to make new discoveries into the associations between the genes, normal biological processes, and diseases that affect aging, healthspan, and lifespan.","venue":"","listofauthors":"J. Bubier, G. Sutphin, T. Reynolds, R. Korstanje, A. Fuksman-Kumpa, E. Baker, M. Langston, E. Chesler","citations":0,"year":2018,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2018,\n\tdoi = {10.1101/451013},\n\turl = {https://doi.org/10.1101%2F451013},\n\tyear = 2018,\n\tmonth = {oct},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {Jason A Bubier and George L Sutphin and Timothy J Reynolds and Ron Korstanje and Axis Fuksman-Kumpa and Erich J Baker and Michael A Langston and Elissa J Chesler},\n\ttitle = {Integration of heterogeneous functional genomics data in gerontology research identifies genes and pathway underlying aging across species}\n}","authorsSemantic":[3]},{"id":179,"title":"Classes, Databases, and Prediction Methods of Pharmaceutically and Commercially Important Cystine-Stabilized Peptides","doi":"10.3390/toxins10060251","description":"Cystine-stabilized peptides represent a large family of peptides characterized by high structural stability and bactericidal, fungicidal, or insecticidal properties. Found throughout a wide range of taxa, this broad and functionally important family can be subclassified into distinct groups dependent upon their number and type of cystine bonding patters, tertiary structures, and/or their species of origin. Furthermore, the annotation of proteins related to the cystine-stabilized family are under-represented in the literature due to their difficulty of isolation and identification. As a result, there are several recent attempts to collate them into data resources and build analytic tools for their dynamic prediction. Ultimately, the identification and delivery of new members of this family will lead to their growing inclusion into the repertoire of commercial viable alternatives to antibiotics and environmentally safe insecticides. This review of the literature and current state of cystine-stabilized peptide biology is aimed to better describe peptide subfamilies, identify databases and analytics resources associated with specific cystine-stabilized peptides, and highlight their current commercial success.","venue":"Toxins","listofauthors":"S. Islam, C. Kearney, E. Baker","citations":3,"year":2018,"publisher":"MDPI AG","pages":"251","volume":"10","number":"6","bibtex":"@article{2018,\n\tdoi = {10.3390/toxins10060251},\n\turl = {https://doi.org/10.3390%2Ftoxins10060251},\n\tyear = 2018,\n\tmonth = {jun},\n\tpublisher = {{MDPI} {AG}},\n\tvolume = {10},\n\tnumber = {6},\n\tpages = {251},\n\tauthor = {S Islam and Christopher Kearney and Erich Baker},\n\ttitle = {Classes, Databases, and Prediction Methods of Pharmaceutically and Commercially Important Cystine-Stabilized Peptides}\n}","authorsSemantic":[3]},{"id":180,"title":"Optimizing Support Vector Machine Analysis in Low Density Biological Data Sets","doi":"10.1109/CSCI46756.2018.00263","description":"We explore the effectiveness of Support Vector Machines (SVM) for classification in a sparse data set. Non-human primate models are utilized to analyze Alcohol Use Disorders (AUDs); however, the resulting data have a limited sample size. The challenge of low sample numbers and low replicates are explored using a variety of optimization strategies for feature extraction, including correlation, entropy, density, linear support vector machines for regression (SVR), backward SVR, and forward SVR. We investigate these approaches against the backdrop of the relationship between alcohol consumption and tibial bone mineral density. The results indicate that machine learning (ML) can effectively be used in cases of low and diverse biological data sets. The best relevance feature ranking strategies are correlation, SVR forward, and SVR backward.","venue":"2018 International Conference on Computational Science and Computational Intelligence (CSCI)","listofauthors":"Pablo Rivas, Sharon Moore, U. Iwaniec, R. Turner, Kathy Grant, E. Baker","citations":1,"year":2018,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2018,\n\tdoi = {10.1109/csci46756.2018.00263},\n\turl = {https://doi.org/10.1109%2Fcsci46756.2018.00263},\n\tyear = 2018,\n\tmonth = {dec},\n\tpublisher = {{IEEE}},\n\tauthor = {Pablo Rivas and Sharon Moore and Urszula Iwaniec and Russell Turner and Kathy Grant and Erich Baker},\n\ttitle = {Optimizing Support Vector Machine Analysis in Low Density Biological Data Sets}\n}","authorsSemantic":[3,8]},{"id":181,"title":"Assigning biological function using hidden signatures in cystine-stabilized peptide sequences","doi":"10.1038/s41598-018-27177-8","description":"Cystine-stabilized peptides have great utility as they naturally block ion channels, inhibit acetylcholine receptors, or inactivate microbes. However, only a tiny fraction of these peptides has been characterized. Exploration for novel peptides most efficiently starts with the identification of candidates from genome sequence data. Unfortunately, though cystine-stabilized peptides have shared structures, they have low DNA sequence similarity, restricting the utility of BLAST and even more powerful sequence alignment-based annotation algorithms, such as PSI-BLAST and HMMER. In contrast, a supervised machine learning approach may improve discovery and function assignment of these peptides. To this end, we employed our previously described m-NGSG algorithm, which utilizes hidden signatures embedded in peptide primary sequences that define and categorize structural or functional classes of peptides. From the generalized m-NGSG framework, we derived five specific models that categorize cystine-stabilized peptide sequences into specific functional classes. When compared with PSI-BLAST, HMMER and existing function-specific models, our novel approach (named CSPred) consistently demonstrates superior performance in discovery and function-assignment. We also report an interactive version of CSPred, available through download (https://bitbucket.org/sm_islam/cystine-stabilized-proteins/src) or web interface (watson.ecs.baylor.edu/cspred), for the discovery of cystine-stabilized peptides of specific function from genomic datasets and for genome annotation. We fully describe, in the Availability section following the Discussion, the quick and simple usage of the CsPred website to automatically deliver function assignments for batch submissions of peptide sequences.","venue":"Scientific Reports","listofauthors":"S. Islam, C. Kearney, E. Baker","citations":5,"year":2018,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"8","number":"1","bibtex":"@article{2018,\n\tdoi = {10.1038/s41598-018-27177-8},\n\turl = {https://doi.org/10.1038%2Fs41598-018-27177-8},\n\tyear = 2018,\n\tmonth = {jun},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {8},\n\tnumber = {1},\n\tauthor = {S. M. Ashiqul Islam and Christopher Michel Kearney and Erich J. Baker},\n\ttitle = {Assigning biological function using hidden signatures in cystine-stabilized peptide sequences}\n}","authorsSemantic":[3]},{"id":182,"title":"Protein classification using modified n-grams and skip-grams","doi":"10.1093/bioinformatics/btx823","description":"Motivation\nClassification by supervised machine learning greatly facilitates the annotation of protein characteristics from their primary sequence. However, the feature generation step in this process requires detailed knowledge of attributes used to classify the proteins. Lack of this knowledge risks the selection of irrelevant features, resulting in a faulty model. In this study, we introduce a supervised protein classification method with a novel means of automating the work-intensive feature generation step via a Natural Language Processing (NLP)-dependent model, using a modified combination of n-grams and skip-grams (m-NGSG).\n\n\nResults\nA meta-comparison of cross-validation accuracy with twelve training datasets from nine different published studies demonstrates a consistent increase in accuracy of m-NGSG when compared to contemporary classification and feature generation models. We expect this model to accelerate the classification of proteins from primary sequence data and increase the accessibility of protein characteristic prediction to a broader range of scientists.\n\n\nAvailability and implementation\nm-NGSG is freely available at Bitbucket: https://bitbucket.org/sm_islam/mngsg/src. A web server is available at watson.ecs.baylor.edu/ngsg.\n\n\nContact\nerich_baker@baylor.edu.\n\n\nSupplementary information\nSupplementary data are available at Bioinformatics online.","venue":"Bioinform.","listofauthors":"S. Islam, Benjamin J. Heil, C. Kearney, E. Baker","citations":15,"year":2017,"publisher":"Oxford University Press (OUP)","pages":"1481-1487","volume":"34","number":"9","bibtex":"@article{2017,\n\tdoi = {10.1093/bioinformatics/btx823},\n\turl = {https://doi.org/10.1093%2Fbioinformatics%2Fbtx823},\n\tyear = 2017,\n\tmonth = {dec},\n\tpublisher = {Oxford University Press ({OUP})},\n\tvolume = {34},\n\tnumber = {9},\n\tpages = {1481--1487},\n\tauthor = {S M Ashiqul Islam and Benjamin J Heil and Christopher Michel Kearney and Erich J Baker},\n\teditor = {Alfonso Valencia},\n\ttitle = {Protein classification using modified n-grams and skip-grams}\n}","authorsSemantic":[3]},{"id":183,"title":"CSPred: A machine-learning-based compound model to identify the functional activities of biologically-stable toxins","doi":"10.1109/BIBM.2017.8218014","description":"Pharmaceutical industries are interested in Cysteine-stabilized peptides because they offer an array bioactive properties while being highly stable under a range of physiological conditions. However, it is widely appreciated that only a small fraction of this type of peptides have been experimentally discovered while a large number remain unidentified. However, identification of these cysteine-stabilized peptides using normal sequence alignment is challenging because of the high noise to signal ratio. Therefore, we propose a machine learning-based compound model to predict functional properties of cysteine-stabilized peptides from their primary sequence. We also offer a freely available web-server at http://watson.ecs.baylor.edu/cspred to make the model available to the scientific community.","venue":"2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","listofauthors":"S. Islam, C. Kearney, E. Baker","citations":3,"year":2017,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2017,\n\tdoi = {10.1109/bibm.2017.8218014},\n\turl = {https://doi.org/10.1109%2Fbibm.2017.8218014},\n\tyear = 2017,\n\tmonth = {nov},\n\tpublisher = {{IEEE}},\n\tauthor = {S.M. Ashiqul Islam and Christopher Michel Kearney and Erich J. Baker},\n\ttitle = {{CSPred}: A machine-learning-based compound model to identify the functional activities of biologically-stable toxins}\n}","authorsSemantic":[3]},{"id":184,"title":"Identifying Future Drinkers: Behavioral Analysis of Monkeys Initiating Drinking to Intoxication is Predictive of Future Drinking Classification","doi":"10.1111/acer.13327","description":"Background The Monkey Alcohol Tissue Research Resource (MATRR) is a repository and analytics platform for detailed data derived from well‐documented nonhuman primate (NHP) alcohol self‐administration studies. This macaque model has demonstrated categorical drinking norms reflective of human drinking populations, resulting in consumption pattern classifications of very heavy drinking (VHD), heavy drinking (HD), binge drinking (BD), and low drinking (LD) individuals. Here, we expand on previous findings that suggest ethanol drinking patterns during initial drinking to intoxication can reliably predict future drinking category assignment. Methods The classification strategy uses a machine‐learning approach to examine an extensive set of daily drinking attributes during 90 sessions of induction across 7 cohorts of 5 to 8 monkeys for a total of 50 animals. A Random Forest classifier is employed to accurately predict categorical drinking after 12 months of self‐administration. Results Predictive outcome accuracy is approximately 78% when classes are aggregated into 2 groups, “LD and BD” and “HD and VHD.” A subsequent 2‐step classification model distinguishes individual LD and BD categories with 90% accuracy and between HD and VHD categories with 95% accuracy. Average 4‐category classification accuracy is 74%, and provides putative distinguishing behavioral characteristics between groupings. Conclusions We demonstrate that data derived from the induction phase of this ethanol self‐administration protocol have significant predictive power for future ethanol consumption patterns. Importantly, numerous predictive factors are longitudinal, measuring the change of drinking patterns through 3 stages of induction. Factors during induction that predict future heavy drinkers include being younger at the time of first intoxication and developing a shorter latency to first ethanol drink. Overall, this analysis identifies predictive characteristics in future very heavy drinkers that optimize intoxication, such as having increasingly fewer bouts with more drinks. This analysis also identifies characteristic avoidance of intoxicating topographies in future low drinkers, such as increasing number of bouts and waiting longer before the first ethanol drink.","venue":"Alcoholism, clinical and experimental research","listofauthors":"E. Baker, N. Walter, Alex Salo, Pablo Rivas Perea, Sharon Moore, S. Gonzales, K. Grant","citations":23,"year":2017,"publisher":"Wiley","pages":"626-636","volume":"41","number":"3","bibtex":"@article{2017,\n\tdoi = {10.1111/acer.13327},\n\turl = {https://doi.org/10.1111%2Facer.13327},\n\tyear = 2017,\n\tmonth = {feb},\n\tpublisher = {Wiley},\n\tvolume = {41},\n\tnumber = {3},\n\tpages = {626--636},\n\tauthor = {Erich J. Baker and Nicole A.R. Walter and Alex Salo and Pablo Rivas Perea and Sharon Moore and Steven Gonzales and Kathleen A. Grant},\n\ttitle = {Identifying Future Drinkers: Behavioral Analysis of Monkeys Initiating Drinking to Intoxication is Predictive of Future Drinking Classification}\n}","authorsSemantic":[3]},{"id":185,"title":"On using cached results to enumerate maximal k-cliques in GeneWeaver","doi":"10.1109/FSKD.2017.8393097","description":"The online biological data analytics tool GeneWeaver [1] uses a fast algorithm to directly compute k-cliques between different sets of data. By caching such results, we are able to compute k-clique faster in some cases. We derived a formula to determine whether or not to cache a result. In order to know if the cached results can be used to compute a desired k-clique, we also created a new algorithm to solve the generalized set coverage problem.","venue":"2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","listofauthors":"Qi Yang, G. Speegle, E. Baker","citations":0,"year":2017,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2017,\n\tdoi = {10.1109/fskd.2017.8393097},\n\turl = {https://doi.org/10.1109%2Ffskd.2017.8393097},\n\tyear = 2017,\n\tmonth = {jul},\n\tpublisher = {{IEEE}},\n\tauthor = {Qi Yang and Greg Speegle and Erich J. Baker},\n\ttitle = {On using cached results to enumerate maximal k-cliques in {GeneWeaver}}\n}","authorsSemantic":[10,3]},{"id":186,"title":"Protein classification using modified n-gram and skip-gram models","doi":"10.1101/170407","description":"Motivation Classification by supervised machine learning greatly facilitates the annotation of protein characteristics from their primary sequence. However, the feature generation step in this process requires detailed knowledge of attributes used to classify the proteins. Lack of this knowledge risks the selection of irrelevant features, resulting in a faulty model. In this study, we introduce a means of automating the work-intensive feature generation step via a Natural Language Processing (NLP)-dependent model, using a modified combination of N-Gram and Skip-Gram models (m-NGSG). Results A meta-comparison of cross validation accuracy with twelve training datasets from nine different published studies demonstrates a consistent increase in accuracy of m-NGSG when compared to contemporary classification and feature generation models. We expect this model to accelerate the classification of proteins from primary sequence data and increase the accessibility of protein prediction to a broader range of scientists. Availability m-NGSG is freely available at Bitbucket: https://bitbucket.org/smislam/mngsg/src Supplements link to supplementary documents Contact Erich_Baker@baylor.edu","venue":"","listofauthors":"S. Islam, Benjamin J. Heil, C. Kearney, E. Baker","citations":4,"year":2017,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2017,\n\tdoi = {10.1101/170407},\n\turl = {https://doi.org/10.1101%2F170407},\n\tyear = 2017,\n\tmonth = {jul},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {S M Ashiqul Islam and Benjamin J Heil and Christopher Michel Kearney and Erich J Baker},\n\ttitle = {Protein classification using modifiedn-gramandskip-grammodels}\n}","authorsSemantic":[3]},{"id":194,"title":"GeneWeaver: finding consilience in heterogeneous cross-species functional genomics data","doi":"10.1007/s00335-015-9575-x","description":"A persistent challenge lies in the interpretation of consensus and discord from functional genomics experimentation. Harmonizing and analyzing this data will enable investigators to discover relations of many genes to many diseases, and from many phenotypes and experimental paradigms to many diseases through their genomic substrates. The GeneWeaver.org system provides a platform for cross-species integration and interrogation of heterogeneous curated and experimentally derived functional genomics data. GeneWeaver enables researchers to store, share, analyze, and compare results of their own genome-wide functional genomics experiments in an environment containing rich companion data obtained from major curated repositories, including the Mouse Genome Database and other model organism databases, along with derived data from highly specialized resources, publications, and user submissions. The data, largely consisting of gene sets and putative biological networks, are mapped onto one another through gene identifiers and homology across species. A versatile suite of interactive tools enables investigators to perform a variety of set analysis operations to find consilience among these often noisy experimental results. Fast algorithms enable real-time analysis of large queries. Specific applications include prioritizing candidate genes for quantitative trait loci, identifying biologically valid mouse models and phenotypic assays for human disease, finding the common biological substrates of related diseases, classifying experiments and the biological concepts they represent from empirical data, and applying patterns of genomic evidence to implicate novel genes in disease. These results illustrate an alternative to strict emphasis on replicability, whereby researchers classify experimental results to identify the conditions that lead to their similarity.","venue":"Mammalian Genome","listofauthors":"J. Bubier, C. Phillips, M. Langston, E. Baker, E. Chesler","citations":9,"year":2015,"publisher":"Springer Science and Business Media LLC","pages":"556-566","volume":"26","number":"9-10","bibtex":"@article{2015,\n\tdoi = {10.1007/s00335-015-9575-x},\n\turl = {https://doi.org/10.1007%2Fs00335-015-9575-x},\n\tyear = 2015,\n\tmonth = {jun},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {26},\n\tnumber = {9-10},\n\tpages = {556--566},\n\tauthor = {Jason A. Bubier and Charles A. Phillips and Michael A. Langston and Erich J. Baker and Elissa J. Chesler},\n\ttitle = {{GeneWeaver}: finding consilience in heterogeneous cross-species functional genomics data}\n}","authorsSemantic":[3]},{"id":187,"title":"Protein Classification using Modified N-Gram and Skip-Gram Models: Extended Abstract","doi":"10.1145/3107411.3108193","description":"Machine Learning (ML)-based classification of protein characteristics from primary sequences is an important tool for exploring candidate proteins in targeted drug discovery, mutational analysis, and functional identification. However, ML feature selection requires extensive manual curation and knowledge of protein chemistry, interactions, and micro-environment of the proteins of interest. Current approaches include amino acid composition strategies, specific motif analysis or Quantitative Structure-Activity Relationship (QSAR)-based feature generation methods. In contrast, we propose an automated generalized feature generation method based on Natural Language Processing (NLP), using a modified combination of N-Gram and Skip-Gram models (m-NGSG). Optimal parameters are selected using an adapted grid search algorithm, enabling a high-throughput global application of our approach. A meta-comparison of logistic regression mediated classification approaches exploiting m-NGSG with other published models illustrates enhanced functional and structural binary and multi-class classification accuracy in every instance. The lack of dependence on detailed physicochemical knowledge makes the m-NGSG approach ideal for the exploration of protein characteristics recalcitrant to previous approaches without any loss in predictive accuracy. A further test on prediction quality of m-NGSG on cationic channel blockers with 70% sequence identity from Arthropods demonstrated 94.10% and 92.30% accuracy on the training and test set, respectively. The latter study demonstrates the applicability of m-NGSG model on a functional classification of proteins employing a novel dataset.Thus, without the requirement of expert intervention for optimal feature selection, it is hoped that this automated feature generation approach will reduce the time needed to employ ML classification strategies for prediction of protein characteristics.","venue":"BCB","listofauthors":"S. Islam, C. Kearney, Ankan Choudhury, E. Baker","citations":3,"year":2017,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2017,\n\tdoi = {10.1145/3107411.3108193},\n\turl = {https://doi.org/10.1145%2F3107411.3108193},\n\tyear = 2017,\n\tmonth = {aug},\n\tpublisher = {{ACM}},\n\tauthor = {SM Ashiqul Islam and Christopher Michel Kearney and Ankan Choudhury and Erich J. Baker},\n\ttitle = {Protein Classification using Modified\n\t\t            N-Gram\n\t\t            and\n\t\t            Skip-Gram\n\t\t            Models}\n}","authorsSemantic":[3]},{"id":188,"title":"Integrative Functional Genomics for Systems Genetics in GeneWeaver.org.","doi":"10.1007/978-1-4939-6427-7_6","description":"The abundance of existing functional genomics studies permits an integrative approach to interpreting and resolving the results of diverse systems genetics studies. However, a major challenge lies in assembling and harmonizing heterogeneous data sets across species for facile comparison to the positional candidate genes and coexpression networks that come from systems genetic studies. GeneWeaver is an online database and suite of tools at www.geneweaver.org that allows for fast aggregation and analysis of gene set-centric data. GeneWeaver contains curated experimental data together with resource-level data such as GO annotations, MP annotations, and KEGG pathways, along with persistent stores of user entered data sets. These can be entered directly into GeneWeaver or transferred from widely used resources such as GeneNetwork.org. Data are analyzed using statistical tools and advanced graph algorithms to discover new relations, prioritize candidate genes, and generate function hypotheses. Here we use GeneWeaver to find genes common to multiple gene sets, prioritize candidate genes from a quantitative trait locus, and characterize a set of differentially expressed genes. Coupling a large multispecies repository curated and empirical functional genomics data to fast computational tools allows for the rapid integrative analysis of heterogeneous data for interpreting and extrapolating systems genetics results.","venue":"Methods in molecular biology","listofauthors":"J. Bubier, M. Langston, E. Baker, E. Chesler","citations":6,"year":2016,"publisher":"Springer New York","pages":"131-152","volume":null,"number":null,"bibtex":"@incollection{2016,\n\tdoi = {10.1007/978-1-4939-6427-7_6},\n\turl = {https://doi.org/10.1007%2F978-1-4939-6427-7_6},\n\tyear = 2016,\n\tmonth = {dec},\n\tpublisher = {Springer New York},\n\tpages = {131--152},\n\tauthor = {Jason A. Bubier and Michael A. Langston and Erich J. Baker and Elissa J. Chesler},\n\ttitle = {Integrative Functional Genomics for Systems Genetics in {GeneWeaver}.org}\n}","authorsSemantic":[3]},{"id":189,"title":"Effects of alcohol on c-Myc protein in the brain","doi":"10.1016/j.bbr.2016.11.009","description":"HighlightsGenetic predisposition to alcohol‐related features leads to c‐Myc protein increase in the amygdala (SOT/NOT) and hypothalamus (WSP/WSR).Chronic alcoholism increases c‐Myc protein in the hypothalamus.Methamphetamine exposure increases c‐Myc protein in the amygdala.p21 protein is decreased in withdrawal‐resistant mice and chronic drinking monkeys. ABSTRACT Alcoholism is a disorder categorized by significant impairment that is directly related to persistent and extreme use of alcohol. The effects of alcoholism on c‐Myc protein expression in the brain have been scarcely studied. This is the first study to investigate the role different characteristics of alcoholism have on c‐Myc protein in the brain. We analyzed c‐Myc protein in the hypothalamus and amygdala from five different animal models of alcohol abuse. c‐Myc protein was increased following acute ethanol exposure in a mouse knockout model and following chronic ethanol consumption in vervet monkeys. We also observed increases in c‐Myc protein exposure in animals that are genetically predisposed to alcohol and methamphetamine abuse. Lastly, c‐Myc protein was increased in animals that were acutely exposed to methamphetamine when compared to control treated animals. These results suggest that in substance abuse c‐Myc plays an important role in the brain’s response.","venue":"Behavioural Brain Research","listofauthors":"Tunde O. Akinyeke, Sydney J. Weber, A. Davenport, E. Baker, J. Daunais, J. Raber","citations":3,"year":2017,"publisher":"Elsevier BV","pages":"356-364","volume":"320","number":null,"bibtex":"@article{2017,\n\tdoi = {10.1016/j.bbr.2016.11.009},\n\turl = {https://doi.org/10.1016%2Fj.bbr.2016.11.009},\n\tyear = 2017,\n\tmonth = {mar},\n\tpublisher = {Elsevier {BV}},\n\tvolume = {320},\n\tpages = {356--364},\n\tauthor = {Tunde Akinyeke and Sydney J. Weber and April T. Davenport and Erich J. Baker and James B. Daunais and Jacob Raber},\n\ttitle = {Effects of alcohol on c-Myc protein in the brain}\n}","authorsSemantic":[3]},{"id":190,"title":"GeneWeaver: data driven alignment of cross-species genomics in biology and disease","doi":"10.1093/nar/gkv1329","description":"The GeneWeaver data and analytics website (www.geneweaver.org) is a publically available resource for storing, curating and analyzing sets of genes from heterogeneous data sources. The system enables discovery of relationships among genes, variants, traits, drugs, environments, anatomical structures and diseases implicitly found through gene set intersections. Since the previous review in the 2012 Nucleic Acids Research Database issue, GeneWeaver's underlying analytics platform has been enhanced, its number and variety of publically available gene set data sources has been increased, and its advanced search mechanisms have been expanded. In addition, its interface has been redesigned to take advantage of flexible web services, programmatic data access, and a refined data model for handling gene network data in addition to its original emphasis on gene set data. By enumerating the common and distinct biological molecules associated with all subsets of curated or user submitted groups of gene sets and gene networks, GeneWeaver empowers users with the ability to construct data driven descriptions of shared and unique biological processes, diseases and traits within and across species.","venue":"Nucleic Acids Res.","listofauthors":"E. Baker, J. Bubier, T. Reynolds, M. Langston, E. Chesler","citations":24,"year":2015,"publisher":"Oxford University Press (OUP)","pages":"D555-D559","volume":"44","number":"D1","bibtex":"@article{2015,\n\tdoi = {10.1093/nar/gkv1329},\n\turl = {https://doi.org/10.1093%2Fnar%2Fgkv1329},\n\tyear = 2015,\n\tmonth = {dec},\n\tpublisher = {Oxford University Press ({OUP})},\n\tvolume = {44},\n\tnumber = {D1},\n\tpages = {D555--D559},\n\tauthor = {Erich Baker and Jason A. Bubier and Timothy Reynolds and Michael A. Langston and Elissa J. Chesler},\n\ttitle = {{GeneWeaver}: data driven alignment of cross-species genomics in biology and disease}\n}","authorsSemantic":[3]},{"id":195,"title":"Scalable multipartite subgraph enumeration for integrative analysis of heterogeneous experimental functional genomics data","doi":"10.1145/2808719.2812595","description":"Functional genomics, the effort to understand the role of genomic elements in biological processes, has led to an avalanche of diverse experimental and semantic information defining associations between genes and various biological concepts across species and experimental paradigms. Integrating this rapidly expanding wealth of heterogeneous data, and finding consensus among so many diverse sources for specific research questions, require highly sophisticated big data structures and algorithms for harmonization and scalable analysis. In this context, multipartite graphs can often serve as useful structures for representing questions about the role of genes in multiple, frequently-occurring disease processes. The main focus of this paper is on finding and analyzing efficient algorithms for dense subgraph enumeration in such graphs. An O(3n/3)-time procedure was devised to enumerate all maximal k-partite cliques in a k-partite graph, where k ≥ 3. The maximum number of such cliques is also shown to obey this bound, and thus this procedure obtains the best possible asymptotic performance. Empirical testing on both real and synthetic data is conducted. Concrete applications to biological data are described, as are scalability issues in the context of big data analysis.","venue":"BCB","listofauthors":"C. Phillips, K. Wang, J. Bubier, E. Baker, E. Chesler, M. Langston","citations":5,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/2808719.2812595},\n\turl = {https://doi.org/10.1145%2F2808719.2812595},\n\tyear = 2015,\n\tmonth = {sep},\n\tpublisher = {{ACM}},\n\tauthor = {Charles A. Phillips and Kai Wang and Jason Bubier and Erich J. Baker and Elissa J. Chesler and Michael A. Langston},\n\ttitle = {Scalable multipartite subgraph enumeration for integrative analysis of heterogeneous experimental functional genomics data}\n}","authorsSemantic":[3]},{"id":196,"title":"Algorithmic tools for tripartite data analysis","doi":"10.1186/1471-2105-15-S10-P32","description":"Materials and methods In this work, tripartite graphs are considered. Applications include comparing two sets of many gene-many disease associations. An algorithm is described that finds a maximum triclique in such a graph. It employs a branching strategy inspired by maximum clique algorithms for general graphs. A binary search tree is used, in which branch nodes in the tree represent vertices in the tripartite graph, and in which branching decisions are based on whether a vertex is in or out of a maximum triclique. A reduction rule is also introduced to filter out irrelevant vertices. This algorithm was developed in the context of GeneWeaver, an online system for the integration of functional genomics experimental results. In this system triclique extraction will enable fast transitive association of diseases based on the similarity of gene-disease associations from many experiments. Computational experience with huge volumes of experimental data is described.","venue":"BMC Bioinformatics","listofauthors":"C. Phillips, E. Baker, E. Chesler, M. Langston","citations":0,"year":2014,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"15","number":"S10","bibtex":"@article{2014,\n\tdoi = {10.1186/1471-2105-15-s10-p32},\n\turl = {https://doi.org/10.1186%2F1471-2105-15-s10-p32},\n\tyear = 2014,\n\tmonth = {sep},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {15},\n\tnumber = {S10},\n\tauthor = {Charles A Phillips and Erich J Baker and Elissa J Chesler and Michael A Langston},\n\ttitle = {Algorithmic tools for tripartite data analysis}\n}","authorsSemantic":[3]},{"id":197,"title":"Monkey alcohol tissue research resource: banking tissues for alcohol research.","doi":"10.1111/acer.12467","description":"BACKGROUND\nAn estimated 18 million adults in the United States meet the clinical criteria for diagnosis of alcohol abuse or alcoholism, a disorder ranked as the third leading cause of preventable death. In addition to brain pathology, heavy alcohol consumption is comorbid with damage to major organs including heart, lungs, liver, pancreas, and kidneys. Much of what is known about risk for and consequences of heavy consumption derive from rodent or retrospective human studies. The neurobiological effects of chronic intake in rodent studies may not easily translate to humans due to key differences in brain structure and organization between species, including a lack of higher-order cognitive functions, and differences in underlying prefrontal cortical neural structures that characterize the primate brain. Further, rodents do not voluntarily consume large quantities of ethanol (EtOH) and they metabolize it more rapidly than primates.\n\n\nMETHODS\nThe basis of the Monkey Alcohol Tissue Research Resource (MATRR) is that nonhuman primates, specifically monkeys, show a range of drinking excessive amounts of alcohol (>3.0 g/kg or a 12 drink equivalent per day) over long periods of time (12 to 30 months) with concomitant pathological changes in endocrine, hepatic, and central nervous system (CNS) processes. The patterns and range of alcohol intake that monkeys voluntarily consume parallel what is observed in humans with alcohol use disorders and the longitudinal experimental design spans stages of drinking from the EtOH-naïve state to early exposure through chronic abuse. Age- and sex-matched control animals self-administer an isocaloric solution under identical operant procedures.\n\n\nRESULTS\nThe MATRR is a unique postmortem tissue bank that provides CNS and peripheral tissues, and associated bioinformatics from monkeys that self-administer EtOH using a standardized experimental paradigm to the broader alcohol research community.\n\n\nCONCLUSIONS\nThis resource provides a translational platform from which we can better understand the disease processes associated with alcoholism.","venue":"Alcoholism, clinical and experimental research","listofauthors":"J. Daunais, A. Davenport, C. Helms, S. Gonzales, S. Hemby, D. Friedman, Jonathan P. Farro, E. Baker, K. Grant","citations":22,"year":2014,"publisher":"Wiley","pages":"1973-1981","volume":"38","number":"7","bibtex":"@article{2014,\n\tdoi = {10.1111/acer.12467},\n\turl = {https://doi.org/10.1111%2Facer.12467},\n\tyear = 2014,\n\tmonth = {jun},\n\tpublisher = {Wiley},\n\tvolume = {38},\n\tnumber = {7},\n\tpages = {1973--1981},\n\tauthor = {James B. Daunais and April T. Davenport and Christa M. Helms and Steven W. Gonzales and Scott E. Hemby and David P. Friedman and Jonathan P. Farro and Erich J. Baker and Kathleen A. Grant},\n\ttitle = {Monkey Alcohol Tissue Research Resource: Banking Tissues for Alcohol Research}\n}","authorsSemantic":[3]},{"id":198,"title":"Integrative functional genomic analysis using GeneWeaver","doi":"10.7490/f1000research.1097189.1","description":"null","venue":"","listofauthors":"J. Bubier, M. Langston, E. Baker, E. Chesler","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n<title>Error: DOI Not Found</title>\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n\n<link rel=\"icon\" href=\"/static/img/favicon.png\" />\n<link rel=\"shortcut icon\" href=\"/static/favicon.ico\" type=\"image/x-icon\" /> \n<link href=\"/static/style/new-style2.css\" rel=\"stylesheet\" type=\"text/css\" />\n</head>\n\n<body>\n\n\n<div style=\"background:#fcb426\">\n<img src=\"/static/img/banner-413.gif\" alt=\"Logo\" width=\"620\" height=\"137\" border=\"0\" />\n</div>\n\n<div style=\"height:1px;background:#000000\"></div>\n<div style=\"height:1px;background:#54524f\"></div>\n<div style=\"height:1px;background:#f6911e\"></div>\n\n\n<!-- TABLE FOR NAVIGATION BAR -->\n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" id=\"navtable\" align=\"center\">\n<tr>\n    <td width=\"34\" height=\"26\" bgcolor=\"#231f20\"><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"1\" height=\"1\" /></td>\n    \n    <td height=\"26\" bgcolor=\"#231f20\" class=\"navtext\">\n    <a href=\"http://www.doi.org/index.html\">HOME</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/hb.html\">HANDBOOK</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/factsheets.html\">FACTSHEETS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/faq.html\">FAQs</a> &nbsp;|&nbsp; <a href=\"http://www.doi.org/resources.html\">RESOURCES</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/users.html\">USERS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/announce.html\">NEWS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/idf-members/index.html\">MEMBERS AREA</a>\n    </td>    \n  </tr>\n</table>\n<!-- END TABLE FOR NAVIGATION BAR -->\n\n<div style=\"height:1px;background:#e3a44d\"></div>\n<div style=\"height:3px;background:#4d4942\"></div>\n\n\n\n<!-- TABLE FOR CONTENT -->      \n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" bgcolor=\"#ffffff\">\n<tr>\n<td colspan=\"6\">\n<img src=\"/static/img/transparent.gif\" alt=\"\" width=\"100\" height=\"20\" border=\"0\" />\n</td>\n</tr>\n\n<tr>\n\n<td valign=\"top\">\n\n<h2>DOI Not Found</h2>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n<h3>10.7490/f1000research.1097189.1</h3>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n\n<p>This DOI cannot be found in the DOI System.  Possible reasons are:</p>\n\n\n<ul>\n\n<li style=\"padding-bottom: .5em;\">The DOI is incorrect in your source. Search for the item by name, title, or other metadata using a search engine.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI was copied incorrectly. Check to see that the string includes all the characters before and after the slash and no sentence punctuation marks.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI has not been activated yet.  Please try again later, and report the problem if the error continues.</li>\n\n</ul>\n\n\n\n<div class=\"divider\">&nbsp;</div>\n\n<p>You may report this error to the responsible DOI Registration Agency using the form below.  Include your email address to receive confirmation and feedback.</p>\n\n<div style=\"padding-left: 4em;\">\n\n<form action=\"/notfound\" method=\"post\" enctype=\"application/x-www-form-urlencoded\" name=\"notFoundForm\">\n\n\n<table border=\"0\" cellspacing=\"3\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>\n\n<table border=\"0\" align=\"center\" cellpadding=\"3\" cellspacing=\"3\">\n<tbody><tr>\n<th  align=\"right\" scope=\"row\"><label>DOI:</label></th>\n<td><input name=\"missingHandle\" type=\"text\" value=\"10.7490/f1000research.1097189.1\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\"><label>URL of Web Page Listing the DOI:</label></th>\n<td><input name=\"referringPage\" type=\"text\" value=\"\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\">Your Email Address:</th>\n<td><input name=\"userEmailAddress\" type=\"text\" value=\"Please enter your email address\" size=\"42\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\" valign=\"top\">Additional Information About the Error:</th>\n<td><textarea name=\"comments\" cols=\"30\" rows=\"6\"></textarea></td>\n</tr>\n</tbody>\n</table>\n\n</td>\n</tr>\n<tr>\n\n<td align=\"right\"><p><input name=\"send\" type=\"submit\" value=\"Submit Error Report\" /></p></td>\n</tr>\n</tbody>\n</table>\n\n</form>\n</div>\n\n\n\n\n</td>\n<td><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"20\" height=\"20\" border=\"0\" /></td>\n</tr>\n</table>\n\n<div class=\"divider-full\">&nbsp;</div>\n\n<!-- TABLE FOR FOOTER -->\n\n<table  border=\"0\" cellpadding=\"0\" cellspacing=\"0\" align=\"center\">\n\n<tr>\n<td align=\"center\" colspan=\"2\">\n<a href=\"/help.html\">DOI Resolution Documentation</a>\n</td>\n</tr>\n\n<tr>\n<td align=\"left\" height=\"40\">\n<img src=\"/static/img/Logo_TM.png\" alt=\"DOI_disc_logo\" width=\"24\" height=\"24\" />\n</td>\n\n<td align=\"left\">\n<span style=\"padding-left: 0px; font-size: 11px;\"><span style=\"vertical-align: super;\">&reg;</span>, DOI<span style=\"vertical-align: super;\">&reg;</span>, DOI.ORG<span style=\"vertical-align: super;\">&reg;</span>, and shortDOI<span style=\"vertical-align: super;\">&reg;</span> are trademarks of the International DOI Foundation.</span>\n</td>       \n</tr>\n</table>\n</body>\n</html>\n","authorsSemantic":[3]},{"id":199,"title":"Chronic alcohol self-administration in monkeys shows long-term quantity/frequency categorical stability.","doi":"10.1111/acer.12547","description":"BACKGROUND\nThe current criteria for alcohol use disorders (AUDs) do not include consumption (quantity/frequency) measures of alcohol intake, in part due to the difficulty of these measures in humans. Animal models of ethanol (EtOH) self-administration have been fundamental in advancing our understanding of the neurobiological basis of AUD and can address quantity/frequency measures with accurate measurements over prolonged periods of time. The nonhuman primate model of voluntary oral alcohol self-administration has documented both binge drinking and drinking to dependence and can be used to test the stability of consumption measures over time.\n\n\nMETHODS\nHere, an extensive set of alcohol intakes (g/kg/d) was analyzed from a large multi-cohort population of Rhesus (Macaca mulatta) monkeys (n = 31). Daily EtOH intake was uniformly distributed over chronic (12 months) access for all animals.\n\n\nRESULTS\nUnderlying this distribution of intakes were subpopulations of monkeys that exhibited distinctive clustering of drinking patterns, allowing us to categorically define very heavy drinking (VHD), heavy drinking (HD), binge drinking (BD), and low drinking (LD). These categories were stable across the 12 months assessed by the protocol, but exhibited fluctuations when examined at shorter intervals.\n\n\nCONCLUSIONS\nThe establishment of persistent drinking categories based on quantity/frequency suggests that consumption variables can be used to track long-term changes in behavioral, molecular, or physiochemical mechanisms related to our understanding of diagnosis, prevention, intervention, and treatment efficacies.","venue":"Alcoholism, clinical and experimental research","listofauthors":"E. Baker, Jonathan P. Farro, S. Gonzales, C. Helms, K. Grant","citations":52,"year":2014,"publisher":"Wiley","pages":"2835-2843","volume":"38","number":"11","bibtex":"@article{2014,\n\tdoi = {10.1111/acer.12547},\n\turl = {https://doi.org/10.1111%2Facer.12547},\n\tyear = 2014,\n\tmonth = {nov},\n\tpublisher = {Wiley},\n\tvolume = {38},\n\tnumber = {11},\n\tpages = {2835--2843},\n\tauthor = {Erich J. Baker and Jonathan Farro and Steven Gonzales and Christa Helms and Kathleen A. Grant},\n\ttitle = {Chronic Alcohol Self-Administration in Monkeys Shows Long-Term Quantity/Frequency Categorical Stability}\n}","authorsSemantic":[3]},{"id":206,"title":"On Bipartite Graph Decomposition in the Presence of Noise, with Applications to Biological Data Clustering","doi":null,"description":"We present a novel algorithm for extracting dense, disjoint subgraphs from undirected bipartite graphs. Our procedure successively removes such subgraphs, known as parabicliques, by iteratively isolating a maximum biclique and then expanding it in the presence of missing edges. Hence it relies on our previous work on efficiently finding solutions to the NP-complete maximum biclique problem. It is also resilient to noise in the form of outliers, poorly correlated raw data and so forth. We have implemented the algorithm and tested it on heterogeneous biological graphs that represent, among other things, associations between genes and diseases, phenotypes, and even microbes. This approach to biological data analysis can be employed as a tool for discovering, confirming and hypothesizing the many roles of genes, gene products and a wide variety of other biological network agents.","venue":"CTW","listofauthors":"C. Phillips, J. Jay, E. Baker, E. Chesler, M. Langston","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[3]},{"id":200,"title":"Detection of leukocoria using a soft fusion of expert classifiers under non-clinical settings","doi":"10.1186/1471-2415-14-110","description":"BackgroundLeukocoria is defined as a white reflection and its manifestation is symptomatic of several ocular pathologies, including retinoblastoma (Rb). Early detection of recurrent leukocoria is critical for improved patient outcomes and can be accomplished via the examination of recreational photography. To date, there exists a paucity of methods to automate leukocoria detection within such a dataset.MethodsThis research explores a novel classification scheme that uses fuzzy logic theory to combine a number of classifiers that are experts in performing multichannel detection of leukocoria from recreational photography. The proposed scheme extracts features aided by the discrete cosine transform and the Karhunen-Loeve transformation.ResultsThe soft fusion of classifiers is significantly better than other methods of combining classifiers with p = 1.12 × 10-5. The proposed methodology performs at a 92% accuracy rate, with an 89% true positive rate, and an 11% false positive rate. Furthermore, the results produced by our methodology exhibit the lowest average variance.ConclusionsThe proposed methodology overcomes non-ideal conditions of image acquisition, presenting a competent approach for the detection of leukocoria. Results suggest that recreational photography can be used in combination with the fusion of individual experts in multichannel classification and preprocessing tools such as the discrete cosine transform and the Karhunen-Loeve transformation.","venue":"BMC Ophthalmology","listofauthors":"P. Rivas-Perea, E. Baker, Greg Hamerly, Bryan F. Shaw","citations":13,"year":2014,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"14","number":"1","bibtex":"@article{2014,\n\tdoi = {10.1186/1471-2415-14-110},\n\turl = {https://doi.org/10.1186%2F1471-2415-14-110},\n\tyear = 2014,\n\tmonth = {sep},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {14},\n\tnumber = {1},\n\tauthor = {Pablo Rivas-Perea and Erich Baker and Greg Hamerly and Bryan F Shaw},\n\ttitle = {Detection of leukocoria using a soft fusion of expert classifiers under non-clinical settings}\n}","authorsSemantic":[5,3]},{"id":201,"title":"Contributions of Genomic and Informatic Approaches to Understanding Alcohol Dependence: From Genes to Networks","doi":"10.1016/B978-0-12-405941-2.00025-0","description":"Genomics and bioinformatics have brought a new dimension of research to the neurobiology of alcoholism, enabling the discovery of novel mechanisms and pathways of alcohol use, alcohol effects, and alcohol dependence. As with any bioscience of behavior, a working model of the system is constructed, tested, and refined through investigation and experimentation. Scaling this endeavor to genomics is the interplay between a framework of semantic knowledge bases and data-driven discovery science. The role of these systems in the research process and their impact on alcohol research is described and exemplified through the diverse systems and applications that have emerged, from functional genomics to systems and network analyses.","venue":"","listofauthors":"E. Chesler, E. Baker","citations":0,"year":2014,"publisher":"Elsevier","pages":"523-537","volume":null,"number":null,"bibtex":"@incollection{2014,\n\tdoi = {10.1016/b978-0-12-405941-2.00025-0},\n\turl = {https://doi.org/10.1016%2Fb978-0-12-405941-2.00025-0},\n\tyear = 2014,\n\tpublisher = {Elsevier},\n\tpages = {523--537},\n\tauthor = {E.J. Chesler and E.J. Baker},\n\ttitle = {Contributions of Genomic and Informatic Approaches to Understanding Alcohol Dependence}\n}","authorsSemantic":[3]},{"id":202,"title":"On finding bicliques in bipartite graphs: a novel algorithm and its application to the integration of diverse biological data types","doi":"10.1186/1471-2105-15-110","description":"BackgroundIntegrating and analyzing heterogeneous genome-scale data is a huge algorithmic challenge for modern systems biology. Bipartite graphs can be useful for representing relationships across pairs of disparate data types, with the interpretation of these relationships accomplished through an enumeration of maximal bicliques. Most previously-known techniques are generally ill-suited to this foundational task, because they are relatively inefficient and without effective scaling. In this paper, a powerful new algorithm is described that produces all maximal bicliques in a bipartite graph. Unlike most previous approaches, the new method neither places undue restrictions on its input nor inflates the problem size. Efficiency is achieved through an innovative exploitation of bipartite graph structure, and through computational reductions that rapidly eliminate non-maximal candidates from the search space. An iterative selection of vertices for consideration based on non-decreasing common neighborhood sizes boosts efficiency and leads to more balanced recursion trees.ResultsThe new technique is implemented and compared to previously published approaches from graph theory and data mining. Formal time and space bounds are derived. Experiments are performed on both random graphs and graphs constructed from functional genomics data. It is shown that the new method substantially outperforms the best previous alternatives.ConclusionsThe new method is streamlined, efficient, and particularly well-suited to the study of huge and diverse biological data. A robust implementation has been incorporated into GeneWeaver, an online tool for integrating and analyzing functional genomics experiments, available at http://geneweaver.org. The enormous increase in scalability it provides empowers users to study complex and previously unassailable gene-set associations between genes and their biological functions in a hierarchical fashion and on a genome-wide scale. This practical computational resource is adaptable to almost any applications environment in which bipartite graphs can be used to model relationships between pairs of heterogeneous entities.","venue":"BMC Bioinformatics","listofauthors":"Yun Zhang, C. Phillips, Gary L. Rogers, E. Baker, E. Chesler, M. Langston","citations":101,"year":2014,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"15","number":"1","bibtex":"@article{2014,\n\tdoi = {10.1186/1471-2105-15-110},\n\turl = {https://doi.org/10.1186%2F1471-2105-15-110},\n\tyear = 2014,\n\tmonth = {apr},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {15},\n\tnumber = {1},\n\tauthor = {Yun Zhang and Charles A Phillips and Gary L Rogers and Erich J Baker and Elissa J Chesler and Michael A Langston},\n\ttitle = {On finding bicliques in bipartite graphs: a novel algorithm and its application to the integration of diverse biological data types}\n}","authorsSemantic":[3]},{"id":203,"title":"Colorimetric Image Analysis in Detection of Leukocoria from Retinoblastoma in Snapshots Taken by Standard Digital Photography","doi":null,"description":"null","venue":"","listofauthors":"Katherine E. Talcott, Elizabeth V. Shaw, Rebecca L. Holden, Brandon W. Taylor, E. Baker, Greg Hamerly, A. Kentsis, S. Mukai, C. Rodriguez-Galindo, Bryan F. Shaw","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[3]},{"id":204,"title":"A Context-Driven Gene Prioritization Method for Web-Based Functional Genomics","doi":"10.1007/978-3-642-38036-5_18","description":"Functional genomics experiments often result in large sets of gene centered results associated with biological concepts such as diseases. Prioritization and interpretation of these results involves evaluation of the relevance of genes to various annotations or associated terms and is often executed through the use of prior information in biological databases. These diverse databases are frequently disconnected, or loosely federated data stores. Consequently, assessing the relations among biological entities and constructs, including genes, gene products, diseases, and model organism phenotypes is a challenging task typically requiring manual intervention, and as such only limited information is considered. Extracting and quantifying relations among genes and disease related concepts can be improved through the quantification of the entire contextual similarity of gene representations among the landscape of biological data. We have devised a suitable metric for this analysis which, unlike most similar methods requires no user-defined input parameters. We have demonstrated improved gene prioritization relative to existing metrics and commonly used software systems for gene prioritization. Our approach is implemented as an enhancement to the flexible integrative genomics platform, GeneWeaver.org.","venue":"ISBRA","listofauthors":"J. Jay, E. Baker, E. Chesler","citations":0,"year":2013,"publisher":"Springer Berlin Heidelberg","pages":"161-172","volume":null,"number":null,"bibtex":"@incollection{2013,\n\tdoi = {10.1007/978-3-642-38036-5_18},\n\turl = {https://doi.org/10.1007%2F978-3-642-38036-5_18},\n\tyear = 2013,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {161--172},\n\tauthor = {Jeremy J. Jay and Erich J. Baker and Elissa J. Chesler},\n\ttitle = {A Context-Driven Gene Prioritization Method for Web-Based Functional Genomics}\n}","authorsSemantic":[3]},{"id":205,"title":"GeneWeaver: a web-based system for integrative functional genomics","doi":"10.1093/nar/gkr968","description":"High-throughput genome technologies have produced a wealth of data on the association of genes and gene products to biological functions. Investigators have discovered value in combining their experimental results with published genome-wide association studies, quantitative trait locus, microarray, RNA-sequencing and mutant phenotyping studies to identify gene-function associations across diverse experiments, species, conditions, behaviors or biological processes. These experimental results are typically derived from disparate data repositories, publication supplements or reconstructions from primary data stores. This leaves bench biologists with the complex and unscalable task of integrating data by identifying and gathering relevant studies, reanalyzing primary data, unifying gene identifiers and applying ad hoc computational analysis to the integrated set. The freely available GeneWeaver (http://www.GeneWeaver.org) powered by the Ontological Discovery Environment is a curated repository of genomic experimental results with an accompanying tool set for dynamic integration of these data sets, enabling users to interactively address questions about sets of biological functions and their relations to sets of genes. Thus, large numbers of independently published genomic results can be organized into new conceptual frameworks driven by the underlying, inferred biological relationships rather than a pre-existing semantic framework. An empirical ‘ontology’ is discovered from the aggregate of experimental knowledge around user-defined areas of biological inquiry.","venue":"Nucleic Acids Res.","listofauthors":"E. Baker, J. Jay, J. Bubier, M. Langston, E. Chesler","citations":109,"year":2011,"publisher":"Oxford University Press (OUP)","pages":"D1067-D1076","volume":"40","number":"D1","bibtex":"@article{2011,\n\tdoi = {10.1093/nar/gkr968},\n\turl = {https://doi.org/10.1093%2Fnar%2Fgkr968},\n\tyear = 2011,\n\tmonth = {nov},\n\tpublisher = {Oxford University Press ({OUP})},\n\tvolume = {40},\n\tnumber = {D1},\n\tpages = {D1067--D1076},\n\tauthor = {Erich J. Baker and Jeremy J. Jay and Jason A. Bubier and Michael A. Langston and Elissa J. Chesler},\n\ttitle = {{GeneWeaver}: a web-based system for integrative functional genomics}\n}","authorsSemantic":[3]},{"id":207,"title":"Biological databases for behavioral neurobiology.","doi":"10.1016/B978-0-12-388408-4.00002-2","description":"Databases are, at their core, abstractions of data and their intentionally derived relationships. They serve as a central organizing metaphor and repository, supporting or augmenting nearly all bioinformatics. Behavioral domains provide a unique stage for contemporary databases, as research in this area spans diverse data types, locations, and data relationships. This chapter provides foundational information on the diversity and prevalence of databases, how data structures support the various needs of behavioral neuroscience analysis and interpretation. The focus is on the classes of databases, data curation, and advanced applications in bioinformatics using examples largely drawn from research efforts in behavioral neuroscience.","venue":"International review of neurobiology","listofauthors":"E. Baker","citations":2,"year":2012,"publisher":"Elsevier","pages":"19-38","volume":null,"number":null,"bibtex":"@incollection{2012,\n\tdoi = {10.1016/b978-0-12-388408-4.00002-2},\n\turl = {https://doi.org/10.1016%2Fb978-0-12-388408-4.00002-2},\n\tyear = 2012,\n\tpublisher = {Elsevier},\n\tpages = {19--38},\n\tauthor = {Erich J. Baker},\n\ttitle = {Biological Databases for Behavioral Neurobiology}\n}","authorsSemantic":[3]},{"id":208,"title":"The importance of open-source integrative genomics to drug discovery.","doi":null,"description":"Researchers investigating many areas of disease recognize the value of integrating large-scale genomic experiments across species and experimental methods. Analysis methods have been developed to make use of the breadth and depth of data from new technologies. Current paradigms of data storage, sharing and analysis are not yet ideal for these purposes. Open-access and analysis-enabled repositories are critical to progress, as they put the global integration of genomic data within reach of individual expert investigators. Current analytic approaches use the full scale and scope of data, but require data sharing, interoperability and community recognition of the value of shared information.","venue":"Current opinion in drug discovery & development","listofauthors":"E. Chesler, E. Baker","citations":10,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[3]},{"id":209,"title":"OntologicalDiscovery.org: A web resource for the empirical discovery of phenotypic relations across species and experimental systems","doi":null,"description":"The Ontological Discovery Environment ( http://ontologicaldiscovery.org ) is a free, public Internet resource for the storage, sharing, retrieval and analysis of phenotype-centered genomic data sets. The intent of this resource is to allow the creation of user-defined phenotype categories based on naturally and experimentally observed biological networks, pathways and systems rather than on externally manifested constructs and semantics such as disease names and processes. By extracting the relationships of complex processes from the technology that produces those relationships, this resource meets a growing demand for data integration and hypothesis discovery across multiple experimental contexts, including broad species and phenotype domains. At a highly processed level, analyses of set similarity, distance and hierarchical relations are performed through a modular suite of tools. The core pivot point of analysis is the creation of a bipartite network of gene-phenotype relations, a unique discrete graph approach to gene-set analysis which enables set-set matching of non-referential data. The central organizing metaphor of a gene set may be created, stored and curated by individual users, shared among virtual working groups, or made publicly available. Gene sets submission incorporates a variety of accession numbers, microarray feature IDs, and gene symbols from model organisms, allowing integration across experimentalmore » platforms, literature reviews and other genomic analyses. The sets themselves are annotated with several levels of metadata which may include an unstructured description, publication information and structured community ontologies for anatomy, process and function. Gene set translation to user chosen reference species through gene homology allows translational comparison of models regardless of the face validity of the experimental systems. In addition, computationally derived gene sets can be integrated into phenome interdependency and similarity hierarchy graphs, which are hierarchical trees of phenotypes based on the genes to which they are associated. This provides an empirical discovery of the natural phenotype ontology.« less","venue":"","listofauthors":"E. Baker, Zuopan Li, J. Jay, V. Philip, Yun Zhang, M. Langston, E. Chesler","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[3]},{"id":210,"title":"Ontological Discovery Environment: a system for integrating gene-phenotype associations.","doi":"10.1016/j.ygeno.2009.08.016","description":"The wealth of genomic technologies has enabled biologists to rapidly ascribe phenotypic characters to biological substrates. Central to effective biological investigation is the operational definition of the process under investigation. We propose an elucidation of categories of biological characters, including disease relevant traits, based on natural endogenous processes and experimentally observed biological networks, pathways and systems rather than on externally manifested constructs and current semantics such as disease names and processes. The Ontological Discovery Environment (ODE) is an Internet accessible resource for the storage, sharing, retrieval and analysis of phenotype-centered genomic data sets across species and experimental model systems. Any type of data set representing gene-phenotype relationships, such quantitative trait loci (QTL) positional candidates, literature reviews, microarray experiments, ontological or even meta-data, may serve as inputs. To demonstrate a use case leveraging the homology capabilities of ODE and its ability to synthesize diverse data sets, we conducted an analysis of genomic studies related to alcoholism. The core of ODE's gene set similarity, distance and hierarchical analysis is the creation of a bipartite network of gene-phenotype relations, a unique discrete graph approach to analysis that enables set-set matching of non-referential data. Gene sets are annotated with several levels of metadata, including community ontologies, while gene set translations compare models across species. Computationally derived gene sets are integrated into hierarchical trees based on gene-derived phenotype interdependencies. Automated set identifications are augmented by statistical tools which enable users to interpret the confidence of modeled results. This approach allows data integration and hypothesis discovery across multiple experimental contexts, regardless of the face similarity and semantic annotation of the experimental systems or species domain.","venue":"Genomics","listofauthors":"E. Baker, J. Jay, V. Philip, Yun Zhang, Zuopan Li, R. Kirova, M. Langston, E. Chesler","citations":43,"year":2009,"publisher":"Elsevier BV","pages":"377-387","volume":"94","number":"6","bibtex":"@article{2009,\n\tdoi = {10.1016/j.ygeno.2009.08.016},\n\turl = {https://doi.org/10.1016%2Fj.ygeno.2009.08.016},\n\tyear = 2009,\n\tmonth = {dec},\n\tpublisher = {Elsevier {BV}},\n\tvolume = {94},\n\tnumber = {6},\n\tpages = {377--387},\n\tauthor = {Erich J. Baker and Jeremy J. Jay and Vivek M. Philip and Yun Zhang and Zuopan Li and Roumyana Kirova and Michael A. Langston and Elissa J. Chesler},\n\ttitle = {Ontological discovery environment: A system for integrating gene{\\textendash}phenotype associations}\n}","authorsSemantic":[3]},{"id":211,"title":"Reuniting Families: An Online Database to Aid in the Identification of Undocumented Immigrant Remains *","doi":"10.1111/j.1556-4029.2007.00612.x","description":"Abstract:  The Reuniting Families project attempts to aid federal, state and local agencies currently working towards the identification of deceased undocumented immigrants. This initiative has created a distributed on‐line database, accessible by public officials and private citizens interested in searching for missing individuals based on both phenotypic and genotypic characteristics. This broad effort includes the exhumation of individuals from geographically disparate pauper graves, the classification of their physical characteristics, and the cataloging of observed metric traits in a local relational database, to include associated articles of possession and related metadata. Concurrent with the documentation of physical forensic evidence is the analysis of mitochondrial DNA sequences. Computational techniques and scoring parameters are applied to automate the process of discovery and identification as well at to preserve information on the missing. The result is a prototype knowledgebase that may serve as a model for future efforts in international forensic science collaborations.","venue":"Journal of forensic sciences","listofauthors":"L. Baker, E. Baker","citations":9,"year":2008,"publisher":"Wiley","pages":"50-53","volume":"53","number":"1","bibtex":"@article{2008,\n\tdoi = {10.1111/j.1556-4029.2007.00612.x},\n\turl = {https://doi.org/10.1111%2Fj.1556-4029.2007.00612.x},\n\tyear = 2008,\n\tmonth = {jan},\n\tpublisher = {Wiley},\n\tvolume = {53},\n\tnumber = {1},\n\tpages = {50--53},\n\tauthor = {Lori E. Baker and Erich J. Baker},\n\ttitle = {Reuniting Families: An Online Database to Aid in the Identification of Undocumented Immigrant Remains}\n}","authorsSemantic":[3]},{"id":231,"title":"Accelerating Lloyd’s Algorithm for k -Means Clustering","doi":"10.1007/978-3-319-09259-1_2","description":"The k-means clustering algorithm, a staple of data mining and unsupervised learning, is popular because it is simple to implement, fast, easily parallelized, and offers intuitive results. Lloyd’s algorithm is the standard batch, hill-climbing approach for minimizing the k-means optimization criterion. It spends a vast majority of its time computing distances between each of the k cluster centers and the n data points. It turns out that much of this work is unnecessary, because points usually stay in the same clusters after the first few iterations. In the last decade researchers have developed a number of optimizations to speed up Lloyd’s algorithm for both low- and high-dimensional data.In this chapter we survey some of these optimizations and present new ones. In particular we focus on those which avoid distance calculations by the triangle inequality. By caching known distances and updating them efficiently with the triangle inequality, these algorithms can provably avoid many unnecessary distance calculations. All the optimizations examined produce the same results as Lloyd’s algorithm given the same input and initialization, so are suitable as drop-in replacements. These new algorithms can run many times faster and compute far fewer distances than the standard unoptimized implementation. In our experiments, it is common to see speedups of over 30–50x compared to Lloyd’s algorithm. We examine the trade-offs for using these methods with respect to the number of examples n, dimensions d, clusters k, and structure of the data.","venue":"","listofauthors":"Greg Hamerly, Jonathan Drake","citations":69,"year":2014,"publisher":"Springer International Publishing","pages":"41-78","volume":null,"number":null,"bibtex":"@incollection{2014,\n\tdoi = {10.1007/978-3-319-09259-1_2},\n\turl = {https://doi.org/10.1007%2F978-3-319-09259-1_2},\n\tyear = 2014,\n\tmonth = {oct},\n\tpublisher = {Springer International Publishing},\n\tpages = {41--78},\n\tauthor = {Greg Hamerly and Jonathan Drake},\n\ttitle = {Accelerating Lloyd's Algorithm for k-Means Clustering}\n}","authorsSemantic":[5]},{"id":212,"title":"NFU-Enabled FASTA: moving bioinformatics applications onto wide area networks","doi":"10.1186/1751-0473-2-8","description":"BackgroundAdvances in Internet technologies have allowed life science researchers to reach beyond the lab-centric research paradigm to create distributed collaborations. Of the existing technologies that support distributed collaborations, there are currently none that simultaneously support data storage and computation as a shared network resource, enabling computational burden to be wholly removed from participating clients. Software using computation-enable logistical networking components of the Internet Backplane Protocol provides a suitable means to accomplish these tasks. Here, we demonstrate software that enables this approach by distributing both the FASTA algorithm and appropriate data sets within the framework of a wide area network.ResultsFor large datasets, computation-enabled logistical networks provide a significant reduction in FASTA algorithm running time over local and non-distributed logistical networking frameworks. We also find that genome-scale sizes of the stored data are easily adaptable to logistical networks.ConclusionNetwork function unit-enabled Internet Backplane Protocol effectively distributes FASTA algorithm computation over large data sets stored within the scaleable network. In situations where computation is subject to parallel solution over very large data sets, this approach provides a means to allow distributed collaborators access to a shared storage resource capable of storing the large volumes of data equated with modern life science. In addition, it provides a computation framework that removes the burden of computation from the client and places it within the network.","venue":"Source Code for Biology and Medicine","listofauthors":"E. Baker, Guan-Nan Lin, Huadong Liu, Ravi Kosuri","citations":2,"year":2007,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"2","number":"1","bibtex":"@article{2007,\n\tdoi = {10.1186/1751-0473-2-8},\n\turl = {https://doi.org/10.1186%2F1751-0473-2-8},\n\tyear = 2007,\n\tmonth = {nov},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {2},\n\tnumber = {1},\n\tauthor = {Erich J Baker and Guan N Lin and Huadong Liu and Ravi Kosuri},\n\ttitle = {{NFU}-Enabled {FASTA}: moving bioinformatics applications onto wide area networks}\n}","authorsSemantic":[3]},{"id":213,"title":"GeneKeyDB: A lightweight, gene-centric, relational database to support data mining environments","doi":"10.1186/1471-2105-6-72","description":"BackgroundThe analysis of biological data is greatly enhanced by existing or emerging databases. Most existing databases, with few exceptions are not designed to easily support large scale computational analysis, but rather offer exclusively a web interface to the resource. We have recognized the growing need for a database which can be used successfully as a backend to computational analysis tools and pipelines. Such database should be sufficiently versatile to allow easy system integration.ResultsGeneKeyDB is a gene-centered relational database developed to enhance data mining in biological data sets. The system provides an underlying data layer for computational analysis tools and visualization tools. GeneKeyDB relies primarily on existing database identifiers derived from community databases (NCBI, GO, Ensembl, et al.) as well as the known relationships among those identifiers. It is a lightweight, portable, and extensible platform for integration with computational tools and analysis environments.ConclusionGeneKeyDB can enable analysis tools and users to manipulate the intersections, unions, and differences among different data sets.","venue":"BMC Bioinformatics","listofauthors":"S. Kirov, X. Peng, E. Baker, D. Schmoyer, B. Zhang, J. Snoddy","citations":24,"year":2005,"publisher":"Springer Science and Business Media LLC","pages":"72","volume":"6","number":"1","bibtex":"@article{2005,\n\tdoi = {10.1186/1471-2105-6-72},\n\turl = {https://doi.org/10.1186%2F1471-2105-6-72},\n\tyear = 2005,\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {6},\n\tnumber = {1},\n\tpages = {72},\n\tauthor = {SA Kirov and X Peng and E Baker and D Schmoyer and B Zhang and J Snoddy}\n}","authorsSemantic":[3]},{"id":214,"title":"MuTrack: a genome analysis system for large-scale mutagenesis in the mouse","doi":"10.1186/1471-2105-5-11","description":"BackgroundModern biological research makes possible the comprehensive study and development of heritable mutations in the mouse model at high-throughput. Using techniques spanning genetics, molecular biology, histology, and behavioral science, researchers may examine, with varying degrees of granularity, numerous phenotypic aspects of mutant mouse strains directly pertinent to human disease states. Success of these and other genome-wide endeavors relies on a well-structured bioinformatics core that brings together investigators from widely dispersed institutions and enables them to seamlessly integrate data, observations and discussions.DescriptionMuTrack was developed as the bioinformatics core for a large mouse phenotype screening effort. It is a comprehensive collection of on-line computational tools and tracks thousands of mutagenized mice from birth through senescence and death. It identifies the physical location of mice during an intensive phenotype screening process at several locations throughout the state of Tennessee and collects raw and processed experimental data from each domain. MuTrack's statistical package allows researchers to access a real-time analysis of mouse pedigrees for aberrant behavior, and subsequent recirculation and retesting. The end result is the classification of potential and actual heritable mutant mouse strains that become immediately available to outside researchers who have expressed interest in the mutant phenotype.ConclusionMuTrack demonstrates the effectiveness of using bioinformatics techniques in data collection, integration and analysis to identify unique result sets that are beyond the capacity of a solitary laboratory. By employing the research expertise of investigators at several institutions for a broad-ranging study, the TMGC has amplified the effectiveness of any one consortium member. The bioinformatics strategy presented here lends future collaborative efforts a template for a comprehensive approach to large-scale analysis.","venue":"BMC Bioinformatics","listofauthors":"E. Baker, Leslie Galloway, Barbara Jackson, D. Schmoyer, J. Snoddy","citations":20,"year":2004,"publisher":"Springer Science and Business Media LLC","pages":"11","volume":"5","number":"1","bibtex":"@article{2004,\n\tdoi = {10.1186/1471-2105-5-11},\n\turl = {https://doi.org/10.1186%2F1471-2105-5-11},\n\tyear = 2004,\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {5},\n\tnumber = {1},\n\tpages = {11},\n\tauthor = {Erich J Baker and Leslie Galloway and Barbara Jackson and Denise Schmoyer and Jay Snoddy}\n}","authorsSemantic":[3]},{"id":215,"title":"The influence of evolutionary history on human health and disease","doi":"10.1038/s41576-020-00305-9","description":"Nearly all genetic variants that influence disease risk have human-specific origins; however, the systems they influence have ancient roots that often trace back to evolutionary events long before the origin of humans. Here, we review how advances in our understanding of the genetic architectures of diseases, recent human evolution and deep evolutionary history can help explain how and why humans in modern environments become ill. Human populations exhibit differences in the prevalence of many common and rare genetic diseases. These differences are largely the result of the diverse environmental, cultural, demographic and genetic histories of modern human populations. Synthesizing our growing knowledge of evolutionary history with genetic medicine, while accounting for environmental and social factors, will help to achieve the promise of personalized genomics and realize the potential hidden in an individual’s DNA sequence to guide clinical decisions. In short, precision medicine is fundamentally evolutionary medicine, and integration of evolutionary perspectives into the clinic will support the realization of its full potential.","venue":"Nature reviews. Genetics","listofauthors":"Mary Lauren Benton, A. Abraham, A. Labella, P. Abbot, A. Rokas, J. Capra","citations":8,"year":2021,"publisher":"Springer Science and Business Media LLC","pages":"269-283","volume":"22","number":"5","bibtex":"@article{2021,\n\tdoi = {10.1038/s41576-020-00305-9},\n\turl = {https://doi.org/10.1038%2Fs41576-020-00305-9},\n\tyear = 2021,\n\tmonth = {jan},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {22},\n\tnumber = {5},\n\tpages = {269--283},\n\tauthor = {Mary Lauren Benton and Abin Abraham and Abigail L. LaBella and Patrick Abbot and Antonis Rokas and John A. Capra},\n\ttitle = {The influence of evolutionary history on human health and disease}\n}","authorsSemantic":[4]},{"id":216,"title":"Epithelial-mesenchymal plasticity through loss of CTCF motif accessibility and protein expression","doi":"10.1101/2021.06.08.447526","description":"Epithelial-mesenchymal transition (EMT) and its reversal, mesenchymal-epithelial transition (MET) drive tissue reorganization critical for early development. In carcinomas, processing through EMT, MET or partial states promotes migration, invasion, dormancy, and metastatic colonization. As a reversible process, EMT is inherently regulated at epigenetic and epigenomic levels. To understand the epigenomic nature of reversible EMT and its partial states, we characterized chromatin accessibility dynamics, transcriptomic output, protein expression, and cellular phenotypes during stepwise reversible EMT. We found that the chromatin insulating protein machinery, including CTCF, is suppressed and re-expressed, coincident with broad alterations in chromatin accessibility, during EMT/MET and is lower in triple-negative breast cancer cell lines with EMT features. Through analysis of chromatin accessibility using ATAC-seq, we identify that early phases of EMT are characterized by enrichment for AP-1 family member binding motifs but also by diminished enrichment for CTCF binding motifs. Through loss-of-function analysis we demonstrate that suppression of CTCF alters cellular plasticity, facilitating entrance into a partial EMT state. These findings are indicative of a role of CTCF and chromatin reorganization for epithelial-mesenchymal plasticity.","venue":"","listofauthors":"Kelsey S. Johnson, S. Hussein, Priyanka Chakraborty, Arvind Muruganantham, Sheridan Mikhail, Giovanny Gonzalez, Shu-Sheng Song, M. Jolly, M. Toneff, Mary Lauren Benton, Yin C Lin, J. Taube","citations":1,"year":2021,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2021,\n\tdoi = {10.1101/2021.06.08.447526},\n\turl = {https://doi.org/10.1101%2F2021.06.08.447526},\n\tyear = 2021,\n\tmonth = {jun},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {Kelsey S. Johnson and Shaimaa Hussein and Priyanka Chakraborty and Arvind Muruganantham and Sheridan Mikhail and Giovanny Gonzalez and Shuxuan Song and Mohit Kumar Jolly and Michael J. Toneff and Mary Lauren Benton and Yin C. Lin and Joseph H. Taube},\n\ttitle = {Epithelial-mesenchymal plasticity through loss of {CTCF} motif accessibility and protein expression}\n}","authorsSemantic":[4]},{"id":217,"title":"Diverse functions associate with trans-species polymorphisms in humans","doi":"10.1101/2021.01.21.427090","description":"Long-term balancing selection (LTBS) can maintain allelic variation at a locus over millions of years and through speciation events. Variants shared between species, hereafter “trans-species polymorphisms” (TSPs), often result from LTBS due to host-pathogen interactions. For instance, the major histocompatibility complex (MHC) locus contains TSPs present across primates. Several hundred TSPs have been identified in humans and chimpanzees; however, because many are in non-coding regions of the genome, the functions and adaptive roles for most TSPs remain unknown. We integrated diverse genomic annotations to explore the functions of 125 previously identified non-coding TSPs that are likely under LTBS since the common ancestor of humans and chimpanzees. We analyzed genome-wide functional assays, expression quantitative trait loci (eQTL), genome-wide association studies (GWAS), and phenome-wide association studies (PheWAS). We identify functional annotations for 119 TSP regions, including 71 with evidence of gene regulatory function from GTEx or genome-wide functional genomics data and 21 with evidence of trait association from GWAS and PheWAS. TSPs in humans associate with many immune system phenotypes, including response to pathogens, but we also find associations with a range of other phenotypes, including body mass, alcohol intake, urate levels, chronotype, and risk-taking behavior. The diversity of traits associated with non-coding human TSPs suggest that functions beyond the immune system are often subject to LTBS. Furthermore, several of these trait associations provide support and candidate genetic loci for previous hypothesis about behavioral diversity in great ape populations, such as the importance of variation in sleep cycles and risk sensitivity. Significance statement Most genetic variants present in human populations are young (<100,000 years old); however, a few hundred are millions of years old with origins before the divergence of humans and chimpanzees. These trans-species polymorphisms (TSPs) were likely maintained by balancing selection—evolutionary pressure to maintain genetic diversity at a locus. However, the functions driving this selection, especially for non-coding TSPs, are largely unknown. We integrate genome-wide annotation strategies to demonstrate TSP associations with immune system function, behavior (addition, cognition, risky behavior), uric acid metabolism, and many other phenotypes. These results substantially expand our understanding of functions TSPs and suggest a substantial role for balancing selection beyond the immune system.","venue":"","listofauthors":"Keila Velázquez-Arcelay, Mary Lauren Benton, J. Capra","citations":0,"year":2021,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2021,\n\tdoi = {10.1101/2021.01.21.427090},\n\turl = {https://doi.org/10.1101%2F2021.01.21.427090},\n\tyear = 2021,\n\tmonth = {jan},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {Keila Vel{\\'{a}}zquez-Arcelay and Mary Lauren Benton and John A. Capra},\n\ttitle = {Diverse functions associate with trans-species polymorphisms in humans}\n}","authorsSemantic":[4]},{"id":218,"title":"Predictions, Pivots, and a Pandemic: a Review of 2020's Top Translational Bioinformatics Publications","doi":"10.1055/s-0041-1726540","description":"Summary Objectives: Provide an overview of the emerging themes and notable papers which were published in 2020 in the field of Bioinformatics and Translational Informatics (BTI) for the International Medical Informatics Association Yearbook. Methods: A team of 16 individuals scanned the literature from the past year. Using a scoring rubric, papers were evaluated on their novelty, importance, and objective quality. 1,224 Medical Subject Headings (MeSH) terms extracted from these papers were used to identify themes and research focuses. The authors then used the scoring results to select notable papers and trends presented in this manuscript. Results: The search phase identified 263 potential papers and central themes of coronavirus disease 2019 (COVID-19), machine learning, and bioinformatics were examined in greater detail. Conclusions: When addressing a once in a centruy pandemic, scientists worldwide answered the call, with informaticians playing a critical role. Productivity and innovations reached new heights in both TBI and science, but significant research gaps remain.","venue":"Yearbook of medical informatics","listofauthors":"S. P. Mcgrath, Mary Lauren Benton, Maryam Tavakoli, N. Tatonetti","citations":1,"year":2021,"publisher":"Georg Thieme Verlag KG","pages":"219-225","volume":"30","number":"01","bibtex":"@article{2021,\n\tdoi = {10.1055/s-0041-1726540},\n\turl = {https://doi.org/10.1055%2Fs-0041-1726540},\n\tyear = 2021,\n\tmonth = {aug},\n\tpublisher = {Georg Thieme Verlag {KG}},\n\tvolume = {30},\n\tnumber = {01},\n\tpages = {219--225},\n\tauthor = {Scott P. McGrath and Mary Lauren Benton and Maryam Tavakoli and Nicholas P. Tatonetti},\n\ttitle = {Predictions, Pivots, and a Pandemic: a Review of 2020{\\textquotesingle}s Top Translational Bioinformatics Publications}\n}","authorsSemantic":[4]},{"id":219,"title":"Diverse Functions Associate With Non-Coding Trans-species Polymorphisms in Humans","doi":"10.21203/RS.3.RS-559297/V1","description":"\n Background: Long-term balancing selection (LTBS) can maintain allelic variation at a locus over millions of years and through speciation events. Variants shared between species, hereafter “trans-species polymorphisms” (TSPs), often result from LTBS due to host-pathogen interactions. For instance, the major histocompatibility complex (MHC) locus contains TSPs present across primates. Several hundred candidate TSPs have been identified in humans and chimpanzees; however, because many are in non-coding regions of the genome, the functions and adaptive roles for most TSPs remain unknown. Results: We integrated diverse genomic annotations, with a focus on non-coding regions, to explore the functions of 125 previously identified regions containing multiple TSPs in humans and chimpanzees. We analyzed genome-wide functional assays, expression quantitative trait loci (eQTL), genome-wide association studies (GWAS), and phenome-wide association studies (PheWAS). We identify functional annotations for 119 TSP regions, including 71 with evidence of gene regulatory function from GTEx or genome-wide functional genomics data and 21 with evidence of trait association from GWAS and PheWAS. TSPs in humans associate with many immune system phenotypes, including response to pathogens, but we also find associations with a range of other phenotypes, including body mass, alcohol intake, urate levels, chronotype, and risk-taking behavior. Conclusions: The diversity of traits associated with non-coding human TSPs further support previous hypotheses that functions beyond the immune system are subject to LTBS. Furthermore, several of these trait associations provide support and candidate genetic loci for previous hypothesis about behavioral diversity in great ape populations, such as the importance of variation in sleep cycles and risk sensitivity.","venue":"","listofauthors":"Keila Velázquez-Arcelay, Mary Lauren Benton, J. Capra","citations":0,"year":2021,"publisher":"Research Square Platform LLC","pages":null,"volume":null,"number":null,"bibtex":"@article{2021,\n\tdoi = {10.21203/rs.3.rs-559297/v1},\n\turl = {https://doi.org/10.21203%2Frs.3.rs-559297%2Fv1},\n\tyear = 2021,\n\tmonth = {may},\n\tpublisher = {Research Square Platform {LLC}},\n\tauthor = {Keila Velazquez-Arcelay and Mary Lauren Benton and John A. Capra},\n\ttitle = {Diverse Functions Associate With Non-Coding Trans-species Polymorphisms in Humans}\n}","authorsSemantic":[4]},{"id":229,"title":"2 The Standard k-Means Algorithm does a Lot of Unnecessary Work","doi":null,"description":"The k-means clustering algorithm, a staple of data mining and unsupervised learning, is popular because it is simple to implement, fast, easily parallelized, and offers intuitive results. Lloyd’s algorithm is the standard batch, hill-climbing approach for minimizing the k-means optimization criterion. It spends a vast majority of its time computing distances between each of the k cluster centers and the n data points. It turns out that much of this work is unnecessary, because points usually stay in the same clusters after the first few iterations. In the last decade researchers have developed a number of optimizations to speed up Lloyd’s algorithm for both lowand high-dimensional data. In this chapter we survey some of these optimizations and present new ones. In particular we focus on those which avoid distance calculations by the triangle inequality. By caching known distances and updating them efficiently with the triangle inequality, these algorithms can provably avoid many unnecessary distance calculations. All the optimizations examined produce the same results as Lloyd’s algorithm given the same input and initialization, so are suitable as drop-in replacements. These new algorithms can run many times faster and compute far fewer distances than the standard unoptimized implementation. In our experiments, it is common to see speedups of over 30–50x compared to Lloyd’s algorithm. We examine the trade-offs for using these methods with respect to the number of examples n, dimensions d , clusters k, and structure of the data.","venue":"","listofauthors":"Greg Hamerly, Jonathan Drake","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":220,"title":"Functional annotation of rare structural variation in the human brain","doi":"10.1038/s41467-020-16736-1","description":"Structural variants (SVs) contribute to many disorders, yet, functionally annotating them remains a major challenge. Here, we integrate SVs with RNA-sequencing from human post-mortem brains to quantify their dosage and regulatory effects. We show that genic and regulatory SVs exist at significantly lower frequencies than intergenic SVs. Functional impact of copy number variants (CNVs) stems from both the proportion of genic and regulatory content altered and loss-of-function intolerance of the gene. We train a linear model to predict expression effects of rare CNVs and use it to annotate regulatory disruption of CNVs from 14,891 independent genome-sequenced individuals. Pathogenic deletions implicated in neurodevelopmental disorders show significantly more extreme regulatory disruption scores and if rank ordered would be prioritized higher than using frequency or length alone. This work shows the deleteriousness of regulatory SVs, particularly those altering CTCF sites and provides a simple approach for functionally annotating the regulatory consequences of CNVs. Structural variants (SVs) contribute to the genetic architecture of many brain-related disorders. Here, the authors integrate SV calls from genome sequencing (n = 755) with RNA-seq data (n = 629) from post-mortem dorsal lateral prefrontal cortex to annotate the gene regulatory effects of SVs in the human brain and their potential to contribute to disease.","venue":"Nature Communications","listofauthors":"Lide Han, Xuefang Zhao, Mary Lauren Benton, Thaneer Perumal, Ryan L. Collins, G. Hoffman, Jessica S. Johnson, L. Sloofman, Harold Z. Wang, M. Stone, Schahram Jaroslav Michael Kristen J. Leanne Andrew Joseph D Akbarian Bendl Breen Brennand Brown Browne Buxbaum, S. Akbarian, J. Bendl, M. Breen, K. Brennand, Leanne Brown, A. Browne, J. Buxbaum, A. Charney, A. Chess, Lizette Couto, G. Crawford, Olivia Devillers, B. Devlin, Amanda Dobbyn, E. Domenici, M. Filosi, E. Flatow, N. Francoeur, J. Fullard, S. Gil, K. Girdhar, A. Gulyás-Kovács, R. Gur, C. Hahn, V. Haroutunian, M. Hauberg, L. Huckins, Rivky Jacobov, Yan Jiang, Bibi S. Kassim, Yungil Kim, L. Klei, R. Kramer, Mario Lauria, T. Lehner, D. Lewis, B. Lipska, Kelsey S. Montgomery, Royce B Park, C. Rosenbluh","citations":9,"year":2020,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"11","number":"1","bibtex":"@article{2020,\n\tdoi = {10.1038/s41467-020-16736-1},\n\turl = {https://doi.org/10.1038%2Fs41467-020-16736-1},\n\tyear = 2020,\n\tmonth = {jun},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {11},\n\tnumber = {1},\n\tauthor = {Lide Han and Xuefang Zhao and Mary Lauren Benton and Thaneer Perumal and Ryan L. Collins and Gabriel E. Hoffman and Jessica S. Johnson and Laura Sloofman and Harold Z. Wang and Matthew R. Stone and Schahram Akbarian and Jaroslav Bendl and Michael Breen and Kristen J. Brennand and Leanne Brown and Andrew Browne and Joseph D. Buxbaum and Alexander Charney and Andrew Chess and Lizette Couto and Greg Crawford and Olivia Devillers and Bernie Devlin and Amanda Dobbyn and Enrico Domenici and Michele Filosi and Elie Flatow and Nancy Francoeur and John Fullard and Sergio Espeso Gil and Kiran Girdhar and Attila Guly{\\'{a}}s-Kov{\\'{a}}cs and Raquel Gur and Chang-Gyu Hahn and Vahram Haroutunian and Mads Engel Hauberg and Laura Huckins and Rivky Jacobov and Yan Jiang and Jessica S. Johnson and Bibi Kassim and Yungil Kim and Lambertus Klei and Robin Kramer and Mario Lauria and Thomas Lehner and David A. Lewis and Barbara K. Lipska and Kelsey Montgomery and Royce Park and Chaggai Rosenbluh and Panos Roussos and Douglas M. Ruderfer and Geetha Senthil and Hardik R. Shah and Laura Sloofman and Lingyun Song and Eli Stahl and Patrick Sullivan and Roberto Visintainer and Jiebiao Wang and Ying-Chih Wang and Jennifer Wiseman and Eva Xia and Wen Zhang and Elizabeth Zharovsky and Kristen J. Brennand and Harrison Brand and Solveig K. Sieberts and Stefano Marenco and Mette A. Peters and Barbara K. Lipska and Panos Roussos and John A. Capra and Michael Talkowski and Douglas M. Ruderfer and},\n\ttitle = {Functional annotation of rare structural variation in the human brain}\n}","authorsSemantic":[4]},{"id":221,"title":"Functional annotation of rare structural variation in the human brain","doi":"10.1101/711754","description":"Structural variants (SVs) contribute substantially to risk of many brain related disorders including autism and schizophrenia. However, annotating the potential contribution of SVs to disease remains a major challenge. Here, we integrated high resolution SV calling from genome-sequencing in 755 human post-mortem brains with dorsal lateral prefrontal cortex RNA-sequencing from a subset of 629 samples to quantify the dosage and regulatory effects of SVs. We show that genic (p = 5.44×10−9) and regulatory SVs (enhancer p = 3.22×10−23, CTCF p = 3.86×10−18) are present at significantly lower frequencies than intergenic SVs after correcting for SV length. Copy number variants (CNVs)—deletions and duplications—exhibit a significant quantitative and directional relationship between the proportion of genic and regulatory content altered and gene expression, and the size of the effect is inversely correlated with the loss-of-function intolerance of the gene. We trained a joint linear model that leverages genic and regulatory annotations to predict expression effects of rare CNVs in independent samples (R2 = 0.21-0.41). We further developed a regulatory disruption score for each CNV that aggregates the predicted expression across all affected genes weighted by the genes’ intolerance score and applied it to an independent set of SVs from 14,891 genome-sequenced individuals. Pathogenic deletions implicated in neurodevelopmental disorders by ClinGen had significantly more extreme regulatory disruption scores than the rest of the SVs. Rank ordering based on the most extreme regulatory disruption scores prioritized pathogenic deletions that would not have been prioritized by frequency or length alone. This work points to the deleteriousness of regulatory SVs, particularly those altering CTCF sites. We further provide a simple approach for functionally annotating the regulatory effects of SVs in the human brain that has potential to be useful in larger SV studies and should improve as more regulatory annotation data is generated.","venue":"","listofauthors":"Lide Han, Xuefang Zhao, Mary Lauren Benton, Thaneer Perumal, Ryan L. Collins, G. Hoffman, Jessica S. Johnson, L. Sloofman, Harold Z. Wang, K. Brennand, H. Brand, S. Sieberts, S. Marenco, M. Peters, B. Lipska, P. Roussos, J. Capra, M. Talkowski, D. Ruderfer","citations":10,"year":2019,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2019,\n\tdoi = {10.1101/711754},\n\turl = {https://doi.org/10.1101%2F711754},\n\tyear = 2019,\n\tmonth = {jul},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {Lide Han and Xuefang Zhao and Mary Lauren Benton and Thaneer Perumal and Ryan L. Collins and Gabriel E. Hoffman and Jessica S. Johnson and Laura Sloofman and Harold Z. Wang and Kristen J. Brennand and Harrison Brand and Solveig K. Sieberts and Stefano Marenco and Mette A. Peters and Barbara K. Lipska and Panos Roussos and John A. Capra and Michael Talkowski and Douglas M. Ruderfer and},\n\ttitle = {Functional annotation of rare structural variation in the human brain}\n}","authorsSemantic":[4]},{"id":222,"title":"Genome-wide enhancer annotations differ significantly in genomic distribution, evolution, and function","doi":"10.1186/s12864-019-5779-x","description":"BackgroundNon-coding gene regulatory enhancers are essential to transcription in mammalian cells. As a result, a large variety of experimental and computational strategies have been developed to identify cis-regulatory enhancer sequences. Given the differences in the biological signals assayed, some variation in the enhancers identified by different methods is expected; however, the concordance of enhancers identified by different methods has not been comprehensively evaluated. This is critically needed, since in practice, most studies consider enhancers identified by only a single method. Here, we compare enhancer sets from eleven representative strategies in four biological contexts.ResultsAll sets we evaluated overlap significantly more than expected by chance; however, there is significant dissimilarity in their genomic, evolutionary, and functional characteristics, both at the element and base-pair level, within each context. The disagreement is sufficient to influence interpretation of candidate SNPs from GWAS studies, and to lead to disparate conclusions about enhancer and disease mechanisms. Most regions identified as enhancers are supported by only one method, and we find limited evidence that regions identified by multiple methods are better candidates than those identified by a single method. As a result, we cannot recommend the use of any single enhancer identification strategy in all settings.ConclusionsOur results highlight the inherent complexity of enhancer biology and identify an important challenge to mapping the genetic architecture of complex disease. Greater appreciation of how the diverse enhancer identification strategies in use today relate to the dynamic activity of gene regulatory regions is needed to enable robust and reproducible results.","venue":"BMC genomics","listofauthors":"Mary Lauren Benton, Sai Charan Talipineni, D. Kostka, J. Capra","citations":21,"year":2019,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"20","number":"1","bibtex":"@article{2019,\n\tdoi = {10.1186/s12864-019-5779-x},\n\turl = {https://doi.org/10.1186%2Fs12864-019-5779-x},\n\tyear = 2019,\n\tmonth = {jun},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {20},\n\tnumber = {1},\n\tauthor = {Mary Lauren Benton and Sai Charan Talipineni and Dennis Kostka and John A. Capra},\n\ttitle = {Genome-wide enhancer annotations differ significantly in genomic distribution, evolution, and function}\n}","authorsSemantic":[4]},{"id":223,"title":"25 FUNCTIONAL ANNOTATION OF RARE STRUCTURAL VARIATION IN THE HUMAN BRAIN","doi":"10.1016/j.euroneuro.2019.07.166","description":"null","venue":"European Neuropsychopharmacology","listofauthors":"Lide Han, Xuefang Zhao, Mary Lauren Benton, Thaneer Perumal, Ryan L. Collins, H. Brand, G. Hoffman, Jessica S. Johnson, L. Sloofman, K. Brennand, S. Sieberts, S. Marenco, M. Peters, B. Lipska, P. Roussos, J. Capra, M. Talkowski, D. Ruderfer","citations":1,"year":2019,"publisher":"Elsevier BV","pages":"S72-S73","volume":"29","number":null,"bibtex":"@article{2019,\n\tdoi = {10.1016/j.euroneuro.2019.07.166},\n\turl = {https://doi.org/10.1016%2Fj.euroneuro.2019.07.166},\n\tyear = 2019,\n\tmonth = {oct},\n\tpublisher = {Elsevier {BV}},\n\tvolume = {29},\n\tpages = {S72--S73},\n\tauthor = {Lide Han and Xuefang Zhao and Mary Lauren Benton and Thaneer Perumal and Ryan Collins and Harrison Brand and Gabriel Hoffman and Jessica Johnson and Laura Sloofman and Kristen Brennand and Solveig Sieberts and Stefano Marenco and Mette Peters and Barbara K. Lipska and Panos Roussos and John Capra and Michael Talkowski and Douglas Ruderfer},\n\ttitle = {25 {FUNCTIONAL} {ANNOTATION} {OF} {RARE} {STRUCTURAL} {VARIATION} {IN} {THE} {HUMAN} {BRAIN}}\n}","authorsSemantic":[4]},{"id":230,"title":"Geometric methods to accelerate k-means algorithms","doi":"10.1137/1.9781611974348.37","description":"The k-means algorithm is popular for data clustering applications. Most implementations use Lloyd’s algorithm, which does many unnecessary distance calculations. Several accelerated algorithms (Elkan’s, Hamerly’s, heap, etc.) have recently been developed which produce exactly the same answer as Lloyd’s, only faster. They avoid redundant work using the triangle inequality paired with a set of lower and upper bounds on point-centroid distances. In this paper we propose several novel methods that allow those accelerated algorithms to perform even better, giving up to eight times further speedup. Our methods give tighter lower bound updates, efficiently skip centroids that cannot possibly be close to a set of points, keep extra information about upper bounds to help the heap algorithm avoid more distance computations, and decrease the number of distance calculations that are done in the first iteration.","venue":"SDM","listofauthors":"Petr Rysavý, Greg Hamerly","citations":7,"year":2016,"publisher":"Society for Industrial and Applied Mathematics","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1137/1.9781611974348.37},\n\turl = {https://doi.org/10.1137%2F1.9781611974348.37},\n\tyear = 2016,\n\tmonth = {jun},\n\tpublisher = {Society for Industrial and Applied Mathematics},\n\tauthor = {Petr Ry{\\v{s}}av{\\'{y}} and Greg Hamerly},\n\ttitle = {Geometric methods to accelerate k-means algorithms}\n}","authorsSemantic":[5]},{"id":224,"title":"Genome-wide Enhancer Maps Differ Significantly in Genomic Distribution, Evolution, and Function","doi":"10.1101/176610","description":"Non-coding gene regulatory enhancers are essential to transcription in mammalian cells. As a result, a large variety of experimental and computational strategies have been developed to identify cis-regulatory enhancer sequences. In practice, most studies consider enhancers identified by only a single method, and the concordance of enhancers identified by different methods has not been comprehensively evaluated. Here, we assess the similarities of enhancer sets identified by ten representative strategies in four biological contexts and evaluate the robustness of downstream conclusions to the choice of identification strategy. All pairs of enhancer sets we evaluated overlap significantly more than expected by chance; however, we also found significant dissimilarity between enhancer sets in their genomic characteristics, evolutionary conservation, and association with functional loci within each context. We find most regions identified as enhancers are supported by only one method. The disagreement is sufficient to influence interpretation of GWAS SNPs and eQTL, and to lead to disparate conclusions about enhancer biology and disease mechanisms. We also find only limited evidence that regions identified by multiple enhancer identification methods are better candidates than those identified by a single method. Our results highlight the inherent complexity of enhancer biology and argue that current approaches have yet to adequately account for enhancer diversity. As a result, we cannot recommend the use of any single enhancer identification strategy in isolation. To facilitate assessment of enhancer diversity on studies’ conclusions, we developed creDB, a database of enhancer annotations designed to integrate into bioinformatics workflows. While our findings highlight a major challenge to mapping the genetic architecture of complex disease and interpreting regulatory variants found in patient genomes, a systematic understanding of similarities and differences in enhancer identification methodology will ultimately enable robust inferences about gene regulatory sequences.","venue":"","listofauthors":"Mary Lauren Benton, Sai Charan Talipineni, D. Kostka, J. Capra","citations":4,"year":2017,"publisher":"Cold Spring Harbor Laboratory","pages":null,"volume":null,"number":null,"bibtex":"@article{2017,\n\tdoi = {10.1101/176610},\n\turl = {https://doi.org/10.1101%2F176610},\n\tyear = 2017,\n\tmonth = {aug},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tauthor = {Mary Lauren Benton and Sai Charan Talipineni and Dennis Kostka and John A. Capra},\n\ttitle = {Genome-wide Enhancer Maps Differ Significantly in Genomic Distribution, Evolution, and Function}\n}","authorsSemantic":[4]},{"id":225,"title":"Genome-wide Enhancer Maps Differ Significantly in their Genomic Distribution, Evolution, and Function By","doi":null,"description":"null","venue":"","listofauthors":"Mary Lauren Benton, J. Capra, E. Hodges, J. Hughey","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[4]},{"id":226,"title":"Epo reprograms the epigenome of erythroid cells.","doi":"10.1016/j.exphem.2017.03.004","description":"The hormone erythropoietin (Epo) is required for erythropoiesis, yet its molecular mechanism of action remains poorly understood, particularly with respect to chromatin dynamics. To investigate how Epo modulates the erythroid epigenome, we performed epigenetic profiling using an ex vivo murine cell system that undergoes synchronous erythroid maturation in response to Epo stimulation. Our findings define the repertoire of Epo-modulated enhancers, illuminating a new facet of Epo signaling. First, a large number of enhancers rapidly responded to Epo stimulation, revealing a cis-regulatory network of Epo-responsive enhancers. In contrast, most of the other identified enhancers remained in an active acetylated state during Epo signaling, suggesting that most erythroid enhancers are established at an earlier precursor stage. Second, we identified several hundred super-enhancers that were linked to key erythroid genes, such as Tal1, Bcl11a, and Mir144/451. Third, experimental and computational validation revealed that many predicted enhancer regions were occupied by TAL1 and enriched with DNA-binding motifs for GATA1, KLF1, TAL1/E-box, and STAT5. Additionally, many of these cis-regulatory regions were conserved evolutionarily and displayed correlated enhancer:promoter acetylation. Together, these findings define a cis-regulatory enhancer network for Epo signaling during erythropoiesis, and provide the framework for future studies involving the interplay of epigenetics and Epo signaling.","venue":"Experimental hematology","listofauthors":"Andrea A. Perreault, Mary Lauren Benton, M. Koury, S. Brandt, Bryan J Venters","citations":11,"year":2017,"publisher":"Elsevier BV","pages":"47-62","volume":"51","number":null,"bibtex":"@article{2017,\n\tdoi = {10.1016/j.exphem.2017.03.004},\n\turl = {https://doi.org/10.1016%2Fj.exphem.2017.03.004},\n\tyear = 2017,\n\tmonth = {jul},\n\tpublisher = {Elsevier {BV}},\n\tvolume = {51},\n\tpages = {47--62},\n\tauthor = {Andrea A. Perreault and Mary Lauren Benton and Mark J. Koury and Stephen J. Brandt and Bryan J. Venters},\n\ttitle = {Epo reprograms the epigenome of erythroid cells}\n}","authorsSemantic":[4]},{"id":227,"title":"Autonomous early detection of eye disease in childhood photographs","doi":"10.1126/sciadv.aax6363","description":"A smartphone app searches baby pictures for common and rare eye disorders. The “red reflex test” is used to screen children for leukocoria (“white eye”) in a standard pediatric examination, but is ineffective at detecting many eye disorders. Leukocoria also presents in casual photographs. The clinical utility of screening photographs for leukocoria is unreported. Here, a free smartphone application (CRADLE: ComputeR-Assisted Detector of LEukocoria) was engineered to detect photographic leukocoria and is available for download under the name “White Eye Detector.” This study determined the sensitivity, specificity, and accuracy of CRADLE by retrospectively analyzing 52,982 longitudinal photographs of children, collected by parents before enrollment in this study. The cohort included 20 children with retinoblastoma, Coats’ disease, cataract, amblyopia, or hyperopia and 20 control children. For 80% of children with eye disorders, the application detected leukocoria in photographs taken before diagnosis by 1.3 years (95% confidence interval, 0.4 to 2.3 years). The CRADLE application allows parents to augment clinical leukocoria screening with photography.","venue":"Science Advances","listofauthors":"Micheal C. Munson, Devon L. Plewman, Katelyn M. Baumer, Ryan Henning, Collin T Zahler, Alexander T. Kietzman, Alexandra A. Beard, S. Mukai, L. Diller, Greg Hamerly, Bryan F. Shaw","citations":12,"year":2019,"publisher":"American Association for the Advancement of Science (AAAS)","pages":null,"volume":"5","number":"10","bibtex":"@article{2019,\n\tdoi = {10.1126/sciadv.aax6363},\n\turl = {https://doi.org/10.1126%2Fsciadv.aax6363},\n\tyear = 2019,\n\tmonth = {oct},\n\tpublisher = {American Association for the Advancement of Science ({AAAS})},\n\tvolume = {5},\n\tnumber = {10},\n\tauthor = {Micheal C. Munson and Devon L. Plewman and Katelyn M. Baumer and Ryan Henning and Collin T. Zahler and Alexander T. Kietzman and Alexandra A. Beard and Shizuo Mukai and Lisa Diller and Greg Hamerly and Bryan F. Shaw},\n\ttitle = {Autonomous early detection of eye disease in childhood photographs}\n}","authorsSemantic":[5]},{"id":228,"title":"Chapter 2 Accelerating Lloyd ’ s Algorithm for k-Means Clustering","doi":null,"description":"The k-means clustering algorithm, a staple of data mining and unsupervised learning, is popular because it is simple to implement, fast, easily parallelized, and offers intuitive results. Lloyd’s algorithm is the standard batch, hill-climbing approach for minimizing the k-means optimization criterion. It spends a vast majority of its time computing distances between each of the k cluster centers and the n data points. It turns out that much of this work is unnecessary, because points usually stay in the same clusters after the first few iterations. In the last decade researchers have developed a number of optimizations to speed up Lloyd’s algorithm for both lowand high-dimensional data. In this chapter we survey some of these optimizations and present new ones. In particular we focus on those which avoid distance calculations by the triangle inequality. By caching known distances and updating them efficiently with the triangle inequality, these algorithms can provably avoid many unnecessary distance calculations. All the optimizations examined produce the same results as Lloyd’s algorithm given the same input and initialization, so are suitable as drop-in replacements. These new algorithms can run many times faster and compute far fewer distances than the standard unoptimized implementation. In our experiments, it is common to see speedups of over 30–50x compared to Lloyd’s algorithm. We examine the trade-offs for using these methods with respect to the number of examples n, dimensions d , clusters k, and structure of the data.","venue":"","listofauthors":"Greg Hamerly, Jonathan Drake","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":232,"title":"A Convolutional Neural Network approach for classifying leukocoria","doi":"10.1109/SSIAI.2014.6806016","description":"We use Convolutional Neural Networks to detect leukocoria, or white-eye reflections, in recreational photography. Leukocoria is the most prominent symptom of retinoblastoma, a solid-tumor cancer of the eye that occurs most often in young children. We trained several networks for the task, using training images downloaded from Flickr. We achieved low error rates (<;3%) for classification of eye images into three classes: normal, leukocoric, and pseudo-leukocoric. We also provide a method for tuning the outputs of a trained network to match desired true-positive/false-positive rates.","venue":"2014 Southwest Symposium on Image Analysis and Interpretation","listofauthors":"Ryan Henning, P. Rivas-Perea, Bryan F. Shaw, Greg Hamerly","citations":7,"year":2014,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2014,\n\tdoi = {10.1109/ssiai.2014.6806016},\n\turl = {https://doi.org/10.1109%2Fssiai.2014.6806016},\n\tyear = 2014,\n\tmonth = {apr},\n\tpublisher = {{IEEE}},\n\tauthor = {Ryan Henning and Pablo Rivas-Perea and Bryan Shaw and Greg Hamerly},\n\ttitle = {A Convolutional Neural Network approach for classifying leukocoria}\n}","authorsSemantic":[5]},{"id":233,"title":"Finding the smallest circle containing the iris in the denoised wavelet domain","doi":"10.1109/SSIAI.2014.6806017","description":"Retinoblastoma is a pediatric ocular cancer typically indicated by leukocoria (white-eye pupillary reflex). Early detection of leukocoria can improve health outcomes when it indicates disease, and it can be easily seen in recreational photographs. As part of a system for automatic leukocoria detection, we propose an image processing algorithm for detecting the exact location and radius of the smallest circle containing the iris in an eye image. Our algorithms use both median filters and two-dimensional stationary wavelet transforms and achieve low error rates.","venue":"2014 Southwest Symposium on Image Analysis and Interpretation","listofauthors":"P. Rivas-Perea, Ryan Henning, Bryan F. Shaw, Greg Hamerly","citations":6,"year":2014,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2014,\n\tdoi = {10.1109/ssiai.2014.6806017},\n\turl = {https://doi.org/10.1109%2Fssiai.2014.6806017},\n\tyear = 2014,\n\tmonth = {apr},\n\tpublisher = {{IEEE}},\n\tauthor = {Pablo Rivas-Perea and Ryan Henning and Bryan Shaw and Greg Hamerly},\n\ttitle = {Finding the smallest circle containing the iris in the denoised wavelet domain}\n}","authorsSemantic":[5]},{"id":234,"title":"Colorimetric Image Analysis in Detection of Leukocoria from Retinoblastoma in Snapshots Taken by Standard Digital Photography","doi":null,"description":"null","venue":"","listofauthors":"Katherine E. Talcott, Elizabeth V. Shaw, Rebecca L. Holden, Brandon W. Taylor, E. Baker, Greg Hamerly, A. Kentsis, S. Mukai, C. Rodriguez-Galindo, Bryan F. Shaw","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":235,"title":"Making k-means Even Faster","doi":"10.1137/1.9781611972801.12","description":"The k-means algorithm is widely used for clustering, compressing, and summarizing vector data. In this paper, we propose a new acceleration for exact k-means that gives the same answer, but is much faster in practice. Like Elkan’s accelerated algorithm [8], our algorithm avoids distance computations using distance bounds and the triangle inequality. Our algorithm uses one novel lower bound for point-center distances, which allows it to eliminate the innermost k-means loop 80% of the time or more in our experiments. On datasets of low and medium dimension (e.g. up to 50 dimensions), our algorithm is much faster than other methods, including methods based on low-dimensional indexes, such as k-d trees. Other advantages are that it is very simple to implement and it has a very small memory overhead, much smaller than other accelerated algorithms.","venue":"SDM","listofauthors":"Greg Hamerly","citations":161,"year":2010,"publisher":"Society for Industrial and Applied Mathematics","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2010,\n\tdoi = {10.1137/1.9781611972801.12},\n\turl = {https://doi.org/10.1137%2F1.9781611972801.12},\n\tyear = 2010,\n\tmonth = {apr},\n\tpublisher = {Society for Industrial and Applied Mathematics},\n\tauthor = {Greg Hamerly},\n\ttitle = {Making k-means even faster}\n}","authorsSemantic":[5]},{"id":236,"title":"Efficient Model Selection for Large-Scale Nearest-Neighbor Data Mining","doi":"10.1007/978-3-642-25704-9_6","description":"One of the most widely used models for large-scale data mining is the k-nearest neighbor (k-nn) algorithm. It can be used for classification, regression, density estimation, and information retrieval. To use k-nn, a practitioner must first choose k, usually selecting the k with the minimal loss estimated by cross-validation. In this work, we begin with an existing but little-studied method that greatly accelerates the cross-validation process for selecting k from a range of user-provided possibilities. The result is that a much larger range of k values may be examined more quickly. Next, we extend this algorithm with an additional optimization to provide improved performance for locally linear regression problems. We also show how this method can be applied to automatically select the range of k values when the user has no a priori knowledge of appropriate bounds. Furthermore, we apply statistical methods to reduce the number of examples examined while still finding a likely best k, greatly improving performance for large data sets. Finally, we present both analytical and experimental results that demonstrate these benefits.","venue":"BNCOD","listofauthors":"Greg Hamerly, G. Speegle","citations":8,"year":2012,"publisher":"Springer Berlin Heidelberg","pages":"37-54","volume":null,"number":null,"bibtex":"@incollection{2012,\n\tdoi = {10.1007/978-3-642-25704-9_6},\n\turl = {https://doi.org/10.1007%2F978-3-642-25704-9_6},\n\tyear = 2012,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {37--54},\n\tauthor = {Greg Hamerly and Greg Speegle},\n\ttitle = {Efficient Model Selection for Large-Scale Nearest-Neighbor Data Mining}\n}","authorsSemantic":[5,10]},{"id":237,"title":"Representative Sampling Using SimPoint","doi":"10.1007/978-1-4419-6175-4_10","description":"SimPoint is a technique used to pick what parts of the program’s execution to simulate in order to have a complete picture of execution. SimPoint uses data clustering algorithms from machine learning to automatically find repetitive (similar) patterns in a program’s execution, and it chooses one sample to represent each unique repetitive behavior. Each sample is then simulated and weighted appropriately, and then together the results from these samples represent an accurate picture of the complete execution of the program.","venue":"","listofauthors":"Greg Hamerly, Erez Perelman, T. Sherwood, B. Calder","citations":0,"year":2010,"publisher":"Springer US","pages":"161-177","volume":null,"number":null,"bibtex":"@incollection{2010,\n\tdoi = {10.1007/978-1-4419-6175-4_10},\n\turl = {https://doi.org/10.1007%2F978-1-4419-6175-4_10},\n\tyear = 2010,\n\tpublisher = {Springer {US}},\n\tpages = {161--177},\n\tauthor = {Greg Hamerly and Erez Perelman and Timothy Sherwood and Brad Calder},\n\ttitle = {Representative Sampling Using {SimPoint}}\n}","authorsSemantic":[5]},{"id":238,"title":"Hierarchical Stability-Based Model Selection for Clustering Algorithms","doi":"10.1109/ICMLA.2009.64","description":"We present an algorithm called HS-means which is able to learn the number of clusters in a mixture model. Our method extends the concept of clustering stability to a concept of hierarchical stability. The method chooses a model for the data based on analysis of clustering stability; it then analyzes the stability of each component in the estimated model and chooses a stable model for this component. It continues this recursive stability analysis until all the estimated components are unimodal. In so doing, the method is able to handle hierarchical and symmetric data that existing stability-based algorithms have difficulty with. We test our algorithm on both synthetic datasets and real world datasets. The results show that HS-means outperforms a popular stability-based model selection algorithm, both in terms of handling symmetric data and finding high-quality clusterings in the task of predicting CPU performance.","venue":"2009 International Conference on Machine Learning and Applications","listofauthors":"Bingbei Yin, Greg Hamerly","citations":0,"year":2009,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2009,\n\tdoi = {10.1109/icmla.2009.64},\n\turl = {https://doi.org/10.1109%2Ficmla.2009.64},\n\tyear = 2009,\n\tmonth = {dec},\n\tpublisher = {{IEEE}},\n\tauthor = {Bing Yin and Greg Hamerly},\n\ttitle = {Hierarchical Stability-Based Model Selection for Clustering Algorithms}\n}","authorsSemantic":[5]},{"id":239,"title":"Improving SimPoint accuracy for small simulation budgets with EDCM clustering","doi":null,"description":"Detailed processor simulation is extremely costly on large benchmark suites, where each program may run for billions of instructions and take months of simulation time. We can obtain good approximate answers in less time using limited simulation, but deciding which regions to simulate is a difficult problem. SimPoint is one approach for choosing simulation regions, based on the k-means clustering algorithm. We propose using an alternative clustering model based on a mixture of exponential Dirichlet compound multinomial (EDCM) models. This method outperforms k-means in performance prediction accuracy when simulation budgets are limited. The EDCM mixture can cluster highdimension frequency vector data directly, without dimension reduction, and trains quickly.","venue":"","listofauthors":"Joshua Johnston, Greg Hamerly","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":240,"title":"Cross Binary Simulation Points","doi":"10.1109/ISPASS.2007.363748","description":"Architectures are usually compared by running the same workload on each architecture and comparing performance. When a single compiled binary of a program is executed on many different architectures, techniques like SimPoint can be used to find a small set of samples that represent the majority of the program's execution. Architectures can be compared by simulating their behavior on the code samples selected by SimPoint, to quickly determine which architecture has the best performance. Architectural design space exploration becomes more difficult when different binaries must be used for the same program. These cases arise when evaluating architectures that include ISA extensions, and when evaluating compiler optimizations. This problem domain is the focus of our paper. When multiple binaries are used to evaluate a program, one approach is to create a separate set of simulation points for each binary. This approach works reasonably well for many applications, but breaks down when the simulation points chosen for the different binaries emphasize different parts of the program's execution. This problem can be avoided if simulation points are selected consistently across the different binaries, to ensure that the same parts of program execution are represented in all binaries. In this paper we present an approach that finds a single set of simulation points to be used across all binaries for a single program. This allows for simulation of the same parts of program execution despite changes in the binary due to ISA changes or compiler optimizations","venue":"2007 IEEE International Symposium on Performance Analysis of Systems & Software","listofauthors":"Erez Perelman, Jeremy Lau, H. Patil, A. Jaleel, Greg Hamerly, B. Calder","citations":17,"year":2007,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2007,\n\tdoi = {10.1109/ispass.2007.363748},\n\turl = {https://doi.org/10.1109%2Fispass.2007.363748},\n\tyear = 2007,\n\tmonth = {apr},\n\tpublisher = {{IEEE}},\n\tauthor = {Erez Perelman and Jeremy Lau and Harish Patil and Aamer Jaleel and Greg Hamerly and Brad Calder},\n\ttitle = {Cross Binary Simulation Points}\n}","authorsSemantic":[5]},{"id":241,"title":"PG-means: learning the number of clusters in data","doi":"10.7551/mitpress/7503.003.0054","description":"We present a novel algorithm called PG-means which is able to learn the number of clusters in a classical Gaussian mixture model. Our method is robust and efficient; it uses statistical hypothesis tests on one-dimensional projections of the data and model to determine if the examples are well represented by the model. In so doing, we are applying a statistical test for the entire model at once, not just on a per-cluster basis. We show that our method works well in difficult cases such as non-Gaussian data, overlapping clusters, eccentric clusters, high dimension, and many true clusters. Further, our new method provides a much more stable estimate of the number of clusters than existing methods.","venue":"NIPS","listofauthors":"Y. Feng, Greg Hamerly","citations":62,"year":2007,"publisher":"The MIT Press","pages":null,"volume":null,"number":null,"bibtex":"@incollection{2007,\n\tdoi = {10.7551/mitpress/7503.003.0054},\n\turl = {https://doi.org/10.7551%2Fmitpress%2F7503.003.0054},\n\tyear = 2007,\n\tpublisher = {The {MIT} Press},\n\ttitle = {{PG}-means: learning the number of clusters in data}\n}","authorsSemantic":[5]},{"id":242,"title":"Comparing multinomial and k-means clustering for SimPoint","doi":"10.1109/ISPASS.2006.1620798","description":"SimPoint is a technique used to pick what parts of the program's execution to simulate in order to have a complete picture of execution. SimPoint uses data clustering algorithms from machine learning to automatically find repetitive (similar) patterns in a program's execution, and it chooses one sample to represent each unique repetitive behavior. Together these samples represent an accurate picture of the complete execution of the program. SimPoint is based on the k-means clustering algorithm; recent work proposed using a different clustering method based on multinomial models, but only provided a preliminary comparison and analysis. In this work we provide a detailed comparison of using k-means and multinomial clustering for SimPoint. We show that k-means performs better than the recently proposed multinomial clustering approach. We then propose two improvements to the prior multinomial clustering approach in the areas of feature reduction and the picking of simulation points which allow multinomial clustering to perform as well as k-means. We then conclude by examining how to potentially combine multinomial clustering with k-means.","venue":"2006 IEEE International Symposium on Performance Analysis of Systems and Software","listofauthors":"Greg Hamerly, Erez Perelman, B. Calder","citations":7,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/ispass.2006.1620798},\n\turl = {https://doi.org/10.1109%2Fispass.2006.1620798},\n\tpublisher = {{IEEE}},\n\tauthor = {G. Hamerly and E. Perelman and B. Calder},\n\ttitle = {Comparing multinomial and K-means clustering for {SimPoint}}\n}","authorsSemantic":[5]},{"id":243,"title":"Using Machine Learning to Guide Architecture Simulation","doi":null,"description":"An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efficient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. \n \nTo address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically find repetitive patterns in a program's execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint.","venue":"J. Mach. Learn. Res.","listofauthors":"Greg Hamerly, Erez Perelman, Jeremy Lau, B. Calder, T. Sherwood","citations":27,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":244,"title":"SimPoint 3.0: Faster and More Flexible Program Analysis","doi":null,"description":"This paper describes the new features available in the SimPoint 3.0 release. The release provides two techniques for drastically reducing the run-time of SimPoint: faster searching to find the best clustering, and efficiently clustering large numbers of intervals. SimPoint 3.0 also provides an option to output only the simulation points that represent the majority of execution, which can reduce simulation time without much increase in error. Finally, this release provides support for correctly clustering variable length intervals, taking into consideration the weight of each interval during clustering. This paper describes SimPoint 3.0’s new features, how to use them, and points out some common pitfalls.","venue":"","listofauthors":"Greg Hamerly, Erez Perelman, Jeremy Lau, B. Calder","citations":164,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":245,"title":"SimPoint: Picking Representative Samples to Guide Simulation","doi":"10.1201/9781420037425.CH7","description":"null","venue":"","listofauthors":"Greg Hamerly, T. Sherwood, B. Calder, Erez Perelman","citations":8,"year":2005,"publisher":"CRC Press","pages":"117-138","volume":null,"number":null,"bibtex":"@incollection{2005,\n\tdoi = {10.1201/9781420037425.ch7},\n\turl = {https://doi.org/10.1201%2F9781420037425.ch7},\n\tyear = 2005,\n\tmonth = {sep},\n\tpublisher = {{CRC} Press},\n\tpages = {117--138},\n\tauthor = {Greg Hamerly and Timothy Sherwood and Brad Calder and Erez Perelman},\n\ttitle = {{SimPoint}}\n}","authorsSemantic":[5]},{"id":246,"title":"SimPoint 3.0: Faster and More Flexible Program Phase Analysis","doi":null,"description":"This paper describes the new features available in the SimPoint 3.0 release. The release provides two techniques for drastically reducing the run-time of SimPoint: faster searching to find the best clustering, and efficiently clustering large numbers of intervals. SimPoint 3.0 also provides an option to output only the simulation points that represent the majority of execution, which can reduce simulation time without much increase in error. Finally, this release provides support for correctly clustering variable length intervals, taking into consideration the weight of each interval during clustering. This paper describes SimPoint 3.0’s new features, how to use them, and points out some common pitfalls.","venue":"J. Instr. Level Parallelism","listofauthors":"Greg Hamerly, Erez Perelman, Jeremy Lau, B. Calder","citations":281,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":247,"title":"The Strong correlation Between Code Signatures and Performance","doi":"10.1109/ISPASS.2005.1430578","description":"A recent study examined the use of sampled hardware counters to create sampled code signatures. This approach is attractive because sampled code signatures can be quickly gathered for any application. The conclusion of their study was that there exists a fuzzy correlation between sampled code signatures and performance predictability. The paper raises the question of how much information is lost in the sampling process, and our paper focuses on examining this issue. We first focus on showing that there exists a strong correlation between code signatures and performance. We then examine the relationship between sampled and full code signatures, and how these affect performance predictability. Our results confirm that there is a fuzzy correlation found in recent work for the SPEC programs with sampled code signatures, but that a strong correlation exists with full code signatures. In addition, we propose converting the sampled instruction counts, used in the prior work, into sampled code signatures representing loop and procedure execution frequencies. These sampled loop and procedure code signatures allow phase analysis to more accurately and easily find patterns, and they correlate better with performance","venue":"IEEE International Symposium on Performance Analysis of Systems and Software, 2005. ISPASS 2005.","listofauthors":"Jeremy Lau, J. Sampson, Erez Perelman, Greg Hamerly, B. Calder","citations":103,"year":2005,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2005,\n\tdoi = {10.1109/ispass.2005.1430578},\n\turl = {https://doi.org/10.1109%2Fispass.2005.1430578},\n\tyear = 2005,\n\tpublisher = {{IEEE}},\n\tauthor = {J. Lau and J. Sampson and E. Perelman and G. Hamerly and B. Calder},\n\ttitle = {The Strong correlation Between Code Signatures and Performance}\n}","authorsSemantic":[5]},{"id":248,"title":"Motivation for Variable Length Intervals and Hierarchical Phase Behavior","doi":"10.1109/ISPASS.2005.1430568","description":"Most programs are repetitive, where similar behavior can be seen at different execution times. Proposed algorithms automatically group similar portions of a program's execution into phases, where the intervals in each phase have homogeneous behavior and similar resource requirements. These prior techniques focus on fixed length intervals (such as a hundred million instructions) to find phase behavior. Fixed length intervals can make a program's periodic phase behavior difficult to find, because the fixed interval length can be out of sync with the period of the program's actual phase behavior. In addition, a fixed interval length can only express one level of phase behavior. In this paper, we graphically show that there exists a hierarchy of phase behavior in programs and motivate the need for variable length intervals. We describe the changes applied to SimPoint to support variable length intervals. We finally conclude by providing an initial study into using variable length intervals to guide SimPoint","venue":"IEEE International Symposium on Performance Analysis of Systems and Software, 2005. ISPASS 2005.","listofauthors":"Jeremy Lau, Erez Perelman, Greg Hamerly, T. Sherwood, B. Calder","citations":89,"year":2005,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2005,\n\tdoi = {10.1109/ispass.2005.1430568},\n\turl = {https://doi.org/10.1109%2Fispass.2005.1430568},\n\tyear = 2005,\n\tpublisher = {{IEEE}},\n\tauthor = {J. Lau and E. Perelman and G. Hamerly and T. Sherwood and B. Calder},\n\ttitle = {Motivation for Variable Length Intervals and Hierarchical Phase Behavior}\n}","authorsSemantic":[5]},{"id":249,"title":"How to use SimPoint to pick simulation points","doi":"10.1145/1054907.1054913","description":"Understanding the cycle level behavior of a processor running an application is crucial to modern computer architecture research. To gain this understanding, detailed cycle level simulators are typically employed. Unfortunately, this level of detail comes at the cost of speed, and simulating the full execution of an industry standard benchmark on even the fastest simulator can take weeks to months to complete. This fact has not gone unnoticed, and several techniques have been developed aimed at reducing simulation time.","venue":"PERV","listofauthors":"Greg Hamerly, Erez Perelman, B. Calder","citations":69,"year":2004,"publisher":"Association for Computing Machinery (ACM)","pages":"25-30","volume":"31","number":"4","bibtex":"@article{2004,\n\tdoi = {10.1145/1054907.1054913},\n\turl = {https://doi.org/10.1145%2F1054907.1054913},\n\tyear = 2004,\n\tmonth = {mar},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {31},\n\tnumber = {4},\n\tpages = {25--30},\n\tauthor = {Greg Hamerly and Erez Perelman and Brad Calder},\n\ttitle = {How to use {SimPoint} to pick simulation points}\n}","authorsSemantic":[5]},{"id":250,"title":"Exploring Perceptron-Based Register Value Prediction","doi":null,"description":"Register value prediction has been proposed as a technique to exploit register value reuse, a form of locality where the result produced by an instruction is the same as the value that is already in a destination register or other registers in the register file. Register value prediction allows increased performance by breaking true dependencies between an instruction that exhibits this locality and its dependents. This paper presents a study into using perceptron-based predictors to guide one form of register value prediction. For a given storage budget, we find that on the average a perceptron predictor performs better than previously proposed register value predictors. Secondly, we demonstrate the impact of perceptron history length on register value prediction. Lastly, we analyze predictor structures which improve upon previous predictors targeting register value reuse. With a 4KB hybrid perceptron predictor we show an average speedup of 7.5% for the benchmarks studied.","venue":"","listofauthors":"J. Seng, Greg Hamerly","citations":4,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":251,"title":"Learning the k in k-means","doi":null,"description":"When clustering a dataset, the right number k of clusters to use is often not obvious, and choosing k automatically is a hard algorithmic problem. In this paper we present an improved algorithm for learning k while clustering. The G-means algorithm is based on a statistical test for the hypothesis that a subset of data follows a Gaussian distribution. G-means runs k-means with increasing k in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each k-means center are Gaussian. Two key advantages are that the hypothesis test does not limit the covariance of the data and does not compute a full covariance matrix. Additionally, G-means only requires one intuitive parameter, the standard statistical significance level α. We present results from experiments showing that the algorithm works well, and better than a recent method based on the BIC penalty for model complexity. In these experiments, we show that the BIC is ineffective as a scoring function, since it does not penalize strongly enough the model's complexity.","venue":"NIPS","listofauthors":"Greg Hamerly, C. Elkan","citations":803,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":252,"title":"A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in Computer Science and Engineering","doi":null,"description":"null","venue":"","listofauthors":"Greg Hamerly, Serge J. Belongie, G. Cottrell, S. Dasgupta, V. D. Sa, K. Kreutz-Delgado","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":253,"title":"Using SimPoint for accurate and efficient simulation","doi":"10.1145/885651.781076","description":"null","venue":"SIGMETRICS","listofauthors":"Erez Perelman, Greg Hamerly, M. V. Biesbrouck, T. Sherwood, B. Calder","citations":6,"year":2003,"publisher":"Association for Computing Machinery (ACM)","pages":"318-319","volume":"31","number":"1","bibtex":"@article{2003,\n\tdoi = {10.1145/885651.781076},\n\turl = {https://doi.org/10.1145%2F885651.781076},\n\tyear = 2003,\n\tmonth = {jun},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {31},\n\tnumber = {1},\n\tpages = {318--319},\n\tauthor = {Erez Perelman and Greg Hamerly and Michael Van Biesbrouck and Timothy Sherwood and Brad Calder},\n\ttitle = {Using {SimPoint} for accurate and efficient simulation}\n}","authorsSemantic":[5]},{"id":254,"title":"Learning structure and concepts in data through data clustering","doi":null,"description":"Data clustering is an important and applications-oriented branch of machine learning. Its goal is to estimate the structure or density of a set of data without a training signal. There are many approaches to data clustering that vary in their complexity and effectiveness, due to the wide number of applications that these algorithms have. Due to the explosive growth of the amount of data that humans want to analyze, fast (e.g. linear-time) algorithms are necessary, but they can often give poor quality results. \nWhile maintaining the runtime characteristics of the fast algorithms, we show modifications that improve clustering algorithms in two ways. The first focus is on finding better solutions for a fixed number of clusters. We decompose the algorithms into fundamental parts, and analyze how the parts affect the quality of clustering solutions. The second focus is on estimating the number of clusters efficiently using statistical hypothesis tests, and how that may be applied in novel ways. \nWe also discuss the application of data clustering to the task of learning the structure of computer programs. We show how clustering may be used to improve the accuracy of computer processor simulations while simultaneously improving their efficiency.","venue":"","listofauthors":"Greg Hamerly, C. Elkan","citations":28,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":255,"title":"Using SimPoint for accurate and efficient simulation","doi":"10.1145/781027.781076","description":"Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of a single industry standard benchmark at this level of detail takes on the order of months to complete. This problem is exacerbated by the fact that to properly perform an architectural evaluation requires multiple benchmarks to be evaluated across many separate runs. To address this issue we recently created a tool called SimPoint that automatically finds a small set of Simulation Points to represent the complete execution of a program for efficient and accurate simulation. In this paper we describe how to use the SimPoint tool, and introduce an improved SimPoint algorithm designed to significantly reduce the simulation time required when the simulation environment relies upon fast-forwarding.","venue":"SIGMETRICS '03","listofauthors":"Erez Perelman, Greg Hamerly, Michael Van Biesbrouck, T. Sherwood, B. Calder","citations":301,"year":2003,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2003,\n\tdoi = {10.1145/781027.781076},\n\turl = {https://doi.org/10.1145%2F781027.781076},\n\tyear = 2003,\n\tpublisher = {{ACM} Press},\n\tauthor = {Erez Perelman and Greg Hamerly and Michael Van Biesbrouck and Timothy Sherwood and Brad Calder},\n\ttitle = {Using {SimPoint} for accurate and efficient simulation}\n}","authorsSemantic":[5]},{"id":256,"title":"Discovering and Exploiting Program Phases","doi":"10.1109/MM.2003.1261391","description":"Understanding program behavior is at the foundation of computer architecture and program optimization. Many programs have wildly different behavior on even the largest of scales (that is, over the program's complete execution). During one part of the execution, a program can be completely memory bound; in another, it can repeatedly stall on branch mispredicts. Average statistics gathered about a program might not accurately picture where the real problems lie. This realization has ramifications for many architecture and compiler techniques, from how to best schedule threads on a multithreaded machine, to feedback-directed optimizations, power management, and the simulation and test of architectures. Taking advantage of time-varying behavior requires a set of automated analytic tools and hardware techniques that can discover similarities and changes in program behavior on the largest of time scales. The challenge in building such tools is that during a program's lifetime it can execute billions or trillions of instructions. How can high-level behavior be extracted from this sea of instructions? Some programs change behavior drastically, switching between periods of high and low performance, yet system design and optimization typically focus on average system behavior. It is argued that instead of assuming average behavior, it is now time to model and optimize phase-based program behavior.","venue":"IEEE Micro","listofauthors":"T. Sherwood, Erez Perelman, Greg Hamerly, S. Sair, B. Calder","citations":287,"year":2003,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"84-93","volume":"23","number":"6","bibtex":"@article{2003,\n\tdoi = {10.1109/mm.2003.1261391},\n\turl = {https://doi.org/10.1109%2Fmm.2003.1261391},\n\tyear = 2003,\n\tmonth = {nov},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {23},\n\tnumber = {6},\n\tpages = {84--93},\n\tauthor = {T. Sherwood and E. Perelman and G. Hamerly and S. Sair and B. Calder},\n\ttitle = {Discovering and exploiting program phases}\n}","authorsSemantic":[5]},{"id":257,"title":"Picking statistically valid and early simulation points","doi":"10.1109/PACT.2003.1238020","description":"Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of an industry standard benchmark can take weeks to months to complete. To address this issue we have recently proposed using simulation points (found by only examining basic block execution frequency profiles) to increase the efficiency and accuracy of simulation. Simulation points are a small set of execution samples that when combined represent the complete execution of the program. We present a statistically driven algorithm for forming clusters from which simulation points are chosen, and examine algorithms for picking simulation points earlier in a program's execution-in order to significantly reduce fast-forwarding time during simulation. In addition, we show that simulation points can be used independent of the underlying architecture. The points are generated once for a program/input pair by only examining the code executed. We show the points accurately track hardware metrics (e.g., performance and cache miss rates) between different architecture configurations. They can therefore be used across different architecture configurations to allow a designer to make accurate trade-off decisions between different configurations.","venue":"2003 12th International Conference on Parallel Architectures and Compilation Techniques","listofauthors":"Erez Perelman, Greg Hamerly, B. Calder","citations":252,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/pact.2003.1238020},\n\turl = {https://doi.org/10.1109%2Fpact.2003.1238020},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {E. Perelman and G. Hamerly and B. Calder},\n\ttitle = {Picking statistically valid and early simulation points}\n}","authorsSemantic":[5]},{"id":258,"title":"Picking statistically valid and early simulation points","doi":"10.5555/942806.943854","description":"Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of an industry standard benchmark can take weeks to months to complete. To address this issue we have recently proposed using Simulation Points (found by only examining basic block execution frequency profiles) to increase the efficiency and accuracy of simulation. Simulation points are a small set of execution samples that when combined represent the complete execution of the program.In this paper we present a statistically driven algorithm for forming clusters from which simulation points are chosen, and examine algorithms for picking simulation points earlier in a program's execution - in order to significantly reduce fast-forwarding time during simulation. In addition, we show that simulation points can be used independent of the underlying architecture. The points are generated once for a program/input pair by only examining the code executed. We show the points accurately track hardware metrics (e.g., performance and cache miss rates) between different architecture configurations. They can therefore be used across different architecture configurations to allow a designer to make accurate trade-off decisions between different configurations.","venue":"PACT 2003","listofauthors":"Erez Perelman, Greg Hamerly, B. Calder","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n<title>Error: DOI Not Found</title>\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n\n<link rel=\"icon\" href=\"/static/img/favicon.png\" />\n<link rel=\"shortcut icon\" href=\"/static/favicon.ico\" type=\"image/x-icon\" /> \n<link href=\"/static/style/new-style2.css\" rel=\"stylesheet\" type=\"text/css\" />\n</head>\n\n<body>\n\n\n<div style=\"background:#fcb426\">\n<img src=\"/static/img/banner-413.gif\" alt=\"Logo\" width=\"620\" height=\"137\" border=\"0\" />\n</div>\n\n<div style=\"height:1px;background:#000000\"></div>\n<div style=\"height:1px;background:#54524f\"></div>\n<div style=\"height:1px;background:#f6911e\"></div>\n\n\n<!-- TABLE FOR NAVIGATION BAR -->\n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" id=\"navtable\" align=\"center\">\n<tr>\n    <td width=\"34\" height=\"26\" bgcolor=\"#231f20\"><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"1\" height=\"1\" /></td>\n    \n    <td height=\"26\" bgcolor=\"#231f20\" class=\"navtext\">\n    <a href=\"http://www.doi.org/index.html\">HOME</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/hb.html\">HANDBOOK</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/factsheets.html\">FACTSHEETS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/faq.html\">FAQs</a> &nbsp;|&nbsp; <a href=\"http://www.doi.org/resources.html\">RESOURCES</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/users.html\">USERS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/announce.html\">NEWS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/idf-members/index.html\">MEMBERS AREA</a>\n    </td>    \n  </tr>\n</table>\n<!-- END TABLE FOR NAVIGATION BAR -->\n\n<div style=\"height:1px;background:#e3a44d\"></div>\n<div style=\"height:3px;background:#4d4942\"></div>\n\n\n\n<!-- TABLE FOR CONTENT -->      \n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" bgcolor=\"#ffffff\">\n<tr>\n<td colspan=\"6\">\n<img src=\"/static/img/transparent.gif\" alt=\"\" width=\"100\" height=\"20\" border=\"0\" />\n</td>\n</tr>\n\n<tr>\n\n<td valign=\"top\">\n\n<h2>DOI Not Found</h2>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n<h3>10.5555/942806.943854</h3>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n\n<p>This DOI cannot be found in the DOI System.  Possible reasons are:</p>\n\n\n<ul>\n\n<li style=\"padding-bottom: .5em;\">The DOI is incorrect in your source. Search for the item by name, title, or other metadata using a search engine.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI was copied incorrectly. Check to see that the string includes all the characters before and after the slash and no sentence punctuation marks.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI has not been activated yet.  Please try again later, and report the problem if the error continues.</li>\n\n</ul>\n\n\n\n<div class=\"divider\">&nbsp;</div>\n\n<p>You may report this error to the responsible DOI Registration Agency using the form below.  Include your email address to receive confirmation and feedback.</p>\n\n<div style=\"padding-left: 4em;\">\n\n<form action=\"/notfound\" method=\"post\" enctype=\"application/x-www-form-urlencoded\" name=\"notFoundForm\">\n\n\n<table border=\"0\" cellspacing=\"3\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>\n\n<table border=\"0\" align=\"center\" cellpadding=\"3\" cellspacing=\"3\">\n<tbody><tr>\n<th  align=\"right\" scope=\"row\"><label>DOI:</label></th>\n<td><input name=\"missingHandle\" type=\"text\" value=\"10.5555/942806.943854\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\"><label>URL of Web Page Listing the DOI:</label></th>\n<td><input name=\"referringPage\" type=\"text\" value=\"\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\">Your Email Address:</th>\n<td><input name=\"userEmailAddress\" type=\"text\" value=\"Please enter your email address\" size=\"42\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\" valign=\"top\">Additional Information About the Error:</th>\n<td><textarea name=\"comments\" cols=\"30\" rows=\"6\"></textarea></td>\n</tr>\n</tbody>\n</table>\n\n</td>\n</tr>\n<tr>\n\n<td align=\"right\"><p><input name=\"send\" type=\"submit\" value=\"Submit Error Report\" /></p></td>\n</tr>\n</tbody>\n</table>\n\n</form>\n</div>\n\n\n\n\n</td>\n<td><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"20\" height=\"20\" border=\"0\" /></td>\n</tr>\n</table>\n\n<div class=\"divider-full\">&nbsp;</div>\n\n<!-- TABLE FOR FOOTER -->\n\n<table  border=\"0\" cellpadding=\"0\" cellspacing=\"0\" align=\"center\">\n\n<tr>\n<td align=\"center\" colspan=\"2\">\n<a href=\"/help.html\">DOI Resolution Documentation</a>\n</td>\n</tr>\n\n<tr>\n<td align=\"left\" height=\"40\">\n<img src=\"/static/img/Logo_TM.png\" alt=\"DOI_disc_logo\" width=\"24\" height=\"24\" />\n</td>\n\n<td align=\"left\">\n<span style=\"padding-left: 0px; font-size: 11px;\"><span style=\"vertical-align: super;\">&reg;</span>, DOI<span style=\"vertical-align: super;\">&reg;</span>, DOI.ORG<span style=\"vertical-align: super;\">&reg;</span>, and shortDOI<span style=\"vertical-align: super;\">&reg;</span> are trademarks of the International DOI Foundation.</span>\n</td>       \n</tr>\n</table>\n</body>\n</html>\n","authorsSemantic":[5]},{"id":259,"title":"Alternatives to the k-means algorithm that find better clusterings","doi":"10.1145/584792.584890","description":"We investigate here the behavior of the standard k-means clustering algorithm and several alternatives to it: the k-harmonic means algorithm due to Zhang and colleagues, fuzzy k-means, Gaussian expectation-maximization, and two new variants of k-harmonic means. Our aim is to find which aspects of these algorithms contribute to finding good clusterings, as opposed to converging to a low-quality local optimum. We describe each algorithm in a unified framework that introduces separate cluster membership and data weight functions. We then show that the algorithms do behave very differently from each other on simple low-dimensional synthetic datasets and image segmentation tasks, and that the k-harmonic means method is superior. Having a soft membership function is essential for finding high-quality clusterings, but having a non-constant data weight function is useful also.","venue":"CIKM '02","listofauthors":"Greg Hamerly, C. Elkan","citations":464,"year":2002,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2002,\n\tdoi = {10.1145/584792.584890},\n\turl = {https://doi.org/10.1145%2F584792.584890},\n\tyear = 2002,\n\tpublisher = {{ACM} Press},\n\tauthor = {Greg Hamerly and Charles Elkan},\n\ttitle = {Alternatives to the k-means algorithm that find better clusterings}\n}","authorsSemantic":[5]},{"id":260,"title":"Automatically characterizing large scale program behavior","doi":"10.1145/635508.605403","description":"null","venue":"ASPLOS","listofauthors":"T. Sherwood, Erez Perelman, Greg Hamerly, B. Calder","citations":5,"year":2002,"publisher":"Association for Computing Machinery (ACM)","pages":"45-57","volume":"36","number":"5","bibtex":"@article{2002,\n\tdoi = {10.1145/635508.605403},\n\turl = {https://doi.org/10.1145%2F635508.605403},\n\tyear = 2002,\n\tmonth = {dec},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {36},\n\tnumber = {5},\n\tpages = {45--57},\n\tauthor = {Timothy Sherwood and Erez Perelman and Greg Hamerly and Brad Calder},\n\ttitle = {Automatically characterizing large scale program behavior}\n}","authorsSemantic":[5]},{"id":261,"title":"Automatically characterizing large scale program behavior","doi":"10.1145/605397.605403","description":"Understanding program behavior is at the foundation of computer architecture and program optimization. Many programs have wildly different behavior on even the very largest of scales (over the complete execution of the program). This realization has ramifications for many architectural and compiler techniques, from thread scheduling, to feedback directed optimizations, to the way programs are simulated. However, in order to take advantage of time-varying behavior, we must first develop the analytical tools necessary to automatically and efficiently analyze program behavior over large sections of execution.Our goal is to develop automatic techniques that are capable of finding and exploiting the Large Scale Behavior of programs (behavior seen over billions of instructions). The first step towards this goal is the development of a hardware independent metric that can concisely summarize the behavior of an arbitrary section of execution in a program. To this end we examine the use of Basic Block Vectors. We quantify the effectiveness of Basic Block Vectors in capturing program behavior across several different architectural metrics, explore the large scale behavior of several programs, and develop a set of algorithms based on clustering capable of analyzing this behavior. We then demonstrate an application of this technology to automatically determine where to simulate for a program to help guide computer architecture research.","venue":"ASPLOS X","listofauthors":"T. Sherwood, Erez Perelman, Greg Hamerly, B. Calder","citations":1758,"year":2002,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2002,\n\tdoi = {10.1145/605397.605403},\n\turl = {https://doi.org/10.1145%2F605397.605403},\n\tyear = 2002,\n\tpublisher = {{ACM} Press},\n\tauthor = {Timothy Sherwood and Erez Perelman and Greg Hamerly and Brad Calder},\n\ttitle = {Automatically characterizing large scale program behavior}\n}","authorsSemantic":[5]},{"id":262,"title":"Thesis proposal: High-quality automatic data clustering","doi":null,"description":"Data clustering, the task of grouping related objects in a set of data, is a powerful technique in machine learning. Data clustering is used in many machine learning tasks to learn about data structure, to summarize data, and to find a model of data density, among other uses. The two essential questions in data clustering are “how many groups are there?”, and “what are the groups?” There are many algorithms for the latter problem, but not as many solutions for the former problem. In this proposal I outline goals for improving the answers to both of these questions using a new model selection technique for clustering based on the distortion of data, and analyzing the new k-harmonic means algorithm which has the ability to find higher-quality groupings than standard algorithms such as k-means and EM with a mixture of Gaussians. I also propose interesting datasets and problems in computer architecture simulation to which we will apply the resulting work.","venue":"","listofauthors":"Greg Hamerly","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":263,"title":"Bayesian approaches to failure prediction for disk drives","doi":null,"description":"Hard disk drive failures are rare but are often costly. The ability to predict failures is important to consumers, drive manufacturers, and computer system manufacturers alike. In this paper we investigate the abilities of two Bayesian methods to predict disk drive failures based on measurements of drive internal conditions. We first view the problem from an anomaly detection stance. We introduce a mixture model of naive Bayes submodels (i.e. clusters) that is trained using expectation-maximization. The second method is a naive Bayes classifier, a supervised learning approach. Both methods are tested on realworld data concerning 1936 drives. The predictive accuracy of both algorithms is far higher than the accuracy of thresholding methods used in the disk drive industry today.","venue":"ICML","listofauthors":"Greg Hamerly, C. Elkan","citations":198,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":264,"title":"Comparison of ad hoc network routing protocols","doi":null,"description":"null","venue":"","listofauthors":"J. Bellardo, J. Fang, Greg Hamerly","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":265,"title":"Learning the � in �-means","doi":null,"description":"When clustering a dataset, the right number of clusters to use is often not obvious, and choosing automatically is a hard algorithmic problem. In this paper we present an improved algorithm for learning while clustering. The G-means algorithm is based on a statistical test for the hypothesis that a subset of data follows a Gaussian distribution. G-means runs -means with increasing in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each -means center are Gaussian. Two key advantages are that the hypothesis test does not limit the covariance of the data and does not compute a full covariance matrix. Additionally, G-means only requires one intuitive parameter, the standard statistical significance level . We present results from experiments showing that the algorithm works well, and better than a recent method based on the BIC penalty for model complexity. In these experiments, we show that the BIC is ineffective as a scoring function, since it does not penalize strongly enough the model’s complexity.","venue":"","listofauthors":"Greg Hamerly, C. Elkan","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[5]},{"id":287,"title":"Anti-Symmetry and Logic Simulation","doi":null,"description":"Like ordinary symmetries, anti-symmetries are defined by relations between function cofactors. For ordinary symmetries, two cofactors must be equal, for anti-symmetries two cofactors must be complements of one another. This paper shows that anti-symmetries can be used to improve simulation performance in the same manner as ordinary symmetries. Detailed detection, clustering and simulation algorithms are given along with a set of experimental results to demonstrate the effectiveness of the algorithms. These results show that antisymmetries can be just as effective as ordinary symmetries in enhancing simulation performance. In fact, in some cases, antisymmetries give better performance than ordinary symmetries.","venue":"","listofauthors":"P. Maurer","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":266,"title":"Massive Generation of Data with Random Variates","doi":"10.23919/ANNSIM52504.2021.9552070","description":"The Data Generation Language (DGL) has been widely used to generate random data for simulation and for software testing. Although DGL is highly versatile, its ability to handle different probability distributions was severely limited. The work described here corrects this problem by adding features that can be used to generate variates from a number of different probability distributions. This data can be used directly by a simulator or stored in a file or database table for future use. Variate generation makes use of a basic stream of uniformly distributed random numbers that can be generated by one of 47 different random number generators.","venue":"2021 Annual Modeling and Simulation Conference (ANNSIM)","listofauthors":"P. Maurer","citations":0,"year":2021,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2021,\n\tdoi = {10.23919/annsim52504.2021.9552070},\n\turl = {https://doi.org/10.23919%2Fannsim52504.2021.9552070},\n\tyear = 2021,\n\tmonth = {jul},\n\tpublisher = {{IEEE}},\n\tauthor = {Peter M. Maurer},\n\ttitle = {Massive Generation of Data with Random Variates}\n}","authorsSemantic":[6]},{"id":267,"title":"A nominal/inertial delay metamorphic differential simulator","doi":null,"description":"Metamorphic differential simulation is an effective method of simulating digital circuits that has proven to give significant performance improvements over conventional simulation. Initially it was restricted to the zero-delay timing model, but recent work has extended it to the unit-delay, multi-delay, and nominal-delay models. Although the nominal-delay simulator gave significant improvements over conventional simulation, it was confined to the transport model of gate delays. This model is less realistic than the inertial model of delays, and can lower performance due to the enormous number of events produced. It was not obvious that extension to the inertial model would be possible because it requires a type of event cancellation that is quite different from that used in the previous simulators. We show that such an extension is not only possible, but that it produces an effective simulator that gives significant performance improvement over conventional simulation.","venue":"SummerSim","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":268,"title":"Finite random variates using differential search trees","doi":"10.22360/summersim.2017.scsc.028","description":"Differential search trees have been used for selection with replacement, and for a form of selection without replacement. We show that they can be extended to many different types of selection, both with and without replacement. In addition, virtually every aspect of a differential search tree can be modified dynamically. We provide algorithms for making these modifications. Virtually all differential search tree algorithms are straightforward and easy to implement, especially with our preferred implementation, which is both simple and efficient. Differential search tree operations are virtually all logarithmic with the exception of building the tree and dynamically adding leaves to the tree, which are both linear.","venue":"SummerSim","listofauthors":"P. Maurer","citations":1,"year":2017,"publisher":"Society for Modeling and Simulation International (SCS)","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2017,\n\tdoi = {10.22360/summersim.2017.scsc.028},\n\turl = {https://doi.org/10.22360%2Fsummersim.2017.scsc.028},\n\tyear = 2017,\n\tpublisher = {Society for Modeling and Simulation International ({SCS})},\n\ttitle = {Finite Random Variates Using Differential Search Trees}\n}","authorsSemantic":[6]},{"id":269,"title":"\" A 32 b Floating Point CMOS Digital Signal Proces","doi":null,"description":"Earle et al., \"Exponent Differences and Preshifter', IBM Technical Disclosure Bulletin, vol. 9, No. 7, Dec. '66, pp. 848-849. Sproul, \"High-Speed Floating-Point Accumulator', IBM Tech. Disclosure Bulletin, vol. 14, No. 10, Mar. 72, pp. 2934-2936. \"An IEEE Standard Floating Point Chip\", A. Komal, K. Goskel, Phil W. Diodato, John A. Fields, Ulhas V. Gumaste, Chaw K. Kung, Kingyao Lin, Mario E. Lega, Peter M. Maurer, Thomas K. Ng, Yaw T. Oh, Mark E. Thierbach-AT&T Bell Labs., 1985, IEEE Intml. Solid-State Circuits Conf.-Feb. 13, 1985, pp. 18-19. \"A Single Chip 80b Floating Point Processor '-Karumitsu Takeda, Fumiaki Ishino, Yoshitaka Ito, Ryota Kasai, Takayoshi Nakashima, NTT Atsugi Elec","venue":"","listofauthors":"Schenectady Chung-Yih Ho, J. Karl, Molnár, P. Maurer, T. Ng, T. Nakashima, Tomoji, Nukiyama, Makoto Yoshida, T. Nishitani, Firm-Allen L. Limberg, James C. Davis, Marvin Snyder","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":270,"title":"Time-parallel multi-delay logic simulation","doi":"10.22360/springsim.2016.tmsdevs.037","description":"Multi-delay logic simulation may perform many logic simulations for each gate in the circuit during the simulation of a single input vector. Conventional event driven simulation will perform each of these gate-simulations as a separate operation, producing a single bit of information for each operation. The PC-Set method eliminates the scheduling code of event-driven simulation at the expense of performing unnecessarily gate simulations. This permits much faster simulations except when the activity rate is extremely low. Time-parallel simulation can perform the required simulations for a single gate simultaneously. Up to 64 gate simulations can be performed using a single set of low-level operations. Previous work on time-parallel simulation was done many years ago, and due to resource limitations, it was limited to unit-delay simulations. Modern computers are not so limited, and are able to perform time-parallel multi-delay simulation with no problems. Time-parallel simulation significantly out-performs both conventional event-driven simulation and the PC-set method. Furthermore, recent work in oblivious simulation (time-parallel simulation is an oblivious technique) suggests that oblivious simulation can perform well even with extremely low activity rates, thus permitting time-parallel simulation to outperform conventional event-driven simulation under most conditions.","venue":"2016 Symposium on Theory of Modeling and Simulation (TMS-DEVS)","listofauthors":"P. Maurer","citations":2,"year":2016,"publisher":"Society for Modeling and Simulation International (SCS)","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.22360/springsim.2016.tmsdevs.037},\n\turl = {https://doi.org/10.22360%2Fspringsim.2016.tmsdevs.037},\n\tyear = 2016,\n\tpublisher = {Society for Modeling and Simulation International ({SCS})},\n\ttitle = {Time-Parallel Multi-Delay Logic Simulation}\n}","authorsSemantic":[6]},{"id":271,"title":"Super Symmetric Boolean Functions","doi":null,"description":"Super symmetry is a type of matrix-based symmetry that extends the concept of total symmetry. Super symmetric functions are “even more symmetric” than totally symmetric functions. Even if a function is not super symmetric, the super symmetric transpose matrices can be used to detect partial super symmetries. These partial symmetries can be mixed arbitrarily with ordinary symmetric variable pairs to create large sets of mutually symmetric variables. In addition, one can detect subsets of super symmetric inputs, which are distinct from partial super symmetries. Super symmetry allows many new types of Boolean function symmetry to be detected and exploited.","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":288,"title":"The General Linear Group of GF(2)^3","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":289,"title":"The Subgroups of S5 in Cycle Form","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":290,"title":"The Subgroups of S6 in Cycle Form","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":291,"title":"The Subgroups of S3 in Cycle Form","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":292,"title":"The Subgroups of S4 in Cycle Form","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":272,"title":"A second look at oblivious simulation","doi":null,"description":"The two predominant logic simulation techniques are event-driven simulation, and levelized compiled code simulation. In the past, studies have been performed to characterize these algorithms, and compare their performance under various different conditions. However, these studies were done a number of years ago on equipment that varies substantially from the equipment that is available to us today. Today's equipment is less dependent on small tight loops than the equipment of several years ago. Furthermore, the studies that were done years ago we re generally done assuming a uniform activity rate throughout all parts of a complex circuit. In fact, activity rate varies substantially within a circuit, with high bursts of activity at various times. This paper examines a combined approach that can increase performance despite very low activity rates. A large number of studies were performed that show that the earlier assumptions about event-driven vs. oblivious simulation should be re-examined.","venue":"SummerSim","listofauthors":"P. Maurer","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":273,"title":"The number of conjugate symmetries for Boolean functions with an arbitrary number of inputs","doi":"10.1201/B18592-190","description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":274,"title":"Levelized compiled code multi-delay logic simulation","doi":null,"description":"Levelized Compiled Code (LCC) multi-delay simulation is an idea whose time has come. Although it is not a new idea, when it was first proposed the hardware and software technology of the time was not capable of meeting the demands of such a simulation technique[1]. The basic technique is to predict the points in time when each gate can change value, and generate simulation code to compute the output of the gate at those times. For multi-delay simulation in which each gate has an integer delay greater than or equal to one, many circuits require several megabytes of straight-line code. Running such code was technologically infeasible when the technique was first proposed. However, technology has caught up with the algorithm, and it is now possible to determine whether oblivious multi-delay simulation is a viable method of logic simulation. Our experimental data shows that oblivious multi-delay simulation is many times faster than event-driven simulation for typical circuits, and significantly faster even with extremely demanding simulation parameters.","venue":"SpringSim","listofauthors":"P. Maurer","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":275,"title":"A universal symmetry detection algorithm","doi":"10.1186/s40064-015-1156-7","description":"Research on symmetry detection focuses on identifying and detecting new types of symmetry. The paper presents an algorithm that is capable of detecting any type of permutation-based symmetry, including many types for which there are no existing algorithms. General symmetry detection is library-based, but symmetries that can be parameterized, (i.e. total, partial, rotational, and dihedral symmetry), can be detected without using libraries. In many cases it is faster than existing techniques. Furthermore, it is simpler than most existing techniques, and can easily be incorporated into existing software. The algorithm can also be used with virtually any type of matrix-based symmetry, including conjugate symmetry.","venue":"2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","listofauthors":"P. Maurer","citations":3,"year":2015,"publisher":"Springer Science and Business Media LLC","pages":null,"volume":"4","number":"1","bibtex":"@article{2015,\n\tdoi = {10.1186/s40064-015-1156-7},\n\turl = {https://doi.org/10.1186%2Fs40064-015-1156-7},\n\tyear = 2015,\n\tmonth = {aug},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {4},\n\tnumber = {1},\n\tauthor = {Peter M Maurer},\n\ttitle = {A universal symmetry detection algorithm}\n}","authorsSemantic":[6]},{"id":276,"title":"Matrix Representations of GF(p[superscript n]) over GF(p)","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":277,"title":"A universal symmetry detection algorithm","doi":"10.7873/DATE.2014.312","description":"Research on symmetry detection focuses on identifying and detecting new types of symmetry. We present an algorithm that is capable of detecting any type of permutation-based symmetry, including many types for which there are no existing algorithms. General symmetry detection is library-based, but symmetries that can be parameterized, (i.e. total, partial, rotational, and dihedral symmetry), can be detected without using libraries. In many cases it is faster than existing techniques. Furthermore, it is simpler than most existing techniques, and can easily be incorporated into existing software.","venue":"DATE 2014","listofauthors":"P. Maurer","citations":3,"year":2014,"publisher":"IEEE Conference Publications","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2014,\n\tdoi = {10.7873/date.2014.312},\n\turl = {https://doi.org/10.7873%2Fdate.2014.312},\n\tyear = 2014,\n\tpublisher = {{IEEE} Conference Publications},\n\tauthor = {Peter M. Maurer},\n\ttitle = {A universal symmetry detection algorithm}\n}","authorsSemantic":[6]},{"id":278,"title":"Generator Pairs for all 4x4 GF(2) Representations of S4","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":279,"title":"The Class 10 4x4 Faithful Representations of S4 over GF(2)","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":280,"title":"The Representations of GF(8) in GL3(2)","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":281,"title":"The Number of Conjugates of the Standard Representation of Sn in the General Linear Group over GF(2)","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":282,"title":"Primitive Polynomials for the Field GF(2): Degree 2 through Degree 16","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":283,"title":"Metamorphic differential simulation using the multi-delay timing model","doi":null,"description":"Here we show that the Event Driven Condition Free (EVCF) simulation technique for gate-level circuits can be extended to the multi-delay timing model. In the multi-delay timing model, gates have different integer delays. Events are produced out of order and must be sorted for processing. The EVCF technique has shown spectacular gains in performance for the zero-delay and unit-delay models. The gains here are somewhat less spectacular, but are substantial, showing that the EVCF technique is an effective method for improving the performance of multi delay simulation.","venue":"SpringSim","listofauthors":"P. Maurer","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":284,"title":"The Conjugacy Classes of 3x3 and 4x4 Matrices Over GF(2)","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":285,"title":"The Super Symmetric Representations of S4 in GL3(2)","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":286,"title":"Extending symmetric variable-pair transitivities using state-space transformations","doi":"10.1145/2206781.2206859","description":"Detecting the symmetries of a Boolean function can lead to simpler implementations both at the hardware and software level. Large clusters of mutually symmetric variables are more advantageous than small clusters. One way to extend the symmetry of a function is to detect abstract two-cofactor relations in addition to ordinary symmetric relations. Unfortunately, ordinary symmetries are simply transitive but more complex types of relations are not. This paper shows how to convert the more complex relations into ordinary symmetries, allowing them to be used to form large clusters of symmetric variables, larger than would be possible using ordinary symmetries.","venue":"GLSVLSI '12","listofauthors":"P. Maurer","citations":1,"year":2012,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2012,\n\tdoi = {10.1145/2206781.2206859},\n\turl = {https://doi.org/10.1145%2F2206781.2206859},\n\tyear = 2012,\n\tpublisher = {{ACM} Press},\n\tauthor = {Peter M. Maurer},\n\ttitle = {Extending symmetric variable-pair transitivities using state-space transformations}\n}","authorsSemantic":[6]},{"id":293,"title":"Conjugate symmetry","doi":"10.1007/s10703-011-0116-2","description":"Conjugate symmetry is an entirely new approach to symmetric Boolean functions that can be used to extend existing methods for handling symmetric functions to a much wider class of functions. These are functions that currently appear to have no symmetries of any kind. Conjugate symmetries occur widely in practice. In fact, we show that even the simplest circuits exhibit conjugate symmetry. To demonstrate the effectiveness of conjugate symmetry we modify an existing simulation algorithm, the hyperlinear algorithm, to take advantage of conjugate symmetry. This algorithm can simulate symmetric functions faster than non-symmetric ones, but due to the rarity of symmetric functions, this optimization is of limited benefit. Because the standard benchmark circuits contain many symmetries it is possible to simulate these circuits faster than is possible with the fastest known event-driven algorithm. The detection and exploitation of conjugate symmetries makes use of GF(2) matrices. It is likely that conjugate symmetry and GF(2) matrices will find applications in many other areas of EDA.","venue":"Formal Methods Syst. Des.","listofauthors":"P. Maurer","citations":6,"year":2011,"publisher":"Springer Science and Business Media LLC","pages":"263-288","volume":"38","number":"3","bibtex":"@article{2011,\n\tdoi = {10.1007/s10703-011-0116-2},\n\turl = {https://doi.org/10.1007%2Fs10703-011-0116-2},\n\tyear = 2011,\n\tmonth = {mar},\n\tpublisher = {Springer Science and Business Media {LLC}},\n\tvolume = {38},\n\tnumber = {3},\n\tpages = {263--288},\n\tauthor = {Peter M. Maurer},\n\ttitle = {Conjugate symmetry}\n}","authorsSemantic":[6]},{"id":294,"title":"AN APPLICATION OF GROUP THEORY TO THE ANALYSIS OF SYMMETRIC GATES","doi":null,"description":"A method for determining the symmetries of the inputs of a logic gate either from its truth table or from facts obtained by inspection of its circuit is presented. The symmetry rule of a gate with n inputs is defined in terms of a subgroup of the symmetric group of degree n. This technique leads to an expanded and more complete definition of partial symmetry than has previously appeared. This definition of symmetry is used to show that the set of boolean expressions that represent non-totally-symmetric functions is NPcomplete. The group-theoretic concept of conjugate sets is used to identify symmetry rules that are fundamentally the same but applied to different sets of inputs. A complete analysis of all forms of symmetry for 2-input, 3-input and 4-input gates is provided. An example is given to show how this theory was applied to a problem in VLSI design verification. Editor’s addendum to the abstract: This paper was originally written in 1985 while I was a member of technical staff at the now defunct Bell Laboratories. Although I had high hopes for a publication, I spent many years attempting to publish this work without success. The theorists considered it too trival, while the practitioners considered it too theoretical. I disagree with both, but I wasn’t able to convince anyone. While it is less sophisticated than any paper I would write today, I believe that the material is worth knowing and not available in any other form. Thus I am releasing this paper as a Baylor Technical Report. This copy of the paper was OCRed from a paper copy and extensively edited to remove typos and OCR errors. Any remaining errors are mine and mine alone.","venue":"","listofauthors":"P. Maurer","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":295,"title":"The ISCAS89 Benchmarks in FHDL Format","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":296,"title":"The Inversion-Algorithm Software","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":297,"title":"Categories for Component Level Design","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":298,"title":"The Parallel Technique Software Package","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":299,"title":"Three-Valued Simulation with the Inversion Algorithm","doi":null,"description":"The Inversion Algorithm is an event-driven logic simulation technique that is competitive with Levelized Compiled Code Simulation. Previous versions of the Inversion Algorithm have been limited to purely binary simulation. The algorithm presented here extends the Inversion Algorithm to three-valued simulation while preserving the desirable properties of the two-valued algorithm. Because of the richer transformation structure used in three-valued simulation, the scheduling technique is significantly more complex than that of the two-valued algorithm. The procedure for collapsing simultaneous events is also significantly more complex. Once a three-valued net achieves a stable binary value, it is possible to replace the three-valued simulation with a more efficient two-valued simulation. Experimental data shows that the threevalued algorithm is also competitive with levelized compiled code simulation. Nature abhors a vacuum. THREE-VALUED SIMULATION WITH THE INVERSION ALGORITHM Peter M. Maurer William J. Schilp ENG 118 Department of Computer Science & Engineering University of South Florida Tampa, FL 33620","venue":"","listofauthors":"P. Maurer","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":300,"title":"The GF2Matrices Classes: A Programming Package for Mathematical Research","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":301,"title":"Using the Connlib Package to Obtain Parsed Netlist Data","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":302,"title":"The Shadow Algorithm Software Package","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":303,"title":"Unit Delay Scheduling for the Inversion Algorithm","doi":null,"description":"The Inversion Algorithm is an event driven algorithm whose performance meets or exceeds that of Levelized Compiled Code simulation, even when the activity rate is unrealistically high. Existing implementations of the Inversion Algorithm are based on the Zero Delay model. This paper presents an implementation which is based on the Unit-Delay model. Although the most basic form of the Inversion Algorithm can be converted to Unit Delay with little difficulty, special considerations must be taken to avoid scheduling conflicts. The main problems discussed in this paper are avoiding scheduling conflicts, and minimizing the amount of storage space required to do so. These problems are made considerably more difficult by the deletion of NOT gates and the collapsing of various connections. These optimizations transform the simulation into a multi-delay simulation under the transport delay model. A complete solution to the scheduling problem is presented under these conditions.","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":304,"title":"The Complexity of Detecting Symmetric Functions","doi":null,"description":"The characterization of the symmetries of boolean functions is important both in automatic layout synthesis, and in automatic verification of manually created layouts. It is possible to characterize the symmetries of an n-input boolean function as an arbitrary subgroup, G, of Sn, the symmetric group of order n. Given an expression e, which represents an n-input boolean function F, and a subgroup G of Sn, the problem of whether F possesses symmetry G is an NP-complete problem. The concept of an orbit can be used to characterize the various types of symmetry for a specified number of inputs. This classification can then be used, along with a few partitioning rules to completely determine the symmetries of a boolean function. This technique requires that the truth-table of a function be completely enumerated, and thus has a running time proportional to 2n, where n is the number of inputs of the function. Some of the mathematical concepts presented to support the NP-completeness result have intriguing possibilities for circuit minimization. THE COMPLEXITY OF DETECTING SYMMETRIC FUNCTIONS Peter M. Maurer Dept. of Computer Science Baylor University Waco, TX","venue":"","listofauthors":"P. Maurer","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":305,"title":"The EVCF Software Package","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":306,"title":"The FHDL LCC Simulator","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":315,"title":"Converting command-line applications into binary components: Research Articles","doi":"10.1002/SPE.V35:8","description":"A binary component is a separately compiled program that can be used as a part of a larger program. Binary components generally conform to an accepted technology such as JavaBeans or ActiveX, and generally support a rich program interface containing properties, methods and events. Binary components are generally used in a graphical user interface (GUI) environment. There are a number of benefits to be realized by converting command-line software into binary components. The most important of these is that GUI environments are more popular and more familiar to most people than command-line environments. Using binary components can greatly simplify a GUI implementation, to the point where it is only slightly more complicated than a typical command-line implementation. However there are benefits that go beyond mere convenience. Binary components have much richer interfaces than command-line programs. Binary components are service-oriented rather than task-oriented. A task-oriented program has a main routine that is devoted to accomplishing a single task. A service-oriented component has no main routine or main function, but instead provides a variety of services to its clients. Binary components can be easily integrated with one another, which permits a design where each major feature of an application is implemented in a different component. Such a design encourages software reuse at the component level and facilitates low-impact feature upgrades. We first delineate a design-pattern-based methodology for converting command-line programs into components. We then illustrate these principles using two projects, a simulation system for digital circuits, and a data generation system for software and hardware testing. Copyright © 2005 John Wiley & Sons, Ltd.","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":"Resource not found.","authorsSemantic":[6]},{"id":316,"title":"How to Make Program Assessment Work for You","doi":null,"description":"null","venue":"FECS","listofauthors":"William B. Poucher, P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":317,"title":"Converting command‐line applications into binary components","doi":"10.1002/spe.659","description":"A binary component is a separately compiled program that can be used as a part of a larger program. Binary components generally conform to an accepted technology such as JavaBeans or ActiveX, and generally support a rich program interface containing properties, methods and events. Binary components are generally used in a graphical user interface (GUI) environment. There are a number of benefits to be realized by converting command‐line software into binary components. The most important of these is that GUI environments are more popular and more familiar to most people than command‐line environments. Using binary components can greatly simplify a GUI implementation, to the point where it is only slightly more complicated than a typical command‐line implementation. However there are benefits that go beyond mere convenience. Binary components have much richer interfaces than command‐line programs. Binary components are service‐oriented rather than task‐oriented. A task‐oriented program has a main routine that is devoted to accomplishing a single task. A service‐oriented component has no main routine or main function, but instead provides a variety of services to its clients. Binary components can be easily integrated with one another, which permits a design where each major feature of an application is implemented in a different component. Such a design encourages software reuse at the component level and facilitates low‐impact feature upgrades. We first delineate a design‐pattern‐based methodology for converting command‐line programs into components. We then illustrate these principles using two projects, a simulation system for digital circuits, and a data generation system for software and hardware testing. Copyright © 2005 John Wiley & Sons, Ltd.","venue":"Softw. Pract. Exp.","listofauthors":"P. Maurer","citations":1,"year":2005,"publisher":"Wiley","pages":"787-797","volume":"35","number":"8","bibtex":"@article{2005,\n\tdoi = {10.1002/spe.659},\n\turl = {https://doi.org/10.1002%2Fspe.659},\n\tyear = 2005,\n\tpublisher = {Wiley},\n\tvolume = {35},\n\tnumber = {8},\n\tpages = {787--797},\n\tauthor = {Peter M. Maurer},\n\ttitle = {Converting command-line applications into binary components}\n}","authorsSemantic":[6]},{"id":318,"title":"Metamorphic programming: unconventional high performance","doi":"10.1109/MC.2004.1274000","description":"A programming methodology that violates most of the rules of good programming has shown spectacular reductions in simulation times on several benchmarks. Applying this technique in logic-level VLSI circuit simulation also improved simulation performance. For a new VLSI circuit, faster simulation translates into faster time to market, so even the most peculiar programming type is worth exploring if the carrot is increased performance. Discovering efficient and effective metamorphic programming techniques across a range of problems outside simulation will require a concerted effort across the software community. The most important problem is the lack of metamorphic constructs in mainstream high-level languages.","venue":"Computer","listofauthors":"P. Maurer","citations":4,"year":2004,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"30-38","volume":"37","number":"3","bibtex":"@article{2004,\n\tdoi = {10.1109/mc.2004.1274000},\n\turl = {https://doi.org/10.1109%2Fmc.2004.1274000},\n\tyear = 2004,\n\tmonth = {mar},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {37},\n\tnumber = {3},\n\tpages = {30--38},\n\tauthor = {P.M. Maurer},\n\ttitle = {Metamorphic programming: unconventional high performance}\n}","authorsSemantic":[6]},{"id":319,"title":"Efficient event-driven simulation by exploiting the output observability of gate clusters","doi":"10.1109/TCAD.2003.818305","description":"State machine-based simulation is an efficient event-driven simulation technique which has been used to simulate gate-level circuits. Although this approach has been proven to be successful, the techniques that have been used up to now have been limited in several ways. This paper introduces generic state machines that can be used in a wide variety of contexts. These machines are then used to create efficient new simulation techniques that can be applied not just to gates, but to a wide variety of specifications, including Boolean expressions, binary decision diagrams and truth tables. These techniques can combine clusters of gates into a single simulation unit that can be simulated as efficiently as a single gate. The resultant state machines can be simplified by identifying and combining symmetric inputs. Two different types of symmetries can be detected, ordinary symmetry, such as that exhibited by an AND gate, and inverted symmetry, where an input is symmetric with the complement of another input. Both sorts of symmetry can be used for state machine simplification. Experimental results show a substantial reduction in simulation time for complex circuits.","venue":"IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.","listofauthors":"P. Maurer","citations":7,"year":2003,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"1471-1486","volume":"22","number":"11","bibtex":"@article{2003,\n\tdoi = {10.1109/tcad.2003.818305},\n\turl = {https://doi.org/10.1109%2Ftcad.2003.818305},\n\tyear = 2003,\n\tmonth = {nov},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {22},\n\tnumber = {11},\n\tpages = {1471--1486},\n\tauthor = {P.M. Maurer},\n\ttitle = {Efficient event-driven simulation by exploiting the output observability of gate clusters}\n}","authorsSemantic":[6]},{"id":320,"title":"Component Level Programming","doi":null,"description":"1. Introduction. 2. Visual Basic Programming. 3. A Brief Survey of Component Technologies. 4. Component-Based Application Design. 5. Categorizing Components. 6. Models. 7. Editors. 8. Background Editors. 9. Serializers. 10. Displays. 11. Accessors. 12. Caches. 13. Filters. 14. UI Widgets. 15. Decorations. 16. Function Libraries. 17. Service Wrappers. 18. Containers. 19. Semi-Persistent Objects. 20. The Future. Appendix A: Object-Oriented Design. Appendix B: Programming the Windows GUI. Appendix C: MFC and ATL. Appendix D: Using ActiveX Controls on the Web.","venue":"","listofauthors":"P. Maurer","citations":9,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":321,"title":"Random characterization of design automation algorithms","doi":"10.1109/ISVLSI.2003.1183493","description":"Randomly generated Directed Acyclic Graphs (DAGs) can be used to generate various kinds of EDA test data. For example, they can be used to characterize channel routing algorithms. This paper uses such data to characterize the relative performance of a number of different channel routing algorithms, with the aim of determining those factors that have the most effect on routing performance. Our studies show very little difference in the algorithms studied Factors that have been considered to provide performance improvements are shown to be unimportant, and in some cases even detrimental to average routing performance. This study suggests that \"well known\" algorithms are not really well known at all, and that more extensive data is needed to characterize the algorithms that we use everyday.","venue":"IEEE Computer Society Annual Symposium on VLSI, 2003. Proceedings.","listofauthors":"Sandeep K. Kondapuram, P. Maurer","citations":0,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/isvlsi.2003.1183493},\n\turl = {https://doi.org/10.1109%2Fisvlsi.2003.1183493},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {S.K. Kondapuram and P.M. Maurer},\n\ttitle = {Random characterization of design automation algorithms}\n}","authorsSemantic":[6]},{"id":322,"title":"Identifying an appropriate view of software components for undergraduate education","doi":"10.1145/366413.364756","description":"null","venue":"SIGCSE","listofauthors":"Allen S. Parrish, J. Hollingsworth, P. Maurer, B. Shults, B. Weide","citations":0,"year":2001,"publisher":"Association for Computing Machinery (ACM)","pages":"394-395","volume":"33","number":"1","bibtex":"@article{2001,\n\tdoi = {10.1145/366413.364756},\n\turl = {https://doi.org/10.1145%2F366413.364756},\n\tyear = 2001,\n\tmonth = {mar},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {33},\n\tnumber = {1},\n\tpages = {394--395},\n\tauthor = {Allen Parrish and Joe Hollingsworth and Peter Maurer and Benjamin Shults and Bruce Weide},\n\ttitle = {Identifying an appropriate view of software components for undergraduate education}\n}","authorsSemantic":[6]},{"id":323,"title":"Identifying an appropriate view of software components for undergraduate education","doi":"10.1145/364447.364756","description":"Software components have existed in one form or another for a number of years. Work in this area can be classified into two broad categories. On the one hand, a number of researchers have approached the concept of software components from a first principles perspective, advancing ideas regarding what constitutes the ideal component paradigm from perspectives of efficiency, verifiability and reusability. On the other hand, recent commercial advances in a number of popular technologies have elevated the software component concept into widespread use within the software practitioner community. Such technologies include a number of technologies made popular by Microsoft (such as Active-X, COM, DCOM and Visual Basic), as well as CORBA and Java Beans.Neither of these perspectives on software components has become a standard cornerstone of software development pedagogy. Yet both perspectives may have an important role in preparing software developers to build high-quality software in the context of modern software development technologies. In particular, teaching students how to design and construct software components from first principles provides students with important guidance as to the \"right way\" to structure correct and efficient software systems (i.e., with emphasis on \"what\" component-based systems should contain). On the other hand, teaching students about current commercial component technologies exposes students to the important dimension of best commercial practice (i.e., with emphasis on \"how\" component-based systems could be built).The participants of this panel are all actively involved in the development of courses and curricula that provide various perspectives on component-based systems. They represent both the first principles and commercial perspectives discussed above. Position statements for each of the panelists appear below.","venue":"SIGCSE '01","listofauthors":"Allen S. Parrish, J. Hollingsworth, P. Maurer, B. Shults, B. Weide","citations":1,"year":2001,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2001,\n\tdoi = {10.1145/364447.364756},\n\turl = {https://doi.org/10.1145%2F364447.364756},\n\tyear = 2001,\n\tpublisher = {{ACM} Press},\n\tauthor = {Allen Parrish and Joe Hollingsworth and Peter Maurer and Benjamin Shults and Bruce Weide},\n\ttitle = {Identifying an appropriate view of software components for undergraduate education}\n}","authorsSemantic":[6]},{"id":324,"title":"Techniques for high-speed digital circuit simulation","doi":null,"description":"Digital integrated circuits have grown in size and density at an exponential rate since their inception. With this growth, the time and effort involved in verifying the design has also increased significantly. Verification by circuit simulation has become the only practical solution. Many verification solutions exist, but the performance of these solutions is not keeping up with the growth of the designs. In addition, the memory requirement of simulating large designs is also growing at an extremely fast rate. \nThree techniques for high speed digital circuit simulation are presented: unit-delay, multi-delay and packed vector simulation. All of the techniques presented are based on the Inversion algorithm. The Inversion algorithm is a novel technique for event-driven, zero-delay, gate level simulation. To perform timing analysis, a crucial part of the verification process, a simulator must be able to perform unit-delay and multi-delay simulation. \nThe Inversion algorithm is able to perform very efficient zero-delay simulation by using statically linked data structures to represent the components of the circuit during simulation. For unit-delay and multi-delay simulation, circuit components may be simulated multiple times for each input vector. To retain the use of static data structures, multiple copies of each data structure must be available during circuit simulation. This dissertation presents several techniques to significantly reduce the number of data structures created for simulation by analyzing the circuit and creating the minimum number of data structures necessary to correctly simulate the circuit. \nCompiled code simulators have been performing parallel simulation on a uni-processor machine for many years. Packed vector simulation can be accomplished by packing the single bit value of a signal for each input vector to be simulated into a word. Because workstations can perform bit-wise logical operations, each of the bits in the word can represent the signal value for a specific input vector. The Inversion algorithm does not represent gate values as bits and does not perform logical operations for simulation. Instead the algorithm uses a counting technique to perform simulation and stores the state of a gate as a count. To perform parallel simulation on a uni-processor machine with the Inversion algorithm, the counts representing a gate's state must be packed together and manipulated in parallel. A technique is presented that allows the Inversion algorithm to perform parallel simulation with a significant increase in the performance of the simulator.","venue":"","listofauthors":"William J. Schilp, P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":325,"title":"Components: What If They Gave a Revolution and Nobody Came?","doi":"10.1109/2.846315","description":"There have been three great revolutions in computing technology during the past 50 years: the stored-program computer, high-level languages and component-level programming. Although working programmers are well aware of this last revolution, it seems to have escaped the notice of most everyone else. The author feels that academic researchers are doing little or nothing that touches the subject and apart from trade journals and magazines aimed at developers that publishers have all but ignored it. The component-level programming revolution has already happened in the academic community and in the authors opinion, nobody came. The author starts by giving a brief history of components, detailing the controls (tools that allow developers to create visually pleasing dialog boxes by drawing such devices as buttons), visual innovations and the data control of Visual Basic. The author moves on to current component technology. He explains ActiveX and other recent technological developments (like dynamic instantiation and universal standard). He finishes by looking into the future of component technology, explaining semipersistence and marketing flexibility and solutions to transmitting problems.","venue":"Computer","listofauthors":"P. Maurer","citations":25,"year":2000,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"28-34","volume":"33","number":"6","bibtex":"@article{2000,\n\tdoi = {10.1109/2.846315},\n\turl = {https://doi.org/10.1109%2F2.846315},\n\tyear = 2000,\n\tmonth = {jun},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {33},\n\tnumber = {6},\n\tpages = {28--34},\n\tauthor = {P.M. Maurer},\n\ttitle = {Components: what if they gave a revolution and nobody came?}\n}","authorsSemantic":[6]},{"id":378,"title":"Scheduling High-Level Blocks for Functional Simulation","doi":"10.1145/74382.74398","description":"This paper presents a method for scheduling high-level blocks for functional simulation under the assumptions that circuits may be cyclic (due to element grouping), and that blocks cannot be broken down into simpler elements. The solution presented here may simulate one block many times per clock period. Obtaining a minimal schedule for a cyclic circuit is shown to be NP-complete, and two approximation algorithms are presented, along with empirical data to evaluate their effectiveness.","venue":"26th ACM/IEEE Design Automation Conference","listofauthors":"Zhicheng Wang, P. Maurer","citations":20,"year":1989,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1989,\n\tdoi = {10.1145/74382.74398},\n\turl = {https://doi.org/10.1145%2F74382.74398},\n\tyear = 1989,\n\tpublisher = {{ACM} Press},\n\tauthor = {Z. Wang and P. M. Maurer},\n\ttitle = {Scheduling high-level blocks for functional simulation}\n}","authorsSemantic":[6]},{"id":326,"title":"State-machine based logic simulation using three logic values","doi":"10.1109/ICVD.2000.812645","description":"The inversion algorithm is an event-driven logic simulation technique that is competitive with levelized compiled code simulation even at extremely high activity levels. Previous versions of the algorithm have been limited to two-valued simulation. The three-valued inversion algorithm represents a radical redesign of the algorithm to permit three-valued simulation. The three-valued scheduling technique is significantly more complex than that of the two-valued algorithm, as is the procedure for collapsing simultaneous events. The three-valued algorithm is also capable of converting portions of the simulation from three-valued to two-valued once stable binary values have been achieved. This permits significant enhancements in the performance of the algorithm. Experimental data shows that, like the two-valued algorithm, the three-valued algorithm is competitive with two-valued levelized compiled code simulation.","venue":"VLSI Design 2000. Wireless and Digital Imaging in the Millennium. Proceedings of 13th International Conference on VLSI Design","listofauthors":"P. Maurer, William J. Schilp","citations":2,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/icvd.2000.812645},\n\turl = {https://doi.org/10.1109%2Ficvd.2000.812645},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {P.M. Maurer and W.J. Schilp},\n\ttitle = {State-machine based logic simulation using three logic values}\n}","authorsSemantic":[6]},{"id":327,"title":"Event driven simulation without loops or conditionals","doi":"10.1109/ICCAD.2000.896445","description":"The past several years have seen much research in event driven logic simulation. Various logic and delay models have been explored. Most simulation research has focused on improving simulation performance. New approaches to both compiled and event driven simulation have been explored. The internal operations of event-driven simulators can be divided into two categories, scheduling, and gate simulation. Much effort has been focused on reducing the cost of scheduling. There has also been effort to reduce the cost of gate simulation. It has also been shown that explicit computation of gate outputs is unnecessary, as long as event propagation is computed correctly. Even though research has reduced the complexity of both scheduling and gate simulation, it is still necessary to test for event propagation and cancellation, and it is necessary to perform some computations during gate simulation. This paper shows that none of these computations are necessary. Most computations are devoted to testing internal states and computing new internal states. In our technique, subroutine addresses are used to maintain states. This permits the elimination of all state-testing and state-computation code. Our technique is significantly faster than conventional event-driven simulation. Unlike earlier methods, our approach can easily be extended to any logic model or any delay model.","venue":"IEEE/ACM International Conference on Computer Aided Design. ICCAD - 2000. IEEE/ACM Digest of Technical Papers (Cat. No.00CH37140)","listofauthors":"P. Maurer","citations":17,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/iccad.2000.896445},\n\turl = {https://doi.org/10.1109%2Ficcad.2000.896445},\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer},\n\ttitle = {Event driven simulation without loops or conditionals}\n}","authorsSemantic":[6]},{"id":328,"title":"Logic simulation using networks of state machines","doi":"10.1145/343647.343890","description":"This paper shows how to simulate a circuit as an interlocked collection of state machines. Separate state-machines are used to represent nets and gates. The technique permits intermixing of logic models, direct simulation of higher-level functions, and optimization techniques for fanout free circuits. These techniques are an extension of techniques that have been used to achieve high-performance event-driven simulations. New more efficient state-machine implementations are presented, and experimental data is presented that show the efficiency of the new techniques.","venue":"DATE '00","listofauthors":"P. Maurer","citations":5,"year":2000,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2000,\n\tdoi = {10.1145/343647.343890},\n\turl = {https://doi.org/10.1145%2F343647.343890},\n\tyear = 2000,\n\tpublisher = {{ACM} Press},\n\tauthor = {Peter M. Maurer},\n\ttitle = {Logic simulation using networks of state machines}\n}","authorsSemantic":[6]},{"id":329,"title":"Interactive Logic Design on the World Wide Web","doi":"10.1007/978-94-015-9506-3_46","description":"Recently, the University of South Florida Depatttnent of Computer Science and Engineering began a program to provide web-based instruction along with web-based laboratory work for a number of our basic computer engineering courses. The philosopy of this program is to provide a series of “electronic textbooks,” complete with interactive exercises. This program is actually part of a much larger program to create a comprehensive series of web-based Electrical Design Automation (EDA) tools. However, the educational portion of the program is important, and capable of standing on its own merit.","venue":"","listofauthors":"P. Maurer, M. Varanasi","citations":0,"year":2000,"publisher":"Springer Netherlands","pages":"201-204","volume":null,"number":null,"bibtex":"@incollection{2000,\n\tdoi = {10.1007/978-94-015-9506-3_46},\n\turl = {https://doi.org/10.1007%2F978-94-015-9506-3_46},\n\tyear = 2000,\n\tpublisher = {Springer Netherlands},\n\tpages = {201--204},\n\tauthor = {Peter M. Maurer and Murali Varanasi},\n\ttitle = {Interactive Logic Design on the World Wide Web}\n}","authorsSemantic":[6]},{"id":330,"title":"Logic simulation using networks of state machines","doi":"10.1109/DATE.2000.840859","description":"This paper shows how to simulate a circuit as an interlocked collection of state machines. Separate state-machines are used to represent nets and gates. The technique permits intermixing of logic models, direct simulation of higher-level functions, and optimization techniques for fanout free circuits. These techniques are an extension of techniques that have been used to achieve high-performance event-driven simulations. New more efficient state-machine implementations are presented, and experimental data is presented that show the efficiency of the new techniques.","venue":"Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)","listofauthors":"P. Maurer","citations":5,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/date.2000.840859},\n\turl = {https://doi.org/10.1109%2Fdate.2000.840859},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {P.M. Maurer},\n\ttitle = {Logic simulation using networks of state machines}\n}","authorsSemantic":[6]},{"id":331,"title":"Component-level programming: a revolution in software technology","doi":"10.1109/FIE.1999.841564","description":"A profound change is taking place in the world of applications programming. Some of the most profound new developments are web-based hosting of components and distributed components that run interactively on several different machines. Students can be exposed to component-level programming through a visual language such as Visual Basic, but this will not expose the student to the full scope of the most recent developments. This paper describes the advances in component-level programming, and gives suggestions for integrating component-level development into existing curricula. The paper first describes custom controls, the precursor to component-level programming, and shows how the concepts of custom controls have evolved into more comprehensive standards. The use of existing components in several different contexts is discussed. The most important contexts are standard application development, web-based design, and distributed application development. We show how a single component could be used in each of these contexts. We illustrate these uses with examples from our own work in VLSI Design Automation. In addition we discuss the development of new components, and how best to integrate component development into the undergraduate curriculum. Component development is discussed in the context of the emerging COM standard which is available on a wide variety of platforms and a wide variety of programming languages.","venue":"FIE'99 Frontiers in Education. 29th Annual Frontiers in Education Conference. Designing the Future of Science and Engineering Education. Conference Proceedings (IEEE Cat. No.99CH37011","listofauthors":"P. Maurer, M. Varanasi, S. Katkoori, W. Mak","citations":0,"year":0,"publisher":"Stripes Publishing L.L.C","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/fie.1999.841564},\n\turl = {https://doi.org/10.1109%2Ffie.1999.841564},\n\tpublisher = {Stripes Publishing L.L.C},\n\tauthor = {P.M. Maurer and M. Varanasi and S. Katkoori and  Wai-Kai Mak},\n\ttitle = {Component-level programming: a revolution in software technology}\n}","authorsSemantic":[6]},{"id":332,"title":"Software bit-slicing: a technique for improving simulation performance","doi":"10.1145/307418.307514","description":"For some types of simulation, it is difficult or impossible to improve performance by packing several vectors to be packed into a single word. One example of such an algorithm is inversion algorithm, which does not represent net values in the conventional way. This paper presents a novel technique, called software bit-slicing for performing simultaneous simulation of several input vectors on a conventional uniprocessor. As with conventional vector-packing techniques, this technique is able to assign a different input vector to each bit of a word, permitting the simultaneous simulation of n vectors, where n is the number of bits in a word. The inversion algorithm is used to give an example of this technique. For this example, a 6x speedup can be realized by using software bit-slicing. The same technique should be widely applicable to many different types of simulation.","venue":"Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)","listofauthors":"P. Maurer, William J. Schilp","citations":4,"year":1999,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1999,\n\tdoi = {10.1145/307418.307514},\n\turl = {https://doi.org/10.1145%2F307418.307514},\n\tyear = 1999,\n\tpublisher = {{ACM} Press},\n\tauthor = {Peter M. Maurer and William J. Schilp},\n\ttitle = {Software bit-slicing}\n}","authorsSemantic":[6]},{"id":333,"title":"Efficient simulation for hierarchical and partitioned circuits","doi":"10.1109/ICVD.1999.745154","description":"This paper presents new, highly-efficient techniques for simulating extremely large circuits, assuming that hierarchical design techniques have been used. Both hierarchical and partitioned circuits consist of a master circuit and several sub-circuits. Hierarchical circuits permit sub-circuits to be reused, while partitioned circuits permit only a single use of each sub-circuit. Both types of circuits permit multiple levels of hierarchy. In partitioned circuits, triggering is used to perform simulations that are several times faster than Levelized Compiled Code (LCC) simulation. For hierarchical simulation, the concept of boundary activity is introduced. Optimization with respect to boundary activity can produce simulations that are much faster than ordinary flat simulations. It is further shown that hierarchical design can permit the efficient simulation of circuits that cannot be simulated on a single workstation using ordinary flat simulation. Aggressive use of hierarchy is used to demonstrate the simulation of circuits containing as many as four billion (4,000,000,000) gates.","venue":"Proceedings Twelfth International Conference on VLSI Design. (Cat. No.PR00013)","listofauthors":"P. Maurer","citations":4,"year":1999,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1999,\n\tdoi = {10.1109/icvd.1999.745154},\n\turl = {https://doi.org/10.1109%2Ficvd.1999.745154},\n\tyear = 1999,\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer},\n\ttitle = {Efficient simulation for hierarchical and partitioned circuits}\n}","authorsSemantic":[6]},{"id":334,"title":"Enhancing the hardware design experience for computer engineers","doi":"10.1109/FIE.1998.736802","description":"People become engineers because they love to build things. With the recent progress in design automation tools, it is now possible to revise the core computer engineering curriculum to include realistic design experiences in virtually all courses, even those that are not usually thought of as being laboratory courses. The use of design automation makes it possible to assign a large number of simple design exercises in low-level courses, and a few more complex projects in more advanced courses. This paper describes enhancements to three core courses: logic design; computer architecture; and digital system design. These changes are based on a common set of tools that are used throughout the curriculum. Several design exercises are described, along with a suggested sequence for the various exercises. Several laboratory manuals have been developed for use with standard textbooks. The references provide pointers to web sites where the laboratory manuals and tools can be downloaded. Suggestions for textbooks for each course are provided, although the laboratory materials should be useful with a wide variety of texts. Classroom experience and student reactions are also reported.","venue":"FIE '98. 28th Annual Frontiers in Education Conference. Moving from 'Teacher-Centered' to 'Learner-Centered' Education. Conference Proceedings (Cat. No.98CH36214)","listofauthors":"P. Maurer","citations":7,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/fie.1998.736802},\n\turl = {https://doi.org/10.1109%2Ffie.1998.736802},\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer},\n\ttitle = {Enhancing the hardware design experience for computer engineers}\n}","authorsSemantic":[6]},{"id":335,"title":"Packed Input Vector Simulation with the Inversion Algorithm *","doi":null,"description":"The Inversion Algorithm is an event driven algorithm whose performance meets or exceeds that of traditional compiled-code simulators. However, existing implementations of the algorithm do not allow several vectors to be packed into a single word, thus limiting the power of the algorithm. Because the Inversion Algorithm does not represent net values in the conventional way, the simple packing techniques used with other algorithms cannot be used with the Inversion Algorithm. This paper presents a novel technique for performing simultaneous simulation of several input vectors on a conventional uniprocessor. As with conventional vector-packing techniques, this technique is able to assign a different input vector to each bit of a word, permitting the simultaneous simulation of n vectors, where n is the number of bits in a word. * This work was supported in part by the National Science Foundation under grant number MIP-9403414.","venue":"","listofauthors":"William J. Schilp, P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":336,"title":"Electrical design automation: an essential part of a computer engineer's education","doi":"10.1109/FIE.1998.738754","description":"There has been an explosion in engineering technology, due in large part to design automation tools. It is obvious that engineers need to learn to use these tools, but we would insist that it is equally important for engineers to learn the principles upon which these tools are based. Such knowledge allows engineers to play an active role in the development of new tools, and to formulate realistic expectations about future developments in design automation. We identify three major areas of electrical design automation, physical design automation, simulation, and high-level synthesis. We have developed course requirements for each area, and provide a list of important topics within each area. For each of the areas, we identify textbooks and other educational materials. We also provide recommendations for laboratory exercises, and provide pointers to course materials on the world wide web.","venue":"FIE '98. 28th Annual Frontiers in Education Conference. Moving from 'Teacher-Centered' to 'Learner-Centered' Education. Conference Proceedings (Cat. No.98CH36214)","listofauthors":"P. Maurer","citations":8,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/fie.1998.738754},\n\turl = {https://doi.org/10.1109%2Ffie.1998.738754},\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer},\n\ttitle = {Electrical design automation: an essential part of a computer engineer{\\textquotesingle}s education}\n}","authorsSemantic":[6]},{"id":474,"title":"P-Finder: Reconstruction of Signaling Networks from Protein-Protein Interactions and GO Annotations","doi":"10.1109/TCBB.2014.2355216","description":"Because most complex genetic diseases are caused by defects of cell signaling, illuminating a signaling cascade is essential for understanding their mechanisms. We present three novel computational algorithms to reconstruct signaling networks between a starting protein and an ending protein using genome-wide protein-protein interaction (PPI) networks and gene ontology (GO) annotation data. A signaling network is represented as a directed acyclic graph in a merged form of multiple linear pathways. An advanced semantic similarity metric is applied for weighting PPIs as the preprocessing of all three methods. The first algorithm repeatedly extends the list of nodes based on path frequency towards an ending protein. The second algorithm repeatedly appends edges based on the occurrence of network motifs which indicate the link patterns more frequently appearing in a PPI network than in a random graph. The last algorithm uses the information propagation technique which iteratively updates edge orientations based on the path strength and merges the selected directed edges. Our experimental results demonstrate that the proposed algorithms achieve higher accuracy than previous methods when they are tested on well-studied pathways of S. cerevisiae. Furthermore, we introduce an interactive web application tool, called P-Finder, to visualize reconstructed signaling networks.","venue":"IEEE/ACM Transactions on Computational Biology and Bioinformatics","listofauthors":"Young-Rae Cho, Yanan Xin, G. Speegle","citations":7,"year":2015,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"309-321","volume":"12","number":"2","bibtex":"@article{2015,\n\tdoi = {10.1109/tcbb.2014.2355216},\n\turl = {https://doi.org/10.1109%2Ftcbb.2014.2355216},\n\tyear = 2015,\n\tmonth = {mar},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {12},\n\tnumber = {2},\n\tpages = {309--321},\n\tauthor = {Young-Rae Cho and Yanan Xin and Greg Speegle},\n\ttitle = {P-Finder: Reconstruction of Signaling Networks from Protein-Protein Interactions and {GO} Annotations}\n}","authorsSemantic":[10]},{"id":337,"title":"TECHNIQUES FOR MULTI-LEVEL COMPILED SIMULATION","doi":null,"description":"This paper begins attacking the problem of multi-level compiled simulation by presenting a solution to the problem of independent compilation of subcircuits. The solution to this problem is an interface data-structure that contains a public section for I/O ports and a private section that contains internal signals and maintains internal states. One data structure is created for each instance of a subcircuit. The data structures are retained by the circuit-simulator that contains the instance, but are created by the simulator for the instance. This technique prevents changes in the subcircuit from forcing the recompilation of all circuits containing instances of the subcircuit. Since subcircuits can be compiled independently of one another, they can be compiled using different compilers. Furthermore any subroutine that complies with the interface specifications can be incorporated into the simulation. Translation tables are used to move one logic-system to another. Finally, the paper discusses compilation techniques for cyclic circuits. TECHNIQUES FOR MULTI-LEVEL COMPILED SIMULATION* Peter M. Maurer Zhicheng Wang Craig D. Morency Department of Computer Science and Engineering University of South Florida Tampa, FL 33620","venue":"","listofauthors":"P. Maurer, Zhicheng Wang, Craig D. Morency","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":338,"title":"Unit delay simulation with the inversion algorithm","doi":"10.1145/244522.244844","description":"The Inversion Algorithm is an event driven algorithm whose performance meets or exceeds that of Levelized Compiled Code simulation, even when the activity rate is unrealistically high. Existing implementations of the Inversion Algorithm are based on the Zero Delay model. This paper extends the algorithm to more realistic timing models. The main problems discussed in this paper are avoiding scheduling conflicts, and minimizing the amount of storage space. These problems are made considerably more difficult by the deletion of NOT gates and the collapsing of various connections. These optimizations transform the simulation into a multi-delay simulation under the transport delay model. A complete solution to the scheduling problem is presented under these conditions.","venue":"Proceedings of International Conference on Computer Aided Design","listofauthors":"William J. Schilp, P. Maurer","citations":4,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n<title>Error: DOI Not Found</title>\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n\n<link rel=\"icon\" href=\"/static/img/favicon.png\" />\n<link rel=\"shortcut icon\" href=\"/static/favicon.ico\" type=\"image/x-icon\" /> \n<link href=\"/static/style/new-style2.css\" rel=\"stylesheet\" type=\"text/css\" />\n</head>\n\n<body>\n\n\n<div style=\"background:#fcb426\">\n<img src=\"/static/img/banner-413.gif\" alt=\"Logo\" width=\"620\" height=\"137\" border=\"0\" />\n</div>\n\n<div style=\"height:1px;background:#000000\"></div>\n<div style=\"height:1px;background:#54524f\"></div>\n<div style=\"height:1px;background:#f6911e\"></div>\n\n\n<!-- TABLE FOR NAVIGATION BAR -->\n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" id=\"navtable\" align=\"center\">\n<tr>\n    <td width=\"34\" height=\"26\" bgcolor=\"#231f20\"><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"1\" height=\"1\" /></td>\n    \n    <td height=\"26\" bgcolor=\"#231f20\" class=\"navtext\">\n    <a href=\"http://www.doi.org/index.html\">HOME</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/hb.html\">HANDBOOK</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/factsheets.html\">FACTSHEETS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/faq.html\">FAQs</a> &nbsp;|&nbsp; <a href=\"http://www.doi.org/resources.html\">RESOURCES</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/users.html\">USERS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/announce.html\">NEWS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/idf-members/index.html\">MEMBERS AREA</a>\n    </td>    \n  </tr>\n</table>\n<!-- END TABLE FOR NAVIGATION BAR -->\n\n<div style=\"height:1px;background:#e3a44d\"></div>\n<div style=\"height:3px;background:#4d4942\"></div>\n\n\n\n<!-- TABLE FOR CONTENT -->      \n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" bgcolor=\"#ffffff\">\n<tr>\n<td colspan=\"6\">\n<img src=\"/static/img/transparent.gif\" alt=\"\" width=\"100\" height=\"20\" border=\"0\" />\n</td>\n</tr>\n\n<tr>\n\n<td valign=\"top\">\n\n<h2>DOI Not Found</h2>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n<h3>10.1145/244522.244844</h3>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n\n<p>This DOI cannot be found in the DOI System.  Possible reasons are:</p>\n\n\n<ul>\n\n<li style=\"padding-bottom: .5em;\">The DOI is incorrect in your source. Search for the item by name, title, or other metadata using a search engine.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI was copied incorrectly. Check to see that the string includes all the characters before and after the slash and no sentence punctuation marks.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI has not been activated yet.  Please try again later, and report the problem if the error continues.</li>\n\n</ul>\n\n\n\n<div class=\"divider\">&nbsp;</div>\n\n<p>You may report this error to the responsible DOI Registration Agency using the form below.  Include your email address to receive confirmation and feedback.</p>\n\n<div style=\"padding-left: 4em;\">\n\n<form action=\"/notfound\" method=\"post\" enctype=\"application/x-www-form-urlencoded\" name=\"notFoundForm\">\n\n\n<table border=\"0\" cellspacing=\"3\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>\n\n<table border=\"0\" align=\"center\" cellpadding=\"3\" cellspacing=\"3\">\n<tbody><tr>\n<th  align=\"right\" scope=\"row\"><label>DOI:</label></th>\n<td><input name=\"missingHandle\" type=\"text\" value=\"10.1145/244522.244844\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\"><label>URL of Web Page Listing the DOI:</label></th>\n<td><input name=\"referringPage\" type=\"text\" value=\"\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\">Your Email Address:</th>\n<td><input name=\"userEmailAddress\" type=\"text\" value=\"Please enter your email address\" size=\"42\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\" valign=\"top\">Additional Information About the Error:</th>\n<td><textarea name=\"comments\" cols=\"30\" rows=\"6\"></textarea></td>\n</tr>\n</tbody>\n</table>\n\n</td>\n</tr>\n<tr>\n\n<td align=\"right\"><p><input name=\"send\" type=\"submit\" value=\"Submit Error Report\" /></p></td>\n</tr>\n</tbody>\n</table>\n\n</form>\n</div>\n\n\n\n\n</td>\n<td><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"20\" height=\"20\" border=\"0\" /></td>\n</tr>\n</table>\n\n<div class=\"divider-full\">&nbsp;</div>\n\n<!-- TABLE FOR FOOTER -->\n\n<table  border=\"0\" cellpadding=\"0\" cellspacing=\"0\" align=\"center\">\n\n<tr>\n<td align=\"center\" colspan=\"2\">\n<a href=\"/help.html\">DOI Resolution Documentation</a>\n</td>\n</tr>\n\n<tr>\n<td align=\"left\" height=\"40\">\n<img src=\"/static/img/Logo_TM.png\" alt=\"DOI_disc_logo\" width=\"24\" height=\"24\" />\n</td>\n\n<td align=\"left\">\n<span style=\"padding-left: 0px; font-size: 11px;\"><span style=\"vertical-align: super;\">&reg;</span>, DOI<span style=\"vertical-align: super;\">&reg;</span>, DOI.ORG<span style=\"vertical-align: super;\">&reg;</span>, and shortDOI<span style=\"vertical-align: super;\">&reg;</span> are trademarks of the International DOI Foundation.</span>\n</td>       \n</tr>\n</table>\n</body>\n</html>\n","authorsSemantic":[6]},{"id":339,"title":"Bit-parallel multidelay simulation","doi":"10.1109/43.552088","description":"The multidelay parallel (MDP) technique is a multidelay logic simulation algorithm that uses no timing wheel, or any other event-sorting mechanism. Instead, wide bit-fields containing net-values for several different times are used to resolve out-of-order events. Bit-parallel operations are performed to simulate gates at the required times. The MDP technique was originally designed to be implemented in hardware, but the current software version of the algorithm has proven to be competitive with conventional event-driven multidelay simulation. Two versions of the MDP technique are presented in this paper, fixed alignment and variable alignment. The fixed alignment algorithm provides bit-fields that are wide enough to capture any event that could occur during the simulation of an input vector, while the variable alignment algorithm uses a minimum-width bit field which is just wide enough to capture those events that could occur at an individual step in the simulation. A prototype hardware design is discussed briefly.","venue":"IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.","listofauthors":"Y. Lee, P. Maurer","citations":4,"year":1996,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"1547-1554","volume":"15","number":"12","bibtex":"@article{1996,\n\tdoi = {10.1109/43.552088},\n\turl = {https://doi.org/10.1109%2F43.552088},\n\tyear = 1996,\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {15},\n\tnumber = {12},\n\tpages = {1547--1554},\n\tauthor = {Yun Sik Lee and P.M. Maurer},\n\ttitle = {Bit-parallel multidelay simulation}\n}","authorsSemantic":[6]},{"id":340,"title":"Sequential Decoding of Trellis Codes through ISI Channels","doi":null,"description":"The Fano algorithm performance is well understood for the standard additive white Gaussian noise (AWGN) channel, and has been shown to be a feasible decoding method for complex codes [12, 13]. This work aims to determine whether the Fano algorithm performance degrades relative to the Viterbi algorithm when the channel has intersymbol interference. We compare Fano and Viterbi decoding by implementing a short constraint-length code. The performance of the code over an ISI channel using Tomlinson-Harashima precoding is compared to the baseline case of a non-ISI channel with the same effective SNR. A long constraint-length code is also evaluated over both the ISI and non-ISI channels, using only Fano decoding (as Viterbi decoding is computationally infeasible). No significant difference was found between performance over the ISI and non-ISI channel. Thus, if sequential decoding can be used beneficially with high constraint length codes over AWGN channels, then it should be equally beneficial over ISI channels. Thesis Supervisor: Mitchell D. Trott Title: Assistant Professor","venue":"","listofauthors":"P. Maurer","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":341,"title":"OPTIMAL ALGORITHMS -- SUPPLEMENTAL NOTES","doi":null,"description":"null","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":342,"title":"A Multithreading Architecture with Multiple Independent Shared Pipelines","doi":null,"description":"PSAM consists of a number of multithreaded pipeline processors to support parallel computation. Thread control is implemented using a distributed approach, in which each processor can independently initiate and terminate a thread. This thread control mechanism is based on data ow model, and allows the degree of multiprocessing to vary with time. Although the basic processor is von Neumannbased, the overhead to support parallelism is very low.","venue":"","listofauthors":"Wei-Ming Lin, P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":343,"title":"SCHEDULING BLOCKS FOR HIERARCHICAL COMPILED SIMULATION","doi":null,"description":"Although preserving the hierarchy in compiled simulation can significantly reduce the compilation time for the code generated by the circuit compiler, the possibility of introducing pseudo-cycles due to element grouping can impair the performance of the generated code. A new approach to this problem is presented which uses dependency information to reduce the number of times a particular block must be simulated. The problem of determining the minimum schedule is shown to be NP-Complete, and a set of heuristics for the problem is presented. Experimental results for different combinations of these heuristics are presented. An algorithm for determining dependency information from the contents of a block is presented along with a new approach that can be used when the content of one or more blocks is unknown. SCHEDULING BLOCKS FOR HIERARCHICAL COMPILED SIMULATION Peter M. Maurer Department of Computer Science and Engineering University of South Florida Tampa, FL 33620","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":396,"title":"Grasp, Grab or Pinch? Identifying User Preference for In-Air Gestural Manipulation","doi":"10.1145/2983310.2989209","description":"In-air gestural interactions can be used in desktop or other similar systems to perform cursor-based movements typically done with a mouse. One form of interaction that is required here is that of gestural manipulation, such as the ability to select a target and then move or rotate it, or any other forms of manipulation. In this paper, we implemented and evaluated 3 different gestures for users to manipulate objects on a screen, which we referred to as \"grasp,\" \"grab\" and \"pinch.\" We performed a usability study, which showed a strong preference for the \"grasp\" gesture.","venue":"SUI","listofauthors":"Alvin Jude, G. M. Poor, Darren Guinness","citations":8,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/2983310.2989209},\n\turl = {https://doi.org/10.1145%2F2983310.2989209},\n\tyear = 2016,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Alvin Jude and G. Michael Poor and Darren Guinness},\n\ttitle = {Grasp, Grab or Pinch? Identifying User Preference for In-Air Gestural Manipulation}\n}","authorsSemantic":[7]},{"id":344,"title":"GATEWAYS: A TECHNIQUE FOR ADDING EVENT-DRIVEN BEHAVIOR TO COMPILED UNIT-DELAY SIMULATIONS","doi":null,"description":"The gateway technique is a method for switching segments of code into and out of the instruction stream. When added to the straight-line code generated by a compiled simulator, gateways can be used to enhance the performance of the generated code by switching only those segments of code that actually need to be executed into the instruction stream. The convergence algorithm is an oblivious compiled code algorithm that can be used with many different types of circuits, including cyclic asynchronous circuits. In its oblivious form, the convergence algorithm provides only modest gains in performance over interpreted event-driven simulation, but with the addition of gateways, the performance of the algorithm increases significantly. Experimental data shows that with gateways, the convergence algorithm runs in about 1/5th the time required for an interpreted event-driven simulation. Additional work has been done to reduce the amount of code generated by the convergence algorithm, and to enhance the locality of the code to improve its performance on machines with caches. GATEWAYS: A TECHNIQUE FOR ADDING EVENT-DRIVEN BEHAVIOR TO COMPILED UNIT-DELAY SIMULATIONS* Peter M. Maurer Department of Computer Science and Engineering University of South Florida Tampa, FL 33620","venue":"","listofauthors":"P. Maurer","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":345,"title":"USING GATEWAYS WITH LEVELIZED COMPILED SIMULATION","doi":null,"description":"Although Levelized Compiled Code simulation performs well under moderate to high activity conditions, there are many circuits that exhibit extremely low activity rates, either overall, or in certain subcircuits. The techniques presented here allow event-driven behavior to be added to Levelized Compiled Code simulations, with the aim of improving the performance of circuits with very low activity. Two techniques are presented, one which can be applied selectively on a block-by-block basis, and one that can be applied selectively on a gate-by-gate basis. Both concepts are based on the concept of a gateway, or retargetable branch. Performance results are presented for both techniques. USING GATEWAYS WITH LEVELIZED COMPILED SIMULATION Peter M. Maurer Department of Computer Science and Engineering University of South Florida Tampa, FL 33620","venue":"","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":346,"title":"Unit delay simulation with the inversion algorithm","doi":"10.1109/ICCAD.1996.569831","description":"The Inversion Algorithm is an event driven algorithm whose performance meets or exceeds that of Levelized Compiled Code simulation, even when the activity rate is unrealistically high. Existing implementations of the Inversion Algorithm are based on the Zero Delay model. This paper extends the algorithm to more realistic timing models. The main problems discussed in this paper are avoiding scheduling conflicts, and minimizing the amount of storage space. These problems are made considerably more difficult by the deletion of NOT gates and the collapsing of various connections. These optimizations transform the simulation into a multi-delay simulation under the transport delay model. A complete solution to the scheduling problem is presented under these conditions.","venue":"ICCAD 1996","listofauthors":"William J. Schilp, P. Maurer","citations":5,"year":0,"publisher":"IEEE Comput. Soc. Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/iccad.1996.569831},\n\turl = {https://doi.org/10.1109%2Ficcad.1996.569831},\n\tpublisher = {{IEEE} Comput. Soc. Press},\n\tauthor = {W.J. Schilp and P.M. Maurer},\n\ttitle = {Unit delay simulation with the inversion algorithm}\n}","authorsSemantic":[6]},{"id":347,"title":"Strategies for Implementing a Multithreaded Shared Pipeline Processor 1","doi":null,"description":"Themanagement of parallelism, the simultaneous management of multiple environments, and the synchronization of cooperating threads are some of the fundamental issues that need to be solved in a multithreaded pipelined architecture. In this paper, we present several di erent solutions to these problems which are currently implemented in a cycle-accurate software simulator of a multithreaded architecture called the SAM architecture. The simulator is a software prototype of a chip we expect to design and build within the coming year. The architecture contains two special instructions that allow parallelism to vary with time, and a two-level thread queue that allows the degree of parallelism to expand, theoretically, without limit, while still allowing for e cient dispatching of instructions. A concept called the short memory is introduced to support multiple environments. Because we intend to implement the SAM architecture as a real, functioning, generalpurpose machine, we have also addressed such mundane issues as interrupts, traps, system calls, and process synchronization.","venue":"","listofauthors":"Wei-Ming Lin, P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":348,"title":"SAM: A Multithreaded Pipeline Architecture for Data ow Computing","doi":null,"description":"SAM is a multithreaded pipeline architecture which supports multiple instruction threads running on a single shared high speed pipeline. It supports a data ow-style of programming in which the parallelism in the program is represented as a directed acyclic graph. Because the SAM architecture is basically an extension of the von Neumann architecture, conventional sequential programming is also supported. The SAM architecture resembles a RISC machine with a few additional features. Both experimental data and mathematical analysis show that the SAM architecture can achieve a pipeline e ciency of nearly 100% with minimal overhead.","venue":"","listofauthors":"Wei-Ming Lin, P. Maurer","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":349,"title":"Is compiled simulation really faster than interpreted simulation?","doi":"10.1109/ICVD.1996.489615","description":"It is commonly assumed that compiled simulation will provide significant performance improvements over interpreted event-driven simulation. This paper demonstrates that for a new algorithm called the Inversion Algorithm that this assumption is not true. The Inversion Algorithm can be run in either compiled or interpreted mode with only a slight difference in performance. Experimental data confirms this, and also demonstrates that first simulation results can be obtained much faster using the interpreted form of the algorithm.","venue":"Proceedings of 9th International Conference on VLSI Design","listofauthors":"P. Maurer","citations":1,"year":0,"publisher":"IEEE Comput. Soc. Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/icvd.1996.489615},\n\turl = {https://doi.org/10.1109%2Ficvd.1996.489615},\n\tpublisher = {{IEEE} Comput. Soc. Press},\n\tauthor = {P.M. Maurer},\n\ttitle = {Is compiled simulation really faster than interpreted simulation?}\n}","authorsSemantic":[6]},{"id":350,"title":"The Inversion Algorithm For Digital Simulation","doi":"10.1109/ICCAD.1994.629776","description":"The Inversion Algorithm is an event-driven algorithm, whose performance rivals or exceeds that of Levelized Compiled code simulation, even at activity rates of 50% or more. The Inversion Algorithm has several unique features, the most remarkable of which is the size of the run-time code. The basic Algorithm can be implemented using no more than a page of run-time code, although in practice it is more efficient to provide several different variations of the basic algorithm. The run-time code is independent of the circuit under test, so the algorithm can be implemented either as a compiled code or an interpreted simulator with little variation in performance. Because of the small size of the run-time code, the run-time portions of the Inversion Algorithm can be implemented in assembly language for peak efficiency, and still be retargeted for new platforms with little effort.","venue":"IEEE/ACM International Conference on Computer-Aided Design","listofauthors":"P. Maurer","citations":2,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/iccad.1994.629776},\n\turl = {https://doi.org/10.1109%2Ficcad.1994.629776},\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer},\n\ttitle = {The Inversion Algorithm For Digital Simulation}\n}","authorsSemantic":[6]},{"id":475,"title":"Recommendations Made Easy","doi":null,"description":"null","venue":"","listofauthors":"Darren Guinness, P. Karbasi, Rovshen Nazarov, G. Speegle","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":351,"title":"Gateways: a technique for adding event-driven behavior to compiled simulations","doi":"10.1109/43.265675","description":"The gateway technique is a method for switching segments of code into and out of the instruction stream. When added to the straight-line code generated by a compiled simulator, gateways can be used to enhance the performance of the generated code by switching only those segments of code that actually need to be executed into the instruction stream. The convergence algorithm is an oblivious compiled code algorithm that can be used with many different types of circuits, including cyclic asynchronous circuits. In its oblivious form, the convergence algorithm provides only modest gains in performance over interpreted event-driven simulation, but with the addition of gateways, the performance of the algorithm increases significantly. Experimental data shows that with gateways, the convergence algorithm runs in about 1/5th the time required for an interpreted event-driven simulation. Additional work has been done to reduce the amount of code generated by the convergence algorithm, and to enhance the locality of the code to improve its performance on machines with caches. When used with a multi-delay algorithm, gateways allow simulations to be performed in 1/3 the time required by interpreted simulations. Gateways also allow zero-delay simulations to be more responsive to the activity rate of the circuit, and allow event driven simulations to outperform levelized compiled code when the activity rate of the circuit falls below 13%. >","venue":"IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.","listofauthors":"P. Maurer, Y. Lee","citations":15,"year":1994,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"338-352","volume":"13","number":"3","bibtex":"@article{1994,\n\tdoi = {10.1109/43.265675},\n\turl = {https://doi.org/10.1109%2F43.265675},\n\tyear = 1994,\n\tmonth = {mar},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {13},\n\tnumber = {3},\n\tpages = {338--352},\n\tauthor = {P.M. Maurer and  Yun Sik Lee},\n\ttitle = {Gateways: a technique for adding event-driven behavior to compiled simulations}\n}","authorsSemantic":[6]},{"id":352,"title":"The Inversion Algorithm for digital simulation","doi":"10.1145/191326.191425","description":"The Inversion Algorithm is an event-driven algorithm, whose performance rivals or exceeds that of Levelized Compiled code simulation, even at activity rates of 50% or more. The Inversion Algorithm has several unique features, the most remarkable of which is the size of the run-time code. The basic Algorithm can be implemented using no more than a page of run-time code, although in practice it is more efficient to provide several different variations of the basic algorithm. The run-time code is independent of the circuit under test, so the algorithm can be implemented either as a compiled code or an interpreted simulator with little variation in performance. Because of the small size of the run-time code, the run-time portions of the Inversion Algorithm can be implemented in assembly language for peak efficiency, and still be retargeted for new platforms with little effort. THE INVERSION ALGORITHM FOR DIGITAL SIMULATION","venue":"ICCAD","listofauthors":"P. Maurer","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n<title>Error: DOI Not Found</title>\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n\n<link rel=\"icon\" href=\"/static/img/favicon.png\" />\n<link rel=\"shortcut icon\" href=\"/static/favicon.ico\" type=\"image/x-icon\" /> \n<link href=\"/static/style/new-style2.css\" rel=\"stylesheet\" type=\"text/css\" />\n</head>\n\n<body>\n\n\n<div style=\"background:#fcb426\">\n<img src=\"/static/img/banner-413.gif\" alt=\"Logo\" width=\"620\" height=\"137\" border=\"0\" />\n</div>\n\n<div style=\"height:1px;background:#000000\"></div>\n<div style=\"height:1px;background:#54524f\"></div>\n<div style=\"height:1px;background:#f6911e\"></div>\n\n\n<!-- TABLE FOR NAVIGATION BAR -->\n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" id=\"navtable\" align=\"center\">\n<tr>\n    <td width=\"34\" height=\"26\" bgcolor=\"#231f20\"><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"1\" height=\"1\" /></td>\n    \n    <td height=\"26\" bgcolor=\"#231f20\" class=\"navtext\">\n    <a href=\"http://www.doi.org/index.html\">HOME</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/hb.html\">HANDBOOK</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/factsheets.html\">FACTSHEETS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/faq.html\">FAQs</a> &nbsp;|&nbsp; <a href=\"http://www.doi.org/resources.html\">RESOURCES</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/users.html\">USERS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/announce.html\">NEWS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/idf-members/index.html\">MEMBERS AREA</a>\n    </td>    \n  </tr>\n</table>\n<!-- END TABLE FOR NAVIGATION BAR -->\n\n<div style=\"height:1px;background:#e3a44d\"></div>\n<div style=\"height:3px;background:#4d4942\"></div>\n\n\n\n<!-- TABLE FOR CONTENT -->      \n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" bgcolor=\"#ffffff\">\n<tr>\n<td colspan=\"6\">\n<img src=\"/static/img/transparent.gif\" alt=\"\" width=\"100\" height=\"20\" border=\"0\" />\n</td>\n</tr>\n\n<tr>\n\n<td valign=\"top\">\n\n<h2>DOI Not Found</h2>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n<h3>10.1145/191326.191425</h3>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n\n<p>This DOI cannot be found in the DOI System.  Possible reasons are:</p>\n\n\n<ul>\n\n<li style=\"padding-bottom: .5em;\">The DOI is incorrect in your source. Search for the item by name, title, or other metadata using a search engine.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI was copied incorrectly. Check to see that the string includes all the characters before and after the slash and no sentence punctuation marks.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI has not been activated yet.  Please try again later, and report the problem if the error continues.</li>\n\n</ul>\n\n\n\n<div class=\"divider\">&nbsp;</div>\n\n<p>You may report this error to the responsible DOI Registration Agency using the form below.  Include your email address to receive confirmation and feedback.</p>\n\n<div style=\"padding-left: 4em;\">\n\n<form action=\"/notfound\" method=\"post\" enctype=\"application/x-www-form-urlencoded\" name=\"notFoundForm\">\n\n\n<table border=\"0\" cellspacing=\"3\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>\n\n<table border=\"0\" align=\"center\" cellpadding=\"3\" cellspacing=\"3\">\n<tbody><tr>\n<th  align=\"right\" scope=\"row\"><label>DOI:</label></th>\n<td><input name=\"missingHandle\" type=\"text\" value=\"10.1145/191326.191425\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\"><label>URL of Web Page Listing the DOI:</label></th>\n<td><input name=\"referringPage\" type=\"text\" value=\"\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\">Your Email Address:</th>\n<td><input name=\"userEmailAddress\" type=\"text\" value=\"Please enter your email address\" size=\"42\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\" valign=\"top\">Additional Information About the Error:</th>\n<td><textarea name=\"comments\" cols=\"30\" rows=\"6\"></textarea></td>\n</tr>\n</tbody>\n</table>\n\n</td>\n</tr>\n<tr>\n\n<td align=\"right\"><p><input name=\"send\" type=\"submit\" value=\"Submit Error Report\" /></p></td>\n</tr>\n</tbody>\n</table>\n\n</form>\n</div>\n\n\n\n\n</td>\n<td><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"20\" height=\"20\" border=\"0\" /></td>\n</tr>\n</table>\n\n<div class=\"divider-full\">&nbsp;</div>\n\n<!-- TABLE FOR FOOTER -->\n\n<table  border=\"0\" cellpadding=\"0\" cellspacing=\"0\" align=\"center\">\n\n<tr>\n<td align=\"center\" colspan=\"2\">\n<a href=\"/help.html\">DOI Resolution Documentation</a>\n</td>\n</tr>\n\n<tr>\n<td align=\"left\" height=\"40\">\n<img src=\"/static/img/Logo_TM.png\" alt=\"DOI_disc_logo\" width=\"24\" height=\"24\" />\n</td>\n\n<td align=\"left\">\n<span style=\"padding-left: 0px; font-size: 11px;\"><span style=\"vertical-align: super;\">&reg;</span>, DOI<span style=\"vertical-align: super;\">&reg;</span>, DOI.ORG<span style=\"vertical-align: super;\">&reg;</span>, and shortDOI<span style=\"vertical-align: super;\">&reg;</span> are trademarks of the International DOI Foundation.</span>\n</td>       \n</tr>\n</table>\n</body>\n</html>\n","authorsSemantic":[6]},{"id":353,"title":"The Inversion Algorithm for digital simulation","doi":"10.1109/43.644038","description":"The Inversion Algorithm is an event-driven algorithm, whose performance rivals or exceeds that of Levelized Compiled code simulation, even at activity rates of 50% or more. The Inversion Algorithm has several unique features, the most remarkable of which is the size of the run-time code. The basic Algorithm can be implemented using no more than a page of run-time code, although in practice it is more efficient to provide several different variations of the basic algorithm. The run-time code is independent of the circuit under test, so the algorithm can be implemented either as a compiled code or an interpreted simulator with little variation in performance. Because of the small size of the run-time code, the run-time portions of the Inversion Algorithm can be implemented in assembly language for peak efficiency, and still be retargeted for new platforms with little effort.","venue":"ICCAD '94","listofauthors":"P. Maurer","citations":29,"year":1997,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"762-769","volume":"16","number":"7","bibtex":"@article{1997,\n\tdoi = {10.1109/43.644038},\n\turl = {https://doi.org/10.1109%2F43.644038},\n\tyear = 1997,\n\tmonth = {jul},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {16},\n\tnumber = {7},\n\tpages = {762--769},\n\tauthor = {P.M. Maurer},\n\ttitle = {The inversion algorithm for digital simulation}\n}","authorsSemantic":[6]},{"id":354,"title":"Parallel multi-delay simulation","doi":"10.1145/259794.259914","description":"The Multi-DelayParallel (MDP) algorithm is an unconventional multi-delay algorithm in that it uses no timing wheel, or any event-sorting mechanism of any kind. Instead, wide bit-fields containing net values for several different times are used to resolve out-of-order events, and bit-parallel operations are performed to simulate the required gates. The MDP algorithm was designed to be implemented in hardware, and is currently being targeted for a future hardware simulation accelerator, but the current software version of the algorithm has proven to be competitive with conventional interpretive event-driven multi-delay simulation. Because the MDP algorithm can resolve out-of-order events without backing up, or even taking any special notice of them, similar techniques may prove useful for multi-processor event-driven simulation. CATEGORY: 7 Discrete Simulation","venue":"ICCAD '93","listofauthors":"Y. Lee, P. Maurer","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n        \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n<title>Error: DOI Not Found</title>\n\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n\n<link rel=\"icon\" href=\"/static/img/favicon.png\" />\n<link rel=\"shortcut icon\" href=\"/static/favicon.ico\" type=\"image/x-icon\" /> \n<link href=\"/static/style/new-style2.css\" rel=\"stylesheet\" type=\"text/css\" />\n</head>\n\n<body>\n\n\n<div style=\"background:#fcb426\">\n<img src=\"/static/img/banner-413.gif\" alt=\"Logo\" width=\"620\" height=\"137\" border=\"0\" />\n</div>\n\n<div style=\"height:1px;background:#000000\"></div>\n<div style=\"height:1px;background:#54524f\"></div>\n<div style=\"height:1px;background:#f6911e\"></div>\n\n\n<!-- TABLE FOR NAVIGATION BAR -->\n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" id=\"navtable\" align=\"center\">\n<tr>\n    <td width=\"34\" height=\"26\" bgcolor=\"#231f20\"><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"1\" height=\"1\" /></td>\n    \n    <td height=\"26\" bgcolor=\"#231f20\" class=\"navtext\">\n    <a href=\"http://www.doi.org/index.html\">HOME</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/hb.html\">HANDBOOK</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/factsheets.html\">FACTSHEETS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/faq.html\">FAQs</a> &nbsp;|&nbsp; <a href=\"http://www.doi.org/resources.html\">RESOURCES</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/users.html\">USERS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/announce.html\">NEWS</a> &nbsp;|&nbsp;<a href=\"http://www.doi.org/idf-members/index.html\">MEMBERS AREA</a>\n    </td>    \n  </tr>\n</table>\n<!-- END TABLE FOR NAVIGATION BAR -->\n\n<div style=\"height:1px;background:#e3a44d\"></div>\n<div style=\"height:3px;background:#4d4942\"></div>\n\n\n\n<!-- TABLE FOR CONTENT -->      \n<table width=\"100%\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" bgcolor=\"#ffffff\">\n<tr>\n<td colspan=\"6\">\n<img src=\"/static/img/transparent.gif\" alt=\"\" width=\"100\" height=\"20\" border=\"0\" />\n</td>\n</tr>\n\n<tr>\n\n<td valign=\"top\">\n\n<h2>DOI Not Found</h2>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n<h3>10.1145/259794.259914</h3>\n\n<div class=\"divider\">&nbsp;</div>\n\n\n\n\n<p>This DOI cannot be found in the DOI System.  Possible reasons are:</p>\n\n\n<ul>\n\n<li style=\"padding-bottom: .5em;\">The DOI is incorrect in your source. Search for the item by name, title, or other metadata using a search engine.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI was copied incorrectly. Check to see that the string includes all the characters before and after the slash and no sentence punctuation marks.</li>\n\n<li style=\"padding-bottom: .5em;\">The DOI has not been activated yet.  Please try again later, and report the problem if the error continues.</li>\n\n</ul>\n\n\n\n<div class=\"divider\">&nbsp;</div>\n\n<p>You may report this error to the responsible DOI Registration Agency using the form below.  Include your email address to receive confirmation and feedback.</p>\n\n<div style=\"padding-left: 4em;\">\n\n<form action=\"/notfound\" method=\"post\" enctype=\"application/x-www-form-urlencoded\" name=\"notFoundForm\">\n\n\n<table border=\"0\" cellspacing=\"3\" cellpadding=\"3\">\n<tbody>\n<tr>\n<td>\n\n<table border=\"0\" align=\"center\" cellpadding=\"3\" cellspacing=\"3\">\n<tbody><tr>\n<th  align=\"right\" scope=\"row\"><label>DOI:</label></th>\n<td><input name=\"missingHandle\" type=\"text\" value=\"10.1145/259794.259914\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\"><label>URL of Web Page Listing the DOI:</label></th>\n<td><input name=\"referringPage\" type=\"text\" value=\"\" size=\"42\" readonly=\"readonly\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\">Your Email Address:</th>\n<td><input name=\"userEmailAddress\" type=\"text\" value=\"Please enter your email address\" size=\"42\" /></td>\n</tr>\n<tr>\n<th align=\"right\" scope=\"row\" valign=\"top\">Additional Information About the Error:</th>\n<td><textarea name=\"comments\" cols=\"30\" rows=\"6\"></textarea></td>\n</tr>\n</tbody>\n</table>\n\n</td>\n</tr>\n<tr>\n\n<td align=\"right\"><p><input name=\"send\" type=\"submit\" value=\"Submit Error Report\" /></p></td>\n</tr>\n</tbody>\n</table>\n\n</form>\n</div>\n\n\n\n\n</td>\n<td><img src=\"/static/img/transparent.gif\" alt=\"\" width=\"20\" height=\"20\" border=\"0\" /></td>\n</tr>\n</table>\n\n<div class=\"divider-full\">&nbsp;</div>\n\n<!-- TABLE FOR FOOTER -->\n\n<table  border=\"0\" cellpadding=\"0\" cellspacing=\"0\" align=\"center\">\n\n<tr>\n<td align=\"center\" colspan=\"2\">\n<a href=\"/help.html\">DOI Resolution Documentation</a>\n</td>\n</tr>\n\n<tr>\n<td align=\"left\" height=\"40\">\n<img src=\"/static/img/Logo_TM.png\" alt=\"DOI_disc_logo\" width=\"24\" height=\"24\" />\n</td>\n\n<td align=\"left\">\n<span style=\"padding-left: 0px; font-size: 11px;\"><span style=\"vertical-align: super;\">&reg;</span>, DOI<span style=\"vertical-align: super;\">&reg;</span>, DOI.ORG<span style=\"vertical-align: super;\">&reg;</span>, and shortDOI<span style=\"vertical-align: super;\">&reg;</span> are trademarks of the International DOI Foundation.</span>\n</td>       \n</tr>\n</table>\n</body>\n</html>\n","authorsSemantic":[6]},{"id":355,"title":"The shadow algorithm: a scheduling technique for both compiled and interpreted simulation","doi":"10.1109/43.240088","description":"The shadow algorithm, which is an event-driven unit-delay simulation technique that has been designed to take advantage of the instruction caches present in many of the latest workstations, is discussed. The algorithm is based on the threaded-code technique, but uses a dynamically created linked list of environments called shadows. Compiled shadow algorithm simulations run in about 1/5th the time required for a conventional interpreted event-driven simulation. The interpreted shadow algorithm runs in about 1/4th the time of a conventional interpretive simulation. >","venue":"IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.","listofauthors":"P. Maurer","citations":19,"year":1993,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"1411-1413","volume":"12","number":"9","bibtex":"@article{1993,\n\tdoi = {10.1109/43.240088},\n\turl = {https://doi.org/10.1109%2F43.240088},\n\tyear = 1993,\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {12},\n\tnumber = {9},\n\tpages = {1411--1413},\n\tauthor = {P.M. Maurer},\n\ttitle = {The shadow algorithm: a scheduling technique for both compiled and interpreted simulation}\n}","authorsSemantic":[6]},{"id":356,"title":"Parallel multi-delay simulation","doi":"10.1109/ICCAD.1993.580174","description":"The multi-delay parallel (MDP) technique is a multi-delay logic simulation algorithm that uses no event-sorting mechanism. Wide bit-fields and bit-parallel operations are used to resolve out-of-order events. Although the MDP technique was designed to be implemented in hardware, the software version has proven to be competitive with ordinary multi-delay simulation. Two versions of the MDP technique are presented: fixed alignment and variable alignment. The fixed alignment algorithm uses bit-fields that are wide enough to capture any event that could occur during the simulation, while the variable alignment algorithm uses a minimum-width bit field.","venue":"Proceedings of 1993 International Conference on Computer Aided Design (ICCAD)","listofauthors":"Y. Lee, P. Maurer","citations":1,"year":0,"publisher":"IEEE Comput. Soc. Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/iccad.1993.580174},\n\turl = {https://doi.org/10.1109%2Ficcad.1993.580174},\n\tpublisher = {{IEEE} Comput. Soc. Press},\n\tauthor = {Y.S. Lee and P.M. Maurer},\n\ttitle = {Parallel multi-delay simulation}\n}","authorsSemantic":[6]},{"id":357,"title":"Two new techniques for unit-delay compiled simulation","doi":"10.1109/43.159998","description":"The potential change (PC)-set method and the parallel technique for generating compiled unit-delay simulators for acrylic circuits are discussed. The PC-set method analyzes the network, determines the set of potential change times for each net, and generates gate simulations for each potential change. The parallel technique, which is based on the concept of bit-parallel simulation is faster and generates less code than the PC-set method, but it is not amenable to data-parallel simulation of multiple input vectors. Both techniques are based on the well-known levelization algorithm used to generate zero-delay levelized compiled code simulation. Two optimizations of the basic parallel technique are presented, called bit-field trimming and shift elimination. Performance results using the ISCAS 85 benchmarks show a factor-of-four improvement for the PC-set method and a factor-of-ten improvement for the parallel technique. The optimization schemes show an average performance improvement of 47% over the unoptimized simulations. >","venue":"IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.","listofauthors":"P. Maurer","citations":28,"year":1992,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"1120-1130","volume":"11","number":"9","bibtex":"@article{1992,\n\tdoi = {10.1109/43.159998},\n\turl = {https://doi.org/10.1109%2F43.159998},\n\tyear = 1992,\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {11},\n\tnumber = {9},\n\tpages = {1120--1130},\n\tauthor = {P.M. Maurer},\n\ttitle = {Two new techniques for unit-delay compiled simulation}\n}","authorsSemantic":[6]},{"id":358,"title":"Two new techniques for compiled multi-delay logic simulation","doi":"10.1109/DAC.1992.227767","description":"The authors describe two techniques for compiled event driven multidelay logic simulation that provide significant performance improvements over interpreted multidelay logic simulation. These two techniques are based on the concept of retargetable branch instructions that can be used to switch segments of code into and out of the instruction stream. The second algorithm, called the shadow technique, has been designed especially for systems with instruction caches. Benchmark experiments showed that these two techniques were up to 15 times faster than the interpreted multidelay simulator, with an average improvement of about five times for the fastest method.<<ETX>>","venue":"[1992] Proceedings 29th ACM/IEEE Design Automation Conference","listofauthors":"Y. Lee, P. Maurer","citations":18,"year":0,"publisher":"IEEE Comput. Soc. Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/dac.1992.227767},\n\turl = {https://doi.org/10.1109%2Fdac.1992.227767},\n\tpublisher = {{IEEE} Comput. Soc. Press},\n\tauthor = {Y.S. Lee and P.M. Maurer},\n\ttitle = {Two new techniques for compiled multi-delay logic simulation}\n}","authorsSemantic":[6]},{"id":359,"title":"The design and implementation of a grammar‐based data generator","doi":"10.1002/spe.4380220303","description":"DGL is a context‐free grammar‐based language for generating test data. Although many of the features of DGL are implemented in a straighforward way, the implementation of several of the most important features is neither trivial nor obvious. Before one can understand the implementation of these features, however, it is necessary to understand the overall structure of the compiler and its output, which was designed to be flexible enough to incorporate new features easily. Variables and chains are two of the most important features of DGL, and also two of the trickiest features to implement. The run‐time dictionaires, which are built into the C code generated by the compiler, are implemented as pure code rather than as table‐look‐up routines. The compiler itself is reasonably straightforward, except for the expansion of character sets and compile‐time macros. These two features can cause the ‘multi‐dimensional’ expansion of a string, the implementation of which must be carefully designed.","venue":"Softw. Pract. Exp.","listofauthors":"P. Maurer","citations":40,"year":1992,"publisher":"Wiley","pages":"223-244","volume":"22","number":"3","bibtex":"@article{1992,\n\tdoi = {10.1002/spe.4380220303},\n\turl = {https://doi.org/10.1002%2Fspe.4380220303},\n\tyear = 1992,\n\tmonth = {mar},\n\tpublisher = {Wiley},\n\tvolume = {22},\n\tnumber = {3},\n\tpages = {223--244},\n\tauthor = {Peter M. Maurer},\n\ttitle = {The design and implementation of a grammar-based data generator}\n}","authorsSemantic":[6]},{"id":360,"title":"Compiled unit-delay simulation for cyclic circuits","doi":"10.1109/SECON.1992.202332","description":"Three techniques for handling cyclic circuits in a compiled unit-delay simulation are presented. These techniques are based on the PC-set method and the parallel technique of compiled unit-delay simulation. The first technique, called the synchronous parallel technique, is applicable only to synchronous circuits, but provides significant performance improvements over interpreted unit-delay simulation. The second and third techniques. called the convergence algorithm and the asynchronous parallel technique, are applicable to all circuits, both synchronous and asynchronous. The convergence algorithm, which is based on the PC-set method, provided significant performance increases for some circuits, but performed poorly on others. The asynchronous parallel technique performed rather poorly, and is covered only briefly.<<ETX>>","venue":"Proceedings IEEE Southeastcon '92","listofauthors":"P. Maurer, Y.-S. Lee","citations":1,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/secon.1992.202332},\n\turl = {https://doi.org/10.1109%2Fsecon.1992.202332},\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer and Y.S. Lee},\n\ttitle = {Compiled unit-delay simulation for cyclic circuits}\n}","authorsSemantic":[6]},{"id":361,"title":"Two new techniques for compiled multi-delay simulation","doi":"10.1109/SECON.1992.202330","description":"Two techniques for compiled multidelay simulation are presented. One is event-driven and the other is based on the concept of levelized compiled simulation. Experimental results are presented which show a significant performance improvement for compiled event-driven simulation over interpreted event-driven simulation, although this improvement is somewhat less than would normally be expected. An analysis of both the compiled and interpretive simulators that supports the experimental data is presented. The effects of caching and locality of reference are presented for the compiled event-driven simulator. The performance enhancements for the non-event-driven technique are substantial, but this technique has the disadvantage of generating an enormous amount of code for some circuits. Suggestions for future research are also presented.<<ETX>>","venue":"Proceedings IEEE Southeastcon '92","listofauthors":"Y.-S. Lee, P. Maurer","citations":16,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/secon.1992.202330},\n\turl = {https://doi.org/10.1109%2Fsecon.1992.202330},\n\tpublisher = {{IEEE}},\n\tauthor = {Y.-S. Lee and P.M. Maurer},\n\ttitle = {Two new techniques for compiled multi-delay simulation}\n}","authorsSemantic":[6]},{"id":362,"title":"Techniques for unit-delay compiled simulation","doi":"10.1145/123186.123346","description":"The PC-set method and the parallel technique are two methods for performing compiled unit-delay simulation. The PC-set method analyzes the network, determines the set of potential change times for each net, and generates gate simulations for each potential change. The parallel technique, which is based on the concept of parallel fault simulation, is faster and generates less code than the PC-method, but is less flexible. Benchmark comparisons with interpreted event-driven simulation show a factor of four improvement for the PC-set method and a factor of ten improvement for the parallel technique.","venue":"DAC '90","listofauthors":"P. Maurer, Zhicheng Wang","citations":32,"year":1990,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1990,\n\tdoi = {10.1145/123186.123346},\n\turl = {https://doi.org/10.1145%2F123186.123346},\n\tyear = 1990,\n\tpublisher = {{ACM} Press},\n\tauthor = {Peter M. Maurer and Zhicheng Wang},\n\ttitle = {Techniques for unit-delay compiled simulation}\n}","authorsSemantic":[6]},{"id":363,"title":"Scheduling blocks of hierarchical compiled simulation of combinational circuits","doi":"10.1109/43.68405","description":"Several algorithms for scheduling high-level functional blocks that assume the blocks may not be physically divided and that pseudocycles may be present due to the grouping of elements within blocks are presented. These algorithms rely on dependency information derived from the block-definitions to create a logical (rather than physical) partitioning of the circuit. The partitioned network is scheduled by an algorithm that is based on levelized scheduling with various heuristics added. The choice of the algorithm is based on the randomness of the partitioning technique. For highly random partitioning, PCSF performs best, while for partitioning that conforms closely to the signal flow of the circuit, FIQ performs best. For circuits in between, MEO is shown to perform best. These techniques represent an improvement over the technique of simulating blocks in random order, since no block will be scheduled unless there is a potential for performing useful work by simulating the block. The problem of finding the minimal schedule is shown to be NP-complete, hence, a heuristic approach to the problem is justified. A technique for computing the dependencies of a block is also presented. >","venue":"IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.","listofauthors":"P. Maurer","citations":3,"year":1991,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"184-192","volume":"10","number":"2","bibtex":"@article{1991,\n\tdoi = {10.1109/43.68405},\n\turl = {https://doi.org/10.1109%2F43.68405},\n\tyear = 1991,\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {10},\n\tnumber = {2},\n\tpages = {184--192},\n\tauthor = {P.M. Maurer},\n\ttitle = {Scheduling blocks of hierarchical compiled simulation of combinational circuits}\n}","authorsSemantic":[6]},{"id":364,"title":"LECSIM: a levelized event driven compiled logic simulation","doi":"10.1145/123186.123349","description":"LECSIM is a highly efficient logic simulator which integrates the advantages of event driven interpretive simulation and levelized compiled simulation. Two techniques contribute to the high efficiency. First it employs the zero-delay simulation model with levelized event scheduling to eliminate most unnecessary evaluations. Second, it compiles the central event scheduler into simple local scheduling segments which reduces the overhead of event scheduling. Experimental results show that LECSIM runs about 8-77 time faster than traditional unit-delay event-driven interpretive simulator. LECSIM also provides the option of scheduling with respect to individual gates or with respect to fan-out free blocks. When the circuit is partitioned into fan-out free blocks, the speed increases by a factor of 2-3. With partitioning, the speed of LECSIM is only about 1.5-3.4 times slower than a levelized compiled simulation for the combinational circuits we have tested.","venue":"DAC '90","listofauthors":"Zhicheng Wang, P. Maurer","citations":71,"year":1990,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1990,\n\tdoi = {10.1145/123186.123349},\n\turl = {https://doi.org/10.1145%2F123186.123349},\n\tyear = 1990,\n\tpublisher = {{ACM} Press},\n\tauthor = {Zhicheng Wang and Peter M. Maurer},\n\ttitle = {{LECSIM}}\n}","authorsSemantic":[6]},{"id":365,"title":"HDL driven chip layout within the FHDL design framework","doi":"10.1109/SECON.1990.117851","description":"Techniques of automatically generating layout from Florida Hardware Design Language (FHDL) specifications are presented. These techniques allow for the automated layout of read-only memories (ROMs) and programmable logic arrays (PLAs), and they allow for the user-assisted automatic layout of standard-cell blocks. Adaptations of the FHDL and its framework to permit layout synthesis are presented. Cell generation is discussed. Adapting the simulation framework and primitive simulation modelling are discussed.<<ETX>>","venue":"IEEE Proceedings on Southeastcon","listofauthors":"Craig D. Morency, P. Maurer, Z. Wang","citations":2,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/secon.1990.117851},\n\turl = {https://doi.org/10.1109%2Fsecon.1990.117851},\n\tpublisher = {{IEEE}},\n\tauthor = {C.D. Morency and P.M. Maurer and Z. Wang},\n\ttitle = {{HDL} driven chip layout within the {FHDL} design framework}\n}","authorsSemantic":[6]},{"id":366,"title":"LECSIM: a levelized event-driven compiled logic simulator","doi":"10.1109/DAC.1990.114905","description":"LECSIM is an efficient logic simulator which integrates the advantages of event-drive interpretive simulation and levelized compiled simulation. Two techniques contribute to the high efficiency. First, it employs the zero-delay simulation model with levelized event scheduling to eliminate most unnecessary evaluations. Second, it compiles the central event scheduler into simple local scheduling segments which reduces the overhead of event-scheduling. Experimental results show that LECSIM runs about 8-77 times faster than traditional unit-delay event-driven interpretive simulator. LECSIM also provides the option of scheduling with respect to individual gates or to fan-out free blocks. When the circuit is partitioned into fan-out free blocks, the speed increases by a factor of 2-3. With partitioning, the speed of LECSIM is only about 1.5-3.4 times slower than a levelized compiled simulation for the combinational circuits tested.<<ETX>>","venue":"27th ACM/IEEE Design Automation Conference","listofauthors":"Z. Wang, P. Maurer","citations":44,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/dac.1990.114905},\n\turl = {https://doi.org/10.1109%2Fdac.1990.114905},\n\tpublisher = {{IEEE}},\n\tauthor = {Z. Wang and P.M. Maurer},\n\ttitle = {{LECSIM}: a levelized event-driven compiled logic simulator}\n}","authorsSemantic":[6]},{"id":367,"title":"Dynamic functional testing for VLSI circuits","doi":"10.1109/54.64956","description":"The author discusses the two main problems of dynamic testing (i.e. testing while the simulator is running), namely the design of a high-level vector-generation language and the design of the interface between the vector generator and the simulator. He offers guidelines for designing a high-level vector-generation language as well as several examples written in FHDL, a driver language developed at the University of South Florida. The author also describes a solution to interface design that is based on a special interface data structure that supports several styles of vector generators and interactive circuit debugging.<<ETX>>","venue":"IEEE Design & Test of Computers","listofauthors":"P. Maurer","citations":2,"year":1990,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"42-49","volume":"7","number":"6","bibtex":"@article{1990,\n\tdoi = {10.1109/54.64956},\n\turl = {https://doi.org/10.1109%2F54.64956},\n\tyear = 1990,\n\tmonth = {dec},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {7},\n\tnumber = {6},\n\tpages = {42--49},\n\tauthor = {P.M. Maurer},\n\ttitle = {Dynamic functional testing for {VLSI} circuits}\n}","authorsSemantic":[6]},{"id":368,"title":"Techniques for unit-delay compiled simulation","doi":"10.1109/DAC.1990.114903","description":"Two techniques for compiled unit-delay simulation have been presented. These are a PC-set (the set of potential change times) method and a parallel technique. The PC-set method analyzes a network, determines a set of potential change times for each net, and generates gate simulations for each potential change. The parallel technique, which is based on a concept of parallel fault simulation, is faster and generates less code than the PC-method, but it is less flexible. Benchmark comparisons with an interpreted event-driven simulation show a factor of four improvement for the PC-set method and a factor of ten improvement for the parallel technique.<<ETX>>","venue":"27th ACM/IEEE Design Automation Conference","listofauthors":"P. Maurer, Z. Wang","citations":10,"year":1990,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1990,\n\tdoi = {10.1145/123186.123346},\n\turl = {https://doi.org/10.1145%2F123186.123346},\n\tyear = 1990,\n\tpublisher = {{ACM} Press},\n\tauthor = {Peter M. Maurer and Zhicheng Wang},\n\ttitle = {Techniques for unit-delay compiled simulation}\n}","authorsSemantic":[6]},{"id":369,"title":"The FHDL PLA tools","doi":"10.1145/98949.98955","description":"The FHDL (Florida Hardware Design Language) PLA tools provide a means for specifying, simulating, and automatically laying out Programmed Logic Arrays (PLAs). These tools were created to facilitate VLSI design projects, to improve the quality of hardware design courses, and to serve as a basis for future research in VLSI design automation. At the specification level, the PLA tools allow the contents of a PLA to be specified as a set of logic equations. In addition, they provide features for facilitating the construction of PLA-based state machines. Once a PLA has been specified, it can be simulated at a high level in coordination with the simulation of the other portions of a VLSI design. After a PLA has been verified through simulation, it can be laid out automatically through an interface to the Berkeley PLA layout tools. The primary motivation for developing these tools was to provide a basis for future research in VLSI design automation.","venue":"ACM-SE 28","listofauthors":"P. Maurer, Craig D. Morency","citations":0,"year":1990,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1990,\n\tdoi = {10.1145/98949.98955},\n\turl = {https://doi.org/10.1145%2F98949.98955},\n\tyear = 1990,\n\tpublisher = {{ACM} Press},\n\tauthor = {Peter M. Maurer and Craig D. Morency},\n\ttitle = {The {FHDL} {PLA} tools}\n}","authorsSemantic":[6]},{"id":370,"title":"Optimization of the parallel technique for compiled unit-delay simulation","doi":"10.1109/ICCAD.1990.129843","description":"The parallel technique is a purely compiled method for unit-delay simulation that is based on levelized compiled simulation and bit parallel simulation. The parallel technique provides rapid simulations with a reasonable amount of code, but there are opportunities for optimization. The author presents two schemes: bit-field trimming and shift-elimination. Performance results are presented that demonstrate an average performance improvement of 47%.<<ETX>>","venue":"1990 IEEE International Conference on Computer-Aided Design. Digest of Technical Papers","listofauthors":"P. Maurer","citations":8,"year":0,"publisher":"IEEE Comput. Soc. Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/iccad.1990.129843},\n\turl = {https://doi.org/10.1109%2Ficcad.1990.129843},\n\tpublisher = {{IEEE} Comput. Soc. Press},\n\tauthor = {P.M. Maurer},\n\ttitle = {Optimization of the parallel technique for compiled unit-delay simulation}\n}","authorsSemantic":[6]},{"id":371,"title":"Generating test data with enhanced context-free grammars","doi":"10.1109/52.56422","description":"The use of context-free grammars to improve functional testing of very-large-scale integrated circuits is described. It is shown that enhanced context-free grammars are effective tools for generating test data. The discussion covers preliminary considerations, the first tests, generating systematic tests, and testing subroutines. The author's experience using context-free grammars to generate tests for VLSI circuit simulators indicates that they are remarkably effective tools that virtually anyone can use to debug virtually any program.<<ETX>>","venue":"IEEE Software","listofauthors":"P. Maurer","citations":181,"year":1990,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"50-55","volume":"7","number":"4","bibtex":"@article{1990,\n\tdoi = {10.1109/52.56422},\n\turl = {https://doi.org/10.1109%2F52.56422},\n\tyear = 1990,\n\tmonth = {jul},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {7},\n\tnumber = {4},\n\tpages = {50--55},\n\tauthor = {P.M. Maurer},\n\ttitle = {Generating test data with enhanced context-free grammars}\n}","authorsSemantic":[6]},{"id":372,"title":"Standard cell floorplanning within the FHDL automatic placement tool","doi":"10.1109/SECON.1990.117850","description":"Techniques which are being developed for the floorplanning of layouts in the Florida Hardware Design Language (FHDL) layout tool are present. These techniques show a different approach to the floorplanning of those layouts which are to contain a mixture of standard cells and macrocells. General cell placement and floorplanning are discussed. Standard cell collection, block generation, and placement optimization are discussed.<<ETX>>","venue":"IEEE Proceedings on Southeastcon","listofauthors":"Craig D. Morency, P. Maurer","citations":0,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/secon.1990.117850},\n\turl = {https://doi.org/10.1109%2Fsecon.1990.117850},\n\tpublisher = {{IEEE}},\n\tauthor = {C.D. Morency and P.M. Maurer},\n\ttitle = {Standard cell floorplanning within the {FHDL} automatic placement tool}\n}","authorsSemantic":[6]},{"id":373,"title":"The Florida Hardware Design Language","doi":"10.1109/SECON.1990.117849","description":"The Florida Hardware Design Language (FHDL), an expandable language that provides a uniform method for specifying all portions of an integrated circuit's design, is discussed. FHDL was designed to support the development and testing of wafer-scale integrated circuits, especially at the functional level, but can be used to design other integrated circuits at many levels. FHDL is not just a design language, but a framework for developing a wide range of computer-aided design tools. The syntactic structure, extensibility, and automatic layout are discussed.<<ETX>>","venue":"IEEE Proceedings on Southeastcon","listofauthors":"P. Maurer","citations":13,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/secon.1990.117849},\n\turl = {https://doi.org/10.1109%2Fsecon.1990.117849},\n\tpublisher = {{IEEE}},\n\tauthor = {P.M. Maurer},\n\ttitle = {The Florida Hardware Design Language}\n}","authorsSemantic":[6]},{"id":374,"title":"The FHDL macro processor","doi":"10.1145/98949.98958","description":"The FHDL (Florida Hardware Design Language) Macro processor provides a mechanism for extending the language features provided by the other components of the FHDL system (the ROM language, the PLA language, and the logic specification language). The primary use of the Macro processor is to provide flexible cells, such as ripple-carry adders, that can expand to match the size of the interface. The use of the Macro processor for this purpose is transparent with more standard hierarchical specification mechanisms. In addition, the Macro processor was designed to be an implementation vehicle for more sophisticated hardware specification and synthesis systems. The Macro processor provides most of the features found in other macro languages, and provides several new features that are found in few, if any, existing macro languages. The use of the Macro processor for high-level synthesis is the subject of much on-going research. THE FHDL MACRO PROCESSOR Peter M. Maurer Department of Computer Science and Engineering University of South Florida Tampa, FL 33620","venue":"ACM-SE 28","listofauthors":"P. Maurer","citations":0,"year":1990,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1990,\n\tdoi = {10.1145/98949.98958},\n\turl = {https://doi.org/10.1145%2F98949.98958},\n\tyear = 1990,\n\tpublisher = {{ACM} Press},\n\tauthor = {Peter M. Maurer},\n\ttitle = {The {FHDL} macro processor}\n}","authorsSemantic":[6]},{"id":375,"title":"The FHDL ROM tools","doi":"10.1145/98949.98960","description":"The FHDL (Florida Hardware Design Language) ROM tools provide a method for specifying, simulating, and automatically laying out ROMs. The primary focus of the ROM tools is on providing powerful methods for specifying microcode. Because the ROM tools were designed to support both VLSI design projects and other course work in hardware design, the ROM language contains many features that allow it to emulate other ROM programming languages. This allows students to complete laboratory exercises using a language that is similar to the one used in their textbook. Once the contents of a ROM have been specified, the ROM can be simulated concurrently with the simulation of the other hardware comprising the design. This allows designs to be debugged before they are fabricated. Once a design has been verified, the ROM can be laid out automatically and incorporated into a larger VLSI circuit. The automatic layout portion of the FHDL ROM tools is the subject of on-going research at the University of South Florida.","venue":"ACM-SE 28","listofauthors":"P. Maurer, Craig D. Morency","citations":0,"year":1990,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1990,\n\tdoi = {10.1145/98949.98960},\n\turl = {https://doi.org/10.1145%2F98949.98960},\n\tyear = 1990,\n\tpublisher = {{ACM} Press},\n\tauthor = {Peter M. Maurer and Craig D. Morency},\n\ttitle = {The {FHDL} {ROM} tools}\n}","authorsSemantic":[6]},{"id":379,"title":"Design verification of the WE 32106 math accelerator unit","doi":"10.1109/54.7959","description":"An overview is given of the MAU, an IEEE-compatible floating-point accelerator that operates as a coprocessor for the WE 32100 CPU. The chip provides virtually all the features that the IEEE-754 floating-point standard requires, with added software that provides a fully conforming system. A description is then given of the approach used for its design verification, which resulted in a unit that has exhibited no bugs, even two years after its first silicon implementation. Designers used two sets of tools during verification. With the first set, they reduced the time needed to create and execute tests and simplified the development of system tests. With the second set, they created a random test system, which plugged the holes left by system tests. Work underway to apply the techniques to other chips is described.","venue":"IEEE Design & Test of Computers","listofauthors":"P. Maurer","citations":14,"year":1988,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"11-21","volume":"5","number":"3","bibtex":"@article{1988,\n\tdoi = {10.1109/54.7959},\n\turl = {https://doi.org/10.1109%2F54.7959},\n\tyear = 1988,\n\tmonth = {jun},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {5},\n\tnumber = {3},\n\tpages = {11--21},\n\tauthor = {P.M. Maurer},\n\ttitle = {Design verification of the {WE} 32106 math accelerator unit}\n}","authorsSemantic":[6]},{"id":380,"title":"Mapping the Data Flow Model of Computation into an Enhanced Von Neumann Processor","doi":null,"description":"The SAM architecture is an enhanced von Neumann processor that contains inexpensive features for supporting data flow style of parallelism. The architecture gets is name from the basic instructions for supporting parallelism, Split and Merge. It is shown that these instructions can be used to implement the parallel structure of an arbitrary acyclic data flow graph. Features for supporting dynamic parallelism and multiple run-time environments are presented. Implementation issues for supporting instruction execution and the handling of faults and interrupts ar also discussed.","venue":"ICPP","listofauthors":"P. Maurer","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[6]},{"id":381,"title":"A logic-to-logic comparator for VLSI layout verification","doi":"10.1109/43.3221","description":"The logic-to-logic comparator (LLC) is described as a tool that verifies VLSI layouts by comparing the logic diagrams for a circuit with structures extracted from the circuit's layout. LLC can operate at either the gate or the transistor level, although the gate level is preferred. It is similar to other graph-isomorphism-based tools, but incorporates some important improvements. The most important of these is a path-comparison algorithm that uses dynamically calculated global information. In addition, LLC contains a gate-matching algorithm that can use certain types of symmetry to distinguish between gates that appear to be identical to less sophisticated algorithms. LLC has been used to verify several commercially available VLSI chips. >","venue":"IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.","listofauthors":"P. Maurer, Alexander D. Schapira","citations":8,"year":1988,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"897-907","volume":"7","number":"8","bibtex":"@article{1988,\n\tdoi = {10.1109/43.3221},\n\turl = {https://doi.org/10.1109%2F43.3221},\n\tyear = 1988,\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {7},\n\tnumber = {8},\n\tpages = {897--907},\n\tauthor = {P.M. Maurer and A.D. Schapira},\n\ttitle = {A logic-to-logic comparator for {VLSI} layout verification}\n}","authorsSemantic":[6]},{"id":382,"title":"A rose is a rose","doi":"10.2307/2322215","description":"PETER M. MAURER received his B.A. in mathematics in 1969 from St. Benedict's College in Atchison, Kansas. After three years of writing computer programs for the U.S. Army and five more years writing programs for the state government of Iowa, he returned to school at Iowa State University. He received his M.S. in 1979 and his Ph.D. in 1982, both in Computer Science. Since 1982 he has been employed by AT & T Bell Laboratories and x AT & T Information Systems. His main professional interest is using mathematics to solve problems in the design of microprocessors.","venue":"","listofauthors":"P. Maurer","citations":1,"year":1987,"publisher":"JSTOR","pages":"631","volume":"94","number":"7","bibtex":"@article{1987,\n\tdoi = {10.2307/2322215},\n\turl = {https://doi.org/10.2307%2F2322215},\n\tyear = 1987,\n\tmonth = {aug},\n\tpublisher = {{JSTOR}},\n\tvolume = {94},\n\tnumber = {7},\n\tpages = {631},\n\tauthor = {Peter M. Maurer},\n\ttitle = {A Rose is a Rose...}\n}","authorsSemantic":[6]},{"id":383,"title":"An IEEE standard floating point chip","doi":"10.1109/ISSCC.1985.1156748","description":"This paper will report on a 14MHz IEEE standard floating point coprocessor for a 32b microprocessor. Implemented in 1.75μ twin-tub CMOS, the chip supports single, double and extended double precision floating point, as well as 32b integer and 18 digit BCD operations.","venue":"1985 IEEE International Solid-State Circuits Conference. Digest of Technical Papers","listofauthors":"A. Komal, K. Goskel, P. Diodato, J. Fields, U. Gumaste, Chaw Kung, Kingyao Lin, M. Lega, P. Maurer, T. Ng, Yaw Oh, M. Thierbach","citations":7,"year":0,"publisher":"Institute of Electrical and Electronics Engineers","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/isscc.1985.1156748},\n\turl = {https://doi.org/10.1109%2Fisscc.1985.1156748},\n\tpublisher = {Institute of Electrical and Electronics Engineers},\n\tauthor = {A. Komal and K. Goskel and P. Diodato and J. Fields and U. Gumaste and  Chaw Kung and  Kingyao Lin and M. Lega and P. Maurer and  Thomas Ng and  Yaw Oh and M. Thierbach},\n\ttitle = {An {IEEE} standard floating point chip}\n}","authorsSemantic":[6]},{"id":384,"title":"The Use of Combinators in Translating A Purely Functional Language to Low-Level Data-Flow Graphs","doi":"10.1016/0096-0551(83)90004-8","description":"Abstract FCL is a higher-order functional programming language which consolidates and extends a number of desirable features of existing languages. This paper describes the salient features of FCL and an algorithm for translation to highly parallel data flow graphs. The translation algorithm is based on a set of extended “combinators”. The relationship between functional programming languages and demand-driven or data-driven data flow architectures is established.","venue":"Comput. Lang.","listofauthors":"P. Maurer, A. Oldehoeft","citations":5,"year":1983,"publisher":"Elsevier BV","pages":"27-45","volume":"8","number":"1","bibtex":"@article{1983,\n\tdoi = {10.1016/0096-0551(83)90004-8},\n\turl = {https://doi.org/10.1016%2F0096-0551%2883%2990004-8},\n\tyear = 1983,\n\tmonth = {jan},\n\tpublisher = {Elsevier {BV}},\n\tvolume = {8},\n\tnumber = {1},\n\tpages = {27--45},\n\tauthor = {Peter M. Maurer and Arthur E. Oldehoeft},\n\ttitle = {The use of combinators in translating a purely functional language to low-level data-flow graphs}\n}","authorsSemantic":[6]},{"id":385,"title":"FCL: a purely functional language for data-flow programming","doi":"10.31274/RTD-180813-5039","description":"null","venue":"","listofauthors":"P. Maurer","citations":1,"year":0,"publisher":"Iowa State University","pages":null,"volume":null,"number":null,"bibtex":"@phdthesis{1,\n\tdoi = {10.31274/rtd-180815-5039},\n\turl = {https://doi.org/10.31274%2Frtd-180815-5039},\n\tpublisher = {Iowa State University},\n\tauthor = {Peter Michael Maurer},\n\ttitle = {{FCL}: a purely functional language for data-flow programming}\n}","authorsSemantic":[6]},{"id":420,"title":"Deployment and Hyper-Parameter Optimization of Chatbots","doi":null,"description":"Chatbots are a specific application of a set of machine learning algorithms belonging to the family of natural language processing (NLP). Recently, NLP algorithms have gained attention as we are closer to passing the Turing test when they are applied to human-computer interaction-based systems. In this thesis project we will model chatbots using NLP-based machine learning algorithms based on datasets of people. Based on sentences and text from a specific person, we measure how well the chatbot models such person’s writing. In theory, NLP algorithms of the Long Short Term Memory (LSTM) type are capable of remembering, summarizing, and learning patterns of speech, style, and forms of any sequences of text. Results indicate that an LSTMs is capable of generating novel sentences using as a case study Donald Trump’s tweets.","venue":"","listofauthors":"M. Read, Pablo Rivas","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":386,"title":"3D Positional Movement Interaction with User-Defined, Virtual Interface for Music Software: MoveMIDI","doi":"10.1145/3290607.3312954","description":"This paper describes progress made in design and development of a new digital musical instrument (MIDI controller), MoveMIDI, and highlights its unique 3D positional movement interaction design differing from recent orientational and gestural approaches. A user constructs and interacts with MoveMIDI's virtual, 3D interface using handheld position-tracked controllers to control music software, as well as non-musical technology such as stage lighting. MoveMIDI's virtual interface contributes to solving problems difficult to solve with hardware MIDI controller interfaces such as customized positioning and instantiation of interface elements, and accurate, simultaneous control of independent parameters. MoveMIDI's positional interaction mirrors interaction with some physical acoustic instruments and provides visualization for an audience. Beta testers of MoveMIDI have created emergent use cases for the instrument.","venue":"CHI Extended Abstracts","listofauthors":"Timothy Arterbury, G. M. Poor","citations":1,"year":2019,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1145/3290607.3312954},\n\turl = {https://doi.org/10.1145%2F3290607.3312954},\n\tyear = 2019,\n\tmonth = {may},\n\tpublisher = {{ACM}},\n\tauthor = {Timothy Arterbury and G. Michael Poor},\n\ttitle = {3D Positional Movement Interaction with User-Defined, Virtual Interface for Music Software}\n}","authorsSemantic":[7]},{"id":387,"title":"Interaction can hurt - Exploring gesture-based interaction for users with Chronic Pain","doi":"10.1145/3357251.3357589","description":"Chronic Pain is a universal disorder affecting millions of people, influencing even the most basic decisions in their lives. With the computer becoming such an integral part of our society and the ever-expanding interaction paradigm, the need to explore potential computer interactions for people with Chronic Pain has only increased. In this paper we explore the used of gesture-based interaction as a medium with which these users can perform the base operations of computer interaction. We show that, for gestural pointing and selection, modeling users’ interaction space and multimodel interaction performed the best in terms of throughput.","venue":"SUI","listofauthors":"G. M. Poor, Alvin Jude","citations":0,"year":2019,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1145/3357251.3357589},\n\turl = {https://doi.org/10.1145%2F3357251.3357589},\n\tyear = 2019,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {G Michael Poor and Alvin Jude},\n\ttitle = {Interaction can hurt - Exploring gesture-based interaction for users with Chronic Pain}\n}","authorsSemantic":[7]},{"id":388,"title":"The Reality of Reality-Based Interaction","doi":"10.1145/3319617","description":"Frameworks such as Direct Manipulation or Instrumental Interaction have been an important force in HCI research. Evaluating the impact of frameworks can identify whether and how a framework was used, how it has evolved, and what trends have developed over time. However, studying the impact of such theoretical contributions requires consideration of various perspectives and level of impact. As a case study for investigating the impact of theoretical work in HCI, we present our evaluation of the impact of the Reality Based Interaction (RBI) framework, introduced by the authors in 2008. We provide our findings about the impact of the framework both on contemporary research, through content-based citation analysis, and in HCI education, through a survey we conducted on emerging interaction frameworks. The article contributes a comprehensive methodology for evaluating the impact of frameworks through our twofold approach: content-based citation analysis, including the design of a new citation typology; and a survey on the use of frameworks in education using a taxonomy of learning goals. We also consider the role of frameworks in HCI as well as the future of the RBI framework.","venue":"ACM Trans. Comput. Hum. Interact.","listofauthors":"A. Girouard, Orit Shaer, E. Solovey, G. M. Poor, R. Jacob","citations":5,"year":2019,"publisher":"Association for Computing Machinery (ACM)","pages":"1-35","volume":"26","number":"5","bibtex":"@article{2019,\n\tdoi = {10.1145/3319617},\n\turl = {https://doi.org/10.1145%2F3319617},\n\tyear = 2019,\n\tmonth = {sep},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {26},\n\tnumber = {5},\n\tpages = {1--35},\n\tauthor = {Audrey Girouard and Orit Shaer and Erin T. Solovey and G. Michael Poor and Robert J. K. Jacob},\n\ttitle = {The Reality of Reality-Based Interaction}\n}","authorsSemantic":[7]},{"id":389,"title":"3D Positional Movement Interaction with User-Defined, Virtual Interface for Music Software: MoveMIDI","doi":"10.1145/3290607.3313267","description":"This paper describes progress made in design and development of a new digital musical instrument (MIDI controller), MoveMIDI, and highlights its unique 3D positional movement interaction design differing from recent orientational and gestural approaches. A user constructs and interacts with MoveMIDI's virtual, 3D interface using handheld position-tracked controllers to control music software, as well as non-musical technology such as stage lighting. MoveMIDI's virtual interface contributes to solving problems difficult to solve with hardware MIDI controller interfaces such as customized positioning and instantiation of interface elements, and accurate, simultaneous control of independent parameters. MoveMIDI's positional interaction mirrors interaction with some physical acoustic instruments and provides visualization for an audience. Beta testers of MoveMIDI have created emergent use cases for the instrument.","venue":"CHI Extended Abstracts","listofauthors":"Timothy Arterbury, G. M. Poor","citations":0,"year":2019,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1145/3290607.3313267},\n\turl = {https://doi.org/10.1145%2F3290607.3313267},\n\tyear = 2019,\n\tmonth = {may},\n\tpublisher = {{ACM}},\n\tauthor = {Timothy Arterbury and G. Michael Poor},\n\ttitle = {3D Positional Movement Interaction with User-Defined, Virtual Interface for Music Software}\n}","authorsSemantic":[7]},{"id":390,"title":"Interactive 3D Objects, Projections, and Touchscreens","doi":"10.1145/3183654.3183669","description":"It is widely recognized that features of objects in a user interface can impact aspects of user experience, including visual perception and problem solving. The current study looked at two such issues: 3D object projection and interactivity. Both factors are known to separately influence perception and problem solving; this work connected them together. Utilizing a fixed touchscreen user interface, we used two projections of 3D interactive cubes: oblique and parallel. The task was the Cube Comparison Task (CCT), a task known to be sensitive to both projection of 3D objects and interactivity. We measured the impact of projection of the interactive 3D objects on behaviors relating to alignment of object and environmental axes, foreshortening, and mapping of user interface controls. We also collected general performance measures of response time and accuracy in the CCT. We found minimal impact of projection of the interactive 3D objects on foreshortening, mapping of controls or general performance. However, we found that in both projections of interactive 3D objects, alignment between the object and environmental axes had a significant effect, leading to anisotropies in initial patterns of cube rotations. These anisotropies differed by projection. The results suggested that the manipulation of perceptual factors such as projection of interactive 3D objects can have effects on interactive problem solving, though the nuance of such effects may be influenced by additional factors. In particular, the interactive nature of our touchscreen user interface may have mitigated some effects that were otherwise predicted from prior research.","venue":"APAScience","listofauthors":"Samuel D. Jaffee, L. Leventhal, J. Ringenberg, G. M. Poor","citations":0,"year":2018,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2018,\n\tdoi = {10.1145/3183654.3183669},\n\turl = {https://doi.org/10.1145%2F3183654.3183669},\n\tyear = 2018,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\tauthor = {Samuel D. Jaffee and Laura Marie Leventhal and Jordan Ringenberg and G. Michael Poor},\n\ttitle = {Interactive 3D Objects, Projections, and Touchscreens}\n}","authorsSemantic":[7]},{"id":421,"title":"Modeling Five Sentence Quality Representations by Finding Latent Spaces Produced with Deep Long Short-Memory Models","doi":null,"description":"We present a study in which we train neural models that approximate rules that assess the quality of English sentences. We modeled five rules using deep LSTMs trained over a dataset of sentences whose quality is evaluated under such rules. Preliminary results suggest the neural architecture can model such rules to high accuracy.","venue":"WNLP@ACL","listofauthors":"Pablo Rivas","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":391,"title":"Bimanual Word Gesture Keyboards for Mid-air Gestures","doi":"10.1145/3027063.3053137","description":"Mid-air hand gestural interaction has generally been researched as a pointing device. However, recent research has shown potential for text input with the use of word gesture keyboards (WGK), where these forms of interactions require the input system to identify when the gesture has started and when it has stopped. Previous research has had success where the same hand moved the cursor, and performed the activation gesture. In this paper we introduce bimanual interaction for gestural interaction to perform text input with WGK, where one hand moves the cursor while the other hand performs the activation. In our user studies, the bimanual method demonstrated significantly higher results than the state-of-the-art single handed method. We achieved 16 words per minute; about 39% higher than the benchmark, and with significantly lower error rates.","venue":"CHI Extended Abstracts","listofauthors":"Garrett Benoit, G. M. Poor, Alvin Jude","citations":2,"year":2017,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2017,\n\tdoi = {10.1145/3027063.3053137},\n\turl = {https://doi.org/10.1145%2F3027063.3053137},\n\tyear = 2017,\n\tmonth = {may},\n\tpublisher = {{ACM}},\n\tauthor = {Garrett Benoit and G. Michael Poor and Alvin Jude},\n\ttitle = {Bimanual Word Gesture Keyboards for Mid-air Gestures}\n}","authorsSemantic":[7]},{"id":392,"title":"Applying the Norman 1986 User-Centered Model to Post-WIMP UIs","doi":"10.1145/2983531","description":"In recent decades, “post-WIMP” interactions have revolutionized user interfaces (UIs) and led to improved user experiences. However, accounts of post-WIMP UIs typically do not provide theoretical explanations of why these UIs lead to superior performance. In this article, we use Norman’s 1986 model of interaction to describe how post-WIMP UIs enhance users’ mental representations of UI and task. In addition, we present an empirical study of three UIs; in the study, participants completed a standard three-dimensional object manipulation task. We found that the post-WIMP UI condition led to enhancements of mental representation of UI and task. We conclude that the Norman model is a good theoretical framework to study post-WIMP UIs. In addition, by studying post-WIMP UIs in the context of the Norman model, we conclude that mental representation of task may be influenced by the interaction itself; this supposition is an extension of the original Norman model.","venue":"ACM Trans. Comput. Hum. Interact.","listofauthors":"G. M. Poor, Samuel D. Jaffee, L. Leventhal, J. Ringenberg, D. Klopfer, Guy W. Zimmerman, B. A. Klein","citations":5,"year":2016,"publisher":"Association for Computing Machinery (ACM)","pages":"1-33","volume":"23","number":"5","bibtex":"@article{2016,\n\tdoi = {10.1145/2983531},\n\turl = {https://doi.org/10.1145%2F2983531},\n\tyear = 2016,\n\tmonth = {nov},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {23},\n\tnumber = {5},\n\tpages = {1--33},\n\tauthor = {G. Michael Poor and Samuel D. Jaffee and Laura Marie Leventhal and Jordan Ringenberg and Dale S. Klopfer and Guy Zimmerman and Brandi A. Klein},\n\ttitle = {Applying the Norman 1986 User-Centered Model to Post-{WIMP} {UIs}}\n}","authorsSemantic":[7]},{"id":393,"title":"Reporting and Visualizing Fitts's Law: Dataset, Tools and Methodologies","doi":"10.1145/2851581.2892364","description":"In this paper we compare methods of reporting and visualizing Fitts regressions. We show that reporting this metric using mean movement time per user over accuracy-adjusted Index of Difficulty (IDe) produces more descriptive visualization. This method displays variance, which is more useful in understanding the interfaces, than an aggregated means-of-means approach using Index of Difficulty. We demonstrate that there is little difference in slope and intercept between the two methods, but has the potential to uncover wider goodness-of-fit coefficients which could allow for better comparison across experiments. We propose the use of quantile regression to report central tendencies as a trend, rather than box plots. The tools released with this paper can be used with any pointing device evaluation done with the FittsStudy program. The dataset released with this paper contains almost 25,000 samples, which can be used in future research for reporting or visualizing Fitts regressions.","venue":"CHI Extended Abstracts","listofauthors":"Alvin Jude, Darren Guinness, G. M. Poor","citations":10,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/2851581.2892364},\n\turl = {https://doi.org/10.1145%2F2851581.2892364},\n\tyear = 2016,\n\tmonth = {may},\n\tpublisher = {{ACM}},\n\tauthor = {Alvin Jude and Darren Guinness and G. Michael Poor},\n\ttitle = {Reporting and Visualizing Fitts{\\textquotesingle}s Law}\n}","authorsSemantic":[7]},{"id":394,"title":"Correcting Exercise Form Using Body Tracking","doi":"10.1145/2851581.2892519","description":"In the past twenty years, there have been little to no advances in technology used for free weight exercises. However, with the advances of computer vision and the availability of technology such as the XBox Kinect, having computer assisted exercises are a definite possibility for gym experiences. In this paper we examine the possibility of using a real-time correcting tool for a user's form while performing a free weight exercise. The squat exercise was chosen because it is easily track-able due to its rigid and specific set of form specifications that allow it to be easily corrected. Through our pilot study we showed that a user could learn how to correctly perform an exercise and correct their form by using the feedback provided by our software.","venue":"CHI Extended Abstracts","listofauthors":"C. Conner, G. M. Poor","citations":21,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/2851581.2892519},\n\turl = {https://doi.org/10.1145%2F2851581.2892519},\n\tyear = 2016,\n\tmonth = {may},\n\tpublisher = {{ACM}},\n\tauthor = {Caleb Conner and Gene Michael Poor},\n\ttitle = {Correcting Exercise Form Using Body Tracking}\n}","authorsSemantic":[7]},{"id":395,"title":"Improving Gestural Interaction With Augmented Cursors","doi":"10.1145/2983310.2985765","description":"Gesture-based interaction has become more affordable and ubiquitous as an interaction style in recent years. One issue with gestural pointing is the lack of accuracy with smaller targets. In this paper we propose that the use of augmented cursors - which has been shown to improve small target acquisition with a standard mouse - also improves small target acquisition for gestural pointing. In our study we explored the use of Bubble Lens and Bubble Cursor as a means to improve acquisition of smaller targets, and compared it with interactions without them. Our study showed that both methods significantly improved target selection. As part of our study, we also identified the parameters in configuring Bubble Cursor for optimal results.","venue":"SUI","listofauthors":"A. Dover, G. M. Poor, Darren Guinness, Alvin Jude","citations":4,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/2983310.2985765},\n\turl = {https://doi.org/10.1145%2F2983310.2985765},\n\tyear = 2016,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Ashley Dover and G. Michael Poor and Darren Guinness and Alvin Jude},\n\ttitle = {Improving Gestural Interaction With Augmented Cursors}\n}","authorsSemantic":[7]},{"id":422,"title":"Deep Neural Network Representations for Song Audio Matching and Recommendation","doi":null,"description":"With the recent popularity of online music streaming services, companies are trying to get ahead of the competition by generating accurate song recommendations for users that are listening to their service to keep them on the service for longer. We examine the use of autoencoders to train models to learn which songs are similar. We found that the deep autoencoder with multiple hidden layers produced the best results at the cost of high processing time. Additionally, we discuss our design of a Node.js webpage that a user can interact with and receive a recommendation of an existing song that has been uploaded to our server.","venue":"","listofauthors":"Brandon Litwin, Pablo Rivas","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":397,"title":"Models for Rested Touchless Gestural Interaction","doi":"10.1145/2788940.2788948","description":"Touchless mid-air gestural interaction has gained mainstream attention with the emergence of off-the-shelf commodity devices such as the Leap Motion and the Xbox Kinect. One of the issues with this form of interaction is fatigue, a problem colloquially known as the ``Gorilla Arm Syndrome.' However, by allowing interaction from a rested position, whereby the elbow is rested on a surface, this problem can be limited in its effect. In this paper we evaluate 3 possible methods for performing touchless mid-air gestural interaction from a rested position: a basic rested interaction, a simple calibrated interaction which models palm positions onto a hyperplane, and a more complex calibration which models the arm's interaction space using the angles of the forearm as input. The results of this work found that the two modeled interactions conform to Fitts's law and also demonstrated that implementing a simple model can improve interaction by improving performance and accuracy.","venue":"SUI","listofauthors":"Darren Guinness, Alvin Jude, G. M. Poor, A. Dover","citations":21,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/2788940.2788948},\n\turl = {https://doi.org/10.1145%2F2788940.2788948},\n\tyear = 2015,\n\tmonth = {aug},\n\tpublisher = {{ACM}},\n\tauthor = {Darren Guinness and Alvin Jude and G. Michael Poor and Ashley Dover},\n\ttitle = {Models for Rested Touchless Gestural Interaction}\n}","authorsSemantic":[7]},{"id":398,"title":"Modeling Mid-air Gestures With Spherical Coordinates","doi":"10.1145/2788940.2794356","description":"Generally, touchless mid-air gestural interaction use some form of Cartesian coordinate system within the input space. Most implementations map the input space in 3-D and map it to the 2-D of the monitor for output. In our previous work we showed that modeling the interaction space produces better interaction [1]. In this study, we use the Myo armband to show that a modeled interaction space also benefits devices that use spherical coordinates.","venue":"SUI","listofauthors":"Darren Guinness, A. Seung, A. Dover, G. M. Poor, Alvin Jude","citations":1,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/2788940.2794356},\n\turl = {https://doi.org/10.1145%2F2788940.2794356},\n\tyear = 2015,\n\tmonth = {aug},\n\tpublisher = {{ACM}},\n\tauthor = {Darren Guinness and Andrew Seung and Ashley Dover and G. Michael Poor and Alvin Jude},\n\ttitle = {Modeling Mid-air Gestures With Spherical Coordinates}\n}","authorsSemantic":[7]},{"id":399,"title":"Gestures with speech for hand-impaired persons","doi":"10.1145/2661334.2661398","description":"Mid-air hand-gestural interaction generally causes a fatigue due to implementations that require the user to hold their arm out during this interaction. Recent research has discovered a new approach to reduce fatigue related to gestural interaction, by allowing users to rest their elbow on a surface, and calibrate their interaction space from this rested position[1]. Additionally, this approach reduced stress on the hand and wrist compared to the mouse, by shifting much of the load to the forearm and shoulder muscles. In this paper we evaluated gesture and speech multimodal interaction as a form of assistive interaction for those with hand impairments. Two participants with hand impairments were recruited to perform the evaluation. We collected qualitative and quantitative data, which showed promising results in using this method for assistive interaction.","venue":"ASSETS","listofauthors":"Darren Guinness, G. M. Poor, Alvin Jude","citations":4,"year":2014,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2014,\n\tdoi = {10.1145/2661334.2661398},\n\turl = {https://doi.org/10.1145%2F2661334.2661398},\n\tyear = 2014,\n\tpublisher = {{ACM} Press},\n\tauthor = {Darren Guinness and G. Michael Poor and Alvin Jude},\n\ttitle = {Gestures with speech for hand-impaired persons}\n}","authorsSemantic":[7]},{"id":400,"title":"Personal space: user defined gesture space for GUI interaction","doi":"10.1145/2559206.2581242","description":"Reality-Based Interaction (RBI) [14] theorizes that realistic user interactions(UIs)are effective because they exploit users' pre-existing knowledge about their bodies and objects in the world. Gesture based interaction allows users to relay information to a computer through body movement without physical contact with additional hardware such as a mouse or trackball. However, this interaction style requires the users to interact in a manner that is tailored for the system to recognize with very strict rules for bodily interaction, not toward a gesture space that is more natural for the user. In this paper we propose a natural method of gestural input through a user-defined 3-dimensional space. We conducted two pilot studies to assess the performance and usability of these augmented gestural pointing methods for cursor manipulation as compared to a standard mouse interaction as well as the current standard approach used in gestural input.","venue":"CHI Extended Abstracts","listofauthors":"Alvin Jude, G. M. Poor, Darren Guinness","citations":23,"year":2014,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2014,\n\tdoi = {10.1145/2559206.2581242},\n\turl = {https://doi.org/10.1145%2F2559206.2581242},\n\tyear = 2014,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\tauthor = {Alvin Jude and G. Michael Poor and Darren Guinness},\n\ttitle = {Personal space}\n}","authorsSemantic":[7]},{"id":401,"title":"An evaluation of touchless hand gestural interaction for pointing tasks with preferred and non-preferred hands","doi":"10.1145/2639189.2641207","description":"Performance evaluations of touchless gestural interaction are generally done by benchmarking pointing performance against existing interactive devices, requiring the use of user's preferred hand. However, as there is no reason for this interaction to be limited to only one hand, evaluation should rightfully consider both hands. In this paper we evaluate the performance of touchless gestural interaction for pointer manipulation with both the preferred and non-preferred hands. This interaction is benchmarked against the mouse and the touchpad with a multidirectional task. We compared the performance between all devices, improvement in performance between 2 rounds, and the degradation of performance between hands. The results show the mouse has no performance increase between rounds but high degradation across hands, the touchpad has medium performance increase and medium degradation, and gestural interaction has the highest performance increase and the lowest degradation between hands.","venue":"NordiCHI","listofauthors":"Alvin Jude, G. M. Poor, Darren Guinness","citations":16,"year":2014,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2014,\n\tdoi = {10.1145/2639189.2641207},\n\turl = {https://doi.org/10.1145%2F2639189.2641207},\n\tyear = 2014,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Alvin Jude and G. Michael Poor and Darren Guinness},\n\ttitle = {An evaluation of touchless hand gestural interaction for pointing tasks with preferred and non-preferred hands}\n}","authorsSemantic":[7]},{"id":402,"title":"Evaluating multimodal interaction with gestures and speech for point and select tasks","doi":"10.1145/2639189.2670267","description":"Natural interactions such as speech and gestures have achieved mainstream success independently, with consumer products such as Leap Motion popularizing gestures, while mobile phones have embraced speech input. In this paper we designed an interaction style that combines both gestures and speech to evaluate point and select interaction. Our results indicate that while gestures are slower than the mouse, the introduction of speech allows for selection to be performed without negatively impacting navigation. We also found that users can adapt to this interaction quickly and are able to improve their performance with minimal training. This lays the foundation for future work, such as mouse replacement technologies for those with hand impairments.","venue":"NordiCHI","listofauthors":"Alvin Jude, G. M. Poor, Darren Guinness","citations":3,"year":2014,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2014,\n\tdoi = {10.1145/2639189.2670267},\n\turl = {https://doi.org/10.1145%2F2639189.2670267},\n\tyear = 2014,\n\tmonth = {oct},\n\tpublisher = {{ACM}},\n\tauthor = {Alvin Jude and G. Michael Poor and Darren Guinness},\n\ttitle = {Evaluating multimodal interaction with gestures and speech for point and select tasks}\n}","authorsSemantic":[7]},{"id":403,"title":"Mobility Matters: Identifying Cognitive Demands That Are Sensitive to Orientation","doi":"10.1007/978-3-642-40483-2_14","description":"Prior studies have shown benefits of interactions on mobile devices. Device mobility itself changes the nature of the user experience; interactions on mobile devices may present better support for cognition. To better understand cognitive demands related to mobility, the current study investigated presentations on a mobile device for a three-dimensional construction task. The task imposed considerable cognitive load, particularly in demands for mental rotation; individual differences in spatial ability are known to interact with these demands. This study specifically investigated mobile device orientations and participants’ spatial ability. Subjects with low spatial ability were able to complete the task more effectively when shown the presentation in a favorable orientation. Individuals who saw the presentation in an unfavorable orientation and those of low spatial ability, were differentially disadvantaged. We conclude that mobility can reduce cognitive load by limiting demands for spatial processing relating to reorientation.","venue":"INTERACT","listofauthors":"G. M. Poor, Guy W. Zimmerman, D. Klopfer, Samuel D. Jaffee, L. Leventhal, J. Barnes","citations":3,"year":2013,"publisher":"Springer Berlin Heidelberg","pages":"193-210","volume":null,"number":null,"bibtex":"@incollection{2013,\n\tdoi = {10.1007/978-3-642-40483-2_14},\n\turl = {https://doi.org/10.1007%2F978-3-642-40483-2_14},\n\tyear = 2013,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {193--210},\n\tauthor = {G. Michael Poor and Guy Zimmerman and Dale S. Klopfer and Samuel D. Jaffee and Laura Marie Leventhal and Julie Barnes},\n\ttitle = {Mobility Matters: Identifying Cognitive Demands That Are Sensitive to Orientation}\n}","authorsSemantic":[7]},{"id":404,"title":"On interface closeness and problem solving","doi":"10.1145/2460625.2460647","description":"Prior research suggests that \"closer\" interface styles, such as touch and tangible, would yield poorer performance on problem solving tasks as a result of their more natural interaction style. However, virtually no empirical investigations have been conducted to test this assumption. In this paper we describe an empirical study, comparing three interfaces, varying in closeness (mouse, touchscreen, and tangible) on a novel abstract problem solving task. We found that the tangible interface was significantly slower than both the mouse and touch interfaces. However, the touch and tangible interfaces were significantly more efficient than the mouse interface in problem solving across a number of measures. Overall, we found that the touch interface condition offered the best combination of speed and efficiency; in general, the closer interfaces offer significant benefit over the traditional mouse interface on abstract problem solving.","venue":"TEI '13","listofauthors":"T. J. Donahue, G. M. Poor, Martez E. Mott, L. Leventhal, Guy W. Zimmerman, D. Klopfer","citations":5,"year":2013,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2013,\n\tdoi = {10.1145/2460625.2460647},\n\turl = {https://doi.org/10.1145%2F2460625.2460647},\n\tyear = 2013,\n\tpublisher = {{ACM} Press},\n\tauthor = {Thomas J. Donahue and G. Michael Poor and Martez E. Mott and Laura Marie Leventhal and Guy Zimmerman and Dale Klopfer},\n\ttitle = {On interface closeness and problem solving}\n}","authorsSemantic":[7]},{"id":405,"title":"Leveraging motor learning for a tangible password system","doi":"10.1145/2212776.2223842","description":"Tangible user interfaces (TUIs) may allow users to have more direct interaction with systems when compared to traditional graphical user interfaces (GUIs). However, the full range of applications where TUIs can be utilized in practice is unclear. To resolve this problem, the benefits of TUIs must be analyzed and matched to an application domain where they hold advantages over more traditional systems. Since TUIs require users to use their hands in order to interact with the system, there is the possibility for these systems to leverage motor learning to help users perform specific tasks. In this paper we will describe an early attempt to understand how motor learning can be used to create a tangible password system. A novel tangible password system was created and a small study conducted in order to identify future research objectives.","venue":"CHI Extended Abstracts","listofauthors":"Martez E. Mott, T. J. Donahue, G. M. Poor, L. Leventhal","citations":7,"year":2012,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2012,\n\tdoi = {10.1145/2212776.2223842},\n\turl = {https://doi.org/10.1145%2F2212776.2223842},\n\tyear = 2012,\n\tmonth = {may},\n\tpublisher = {{ACM}},\n\tauthor = {Martez Mott and Thomas Donahue and G. Michael Poor and Laura Leventhal},\n\ttitle = {Leveraging motor learning for a tangible password system}\n}","authorsSemantic":[7]},{"id":406,"title":"No User Left Behind: Including Accessibility in Student Projects and the Impact on CS Students’ Attitudes","doi":"10.1145/2160547.2160548","description":"Usability and accessibility have become increasingly important in computing curricula. This article briefly reviews how these concepts may be included in existing courses. The authors conducted a survey of student attitudes toward these issues at the start and end of a usability engineering course that included a group project with an accessibility component. Results of the survey indicate that students’ awareness of issues related to usability and accessibility are increased after taking the course and completing the project. Our work and results are potentially valuable to CS educators in three ways: (1) They validate the usefulness of the survey instrument in assessing pedagogies in usability engineering, (2) They provide useful insights into the attitudes of CS majors relative to the important topics of usability and accessibility, and (3) They point to possible benefits of including usability and accessibility topics into CS curricula.","venue":"TOCE","listofauthors":"G. M. Poor, L. Leventhal, J. Barnes, Duke R. Hutchings, P. Albee, Laura A. Campbell","citations":15,"year":2012,"publisher":"Association for Computing Machinery (ACM)","pages":"1-22","volume":"12","number":"2","bibtex":"@article{2012,\n\tdoi = {10.1145/2160547.2160548},\n\turl = {https://doi.org/10.1145%2F2160547.2160548},\n\tyear = 2012,\n\tmonth = {apr},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {12},\n\tnumber = {2},\n\tpages = {1--22},\n\tauthor = {G. Michael Poor and Laura M. Leventhal and Julie Barnes and Duke R. Hutchings and Paul Albee and Laura Campbell},\n\ttitle = {No User Left Behind}\n}","authorsSemantic":[7]},{"id":407,"title":"Introducing Animatronics to HCI: Extending Reality-Based Interaction","doi":"10.1007/978-3-642-21605-3_65","description":"As both software and hardware technologies have been improved during the past two decades, a number of interfaces have been developed by HCI researchers. As these researchers began to explore the next generation of interaction styles, it was inevitable that they use a lifelike robot (or animatronic) as the basis for interaction. However, the main use up to this point for animatronic technology had been \"edutainment.\" Only recently was animatronic technology even considered for use as an interaction style. In this research, various interaction styles (conventional GUI, AR, 3D graphics, and introducing an animatronic user interface) were used to instruct users on a 3D construction task which was constant across the various styles. From this experiment the placement, if any, of animatronic technology in the realitybased interaction framework will become more apparent.","venue":"HCI","listofauthors":"G. M. Poor, R. Jacob","citations":2,"year":2011,"publisher":"Springer Berlin Heidelberg","pages":"593-602","volume":null,"number":null,"bibtex":"@incollection{2011,\n\tdoi = {10.1007/978-3-642-21605-3_65},\n\turl = {https://doi.org/10.1007%2F978-3-642-21605-3_65},\n\tyear = 2011,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {593--602},\n\tauthor = {G. Michael Poor and Robert J. K. Jacob},\n\ttitle = {Introducing Animatronics to {HCI}: Extending Reality-Based Interaction}\n}","authorsSemantic":[7]},{"id":457,"title":"A Comparison of Elementary Students' Web Searching Behaviors according to Personality Types","doi":null,"description":"null","venue":"FECS","listofauthors":"Jin-hee Ko, Eui-young Kang, Hanil Kim, Eunjee Song","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":408,"title":"More than Speed? An Empirical Study of Touchscreens and Body Awareness on an Object Manipulation Task","doi":"10.1007/978-3-642-21605-3_4","description":"Touchscreen interfaces do more than allow users to execute speedy interactions. Three interfaces (touchscreen, mouse-drag, on-screen button) were used in the service of performing an object manipulation task. Results showed that planning time was shortest with touch screens, that touchscreens allowed high action knowledge users to perform the task more efficiently, and that only with touchscreens was the ability to rotate the object the same across all axes of rotation. The concept of closeness is introduced to explain the potential advantages of touchscreen interfaces.","venue":"HCI","listofauthors":"R. Hippler, D. Klopfer, L. Leventhal, G. M. Poor, B. A. Klein, Samuel D. Jaffee","citations":5,"year":2011,"publisher":"Springer Berlin Heidelberg","pages":"33-42","volume":null,"number":null,"bibtex":"@incollection{2011,\n\tdoi = {10.1007/978-3-642-21605-3_4},\n\turl = {https://doi.org/10.1007%2F978-3-642-21605-3_4},\n\tyear = 2011,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {33--42},\n\tauthor = {Rachelle Kristof Hippler and Dale S. Klopfer and Laura Marie Leventhal and G. Michael Poor and Brandi A. Klein and Samuel D. Jaffee},\n\ttitle = {More than Speed? An Empirical Study of Touchscreens and Body Awareness on an Object Manipulation Task}\n}","authorsSemantic":[7]},{"id":409,"title":"Access-a-WoW: Building an Enhanced World of WarcraftTM UI for Persons with Low Visual Acuity","doi":"10.1007/978-3-642-21663-3_38","description":"World of Warcraft™ (WoW) is a virtual 3D game that offers much in terms of entertainment and collaboration and enjoys extraordinary world wide popularity. However like many other applications that deliver the majority of information visually, the default user interface (UI) is potentially only marginally accessible to users with limited visual acuity. This paper describes the enhanced user interface (UI) we constructed to improve accessibility for these users. We performed a study comparing the two user interfaces; users had simulated low visual acuity. The results of the study suggest that our enhanced UI led to significant improvements in user performance and speed of game play. Our current enhanced UI and planned future work have great potential for expanding opportunities for a user group to participate in the WoW community more fully than is possible with the current UI.","venue":"HCI","listofauthors":"G. M. Poor, T. J. Donahue, Martez E. Mott, Guy W. Zimmerman, L. Leventhal","citations":0,"year":2011,"publisher":"Springer Berlin Heidelberg","pages":"352-361","volume":null,"number":null,"bibtex":"@incollection{2011,\n\tdoi = {10.1007/978-3-642-21663-3_38},\n\turl = {https://doi.org/10.1007%2F978-3-642-21663-3_38},\n\tyear = 2011,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {352--361},\n\tauthor = {G. Michael Poor and Thomas J. Donahue and Martez E. Mott and Guy W. Zimmerman and Laura Marie Leventhal},\n\ttitle = {Access-a-{WoW}: Building an Enhanced World of {WarcraftTM} {UI} for Persons with Low Visual Acuity}\n}","authorsSemantic":[7]},{"id":410,"title":"Thought cubes: exploring the use of an inexpensive brain-computer interface on a mental rotation task","doi":"10.1145/2049536.2049612","description":"Brain-computer interfaces (BCI) allow users to relay information to a computer by capturing reactions to their thoughts via brain waves (or similar measurements). This \"new\" type of interaction allows users with limited motor control to interact with a computer without a mouse/keyboard or other physically manipulated interaction device. While this technology is in its infancy, there have been major strides in the area allowing researchers to investigate potential uses. One of the first such interfaces that has broached the commercial market at an affordable price is the Emotiv \"EPOC\" headset. This paper reports on results of a study exploring usage of the EPOC headset.","venue":"ASSETS","listofauthors":"G. M. Poor, L. Leventhal, Scott Kelley, J. Ringenberg, Samuel D. Jaffee","citations":16,"year":2011,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2011,\n\tdoi = {10.1145/2049536.2049612},\n\turl = {https://doi.org/10.1145%2F2049536.2049612},\n\tyear = 2011,\n\tpublisher = {{ACM} Press},\n\tauthor = {G. MIchael Poor and Laura Marie Leventhal and Scott Kelley and Jordan Ringenberg and Samuel D. Jaffee},\n\ttitle = {Thought cubes}\n}","authorsSemantic":[7]},{"id":411,"title":"\"How Do I Line Up?\": Reducing Mental Transformations to Improve Performance","doi":"10.1007/978-3-642-21602-2_47","description":"Mobile devices and visual-spatial presentations of information are pervasive, especially for tasks in which the mobile device can be moved to close proximity of the task. This mobility allows the user to offload mental workload by allowing physical transformations of the device. In this study, we compared a fixed mobile device, a non-fixed mobile device, and a fixed desktop display to determine the effects imposed by the mental workload of transforming the frames of reference into alignment. Our results indicate that allowing the user to manipulate the device's position can influence performance by reducing the need for mental transformations.","venue":"HCI","listofauthors":"Guy W. Zimmerman, D. Klopfer, G. M. Poor, J. Barnes, L. Leventhal, Samuel D. Jaffee","citations":4,"year":2011,"publisher":"Springer Berlin Heidelberg","pages":"432-440","volume":null,"number":null,"bibtex":"@incollection{2011,\n\tdoi = {10.1007/978-3-642-21602-2_47},\n\turl = {https://doi.org/10.1007%2F978-3-642-21602-2_47},\n\tyear = 2011,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {432--440},\n\tauthor = {Guy W. Zimmerman and Dale Klopfer and G. Michael Poor and Julie Barnes and Laura Leventhal and Samuel D. Jaffee},\n\ttitle = {{\\textquotedblright}How Do I Line Up?{\\textquotedblright}: Reducing Mental Transformations to Improve Performance}\n}","authorsSemantic":[7]},{"id":412,"title":"Accessibility: understanding attitudes of CS students","doi":"10.1145/1639642.1639684","description":"Accessibility and usability have become increasingly important in design and development of technology. This poster briefly reviews how accessibility concepts may be included in computer science courses as students are educated to become practitioners. In a usability engineering course, the authors included a group development project that included an accessibility component. They conducted a survey of student attitudes toward these issues at the start and end of the course. Results of the survey indicate that students' awareness of issues related to usability and accessibility are increased after taking the course and completing the project. In particular, students showed a significant increase in their rating of importance for the item \"broadening the range of technology users\". The authors also performed a factor analysis of the survey responses and determined that items fell into three factors, one of which was concerned with accessibility and usability.","venue":"Assets '09","listofauthors":"G. M. Poor, L. Leventhal, J. Barnes, Duke R. Hutchings","citations":3,"year":2009,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2009,\n\tdoi = {10.1145/1639642.1639684},\n\turl = {https://doi.org/10.1145%2F1639642.1639684},\n\tyear = 2009,\n\tpublisher = {{ACM} Press},\n\tauthor = {G. M. Poor and Laura M. Leventhal and Julie Barnes and Duke R. Hutchings},\n\ttitle = {Accessibility}\n}","authorsSemantic":[7]},{"id":413,"title":"The effects of varying levels of reality-based interaction styles on a subject's ability to perform a three-dimensional construction task","doi":null,"description":"The nature and quality of human-computer interaction (HCI) continues to be of interest to computer science researchers. While most advancement in this realm have come from practical experience, there had not been a great deal of theoretical or experimental research in advancing basic understanding. A new HCI conceptual framework focused on reality-based interactions has been advanced by Jacob et al. (2008). The core problem that the reality-based interaction (RBI) framework addresses is the need for users to more easily use an interface. The core proposition of the framework is that the more reality-based interactions that an interface incorporates, the easier and quicker a user will be able to use the interface. It was apparent from the review of the literature that there is little scientific evidence that supports or refutes this proposition and the framework. \nThe purpose of this study was to investigate whether a user's ability to interact with an interface improves with increases in the amount of reality-based interaction in that interface. More specifically, this research examined delivering instructions to a user for a 3-dimensional (3D) construction task through various reality-based interactions. \nA review of the experimental design creation will be covered. The review will outline the entire development process of all conditions, creation of the conditions, and task selection and refinement. Once the preliminary development is covered, the experiment will be outlined in more detail. \nThe experimental design of this study employed 4 groups to obtain the necessary data to test the hypotheses. The groups were tested on various levels of reality-based interactions. Those treatment groups included the following: (1) a simple graphical user interface (GUI) with a simple slideshow of instructions, (2) a 3-dimensional representation of a person delivering instructions on a 2-dimensional monitor (referred to as the 3D condition), (3) a 3-dimensional representation of a person delivering instructions in an augmented reality (AR) environment using a head-mounted display, and (4) an animatronic representation of a human being delivering instructions. Each of these interacted with subjects through gestures, speech, or pictures, but they did not interact with the construction task itself. \nFor this experiment, performance was measured in terms of 7 dependent variables. These dependent variables were divided into 2 groups. The first group included the number of times a step was repeated, the number of times human assistance was requested, and the total number of errors. The second group included the 4 types of errors that a subject could perform: incorrect connection, incorrect rotation, incorrect piece selection, and incorrect tilt or angle. The higher the occurrence of these dependent variables, the worse a subject's performance. \nAs was shown in the statistical results, there was a significant difference between interaction styles in all 3 of the first group's dependent variables. There were also significant differences between interaction styles in 2 of the dependent variables from the second group: incorrect piece and incorrect angle errors. Therefore, given these results, we are able to reject the null hypothesis and say that there is a significant difference in performance of a user's ability to complete a construction task delivered by 4 interaction styles with varying levels of reality-based interaction techniques.","venue":"","listofauthors":"R. Jacob, G. M. Poor","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[7]},{"id":414,"title":"The effects of varying levels of reality-based interaction styles on a subject's ability to perform a 3D construction task","doi":null,"description":"null","venue":"","listofauthors":"G. M. Poor","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[7]},{"id":415,"title":"Memorable Password Generation with AES in ECB Mode","doi":"10.1007/978-3-030-71017-0_3","description":"This chapter presents ALP: an adjustable lexicon-generated password mechanism. ALP is a password encryption web application. In ALP, a password is generated randomly from an imported dictionary of words. For example, with an entire dictionary, it will pick out random strings that add up to a 128-bit hex key and return the key to the user. The password will be “easy to remember” since it will be made of a chosen pool of words. It will then take in the user’s message to be encrypted and output the encrypted message according to the key. The password generator, if need be, can be switched to a completely random mode which will make it output a string of random numbers and letters if the user does not choose to use the easy-to-remember password generator function. The application will also be able to decrypt the AES-128 encrypted message and transform it back into normal text. This experiment suggests that the proposed model is a block cipher that can successfully generate a memorable random password.","venue":"Advances in Security, Networks, and Internet of Things","listofauthors":"Timothy Hoang, Pablo Rivas","citations":0,"year":2021,"publisher":"Springer International Publishing","pages":"33-38","volume":null,"number":null,"bibtex":"@incollection{2021,\n\tdoi = {10.1007/978-3-030-71017-0_3},\n\turl = {https://doi.org/10.1007%2F978-3-030-71017-0_3},\n\tyear = 2021,\n\tpublisher = {Springer International Publishing},\n\tpages = {33--38},\n\tauthor = {Timothy Hoang and Pablo Rivas},\n\ttitle = {Memorable Password Generation with {AES} in {ECB} Mode}\n}","authorsSemantic":[8]},{"id":416,"title":"Towards Adversarially Robust DDoS-Attack Classification","doi":"10.1109/UEMCON51285.2020.9298167","description":"On the frontier of cybersecurity are a class of emergent security threats that learn to find vulnerabilities in machine learning systems. A supervised machine learning classifier learns a mapping from x to y where x is the input features and y is a vector of associated labels. Neural Networks are state of the art performers on most vision, audio, and natural language processing tasks. Neural Networks have been shown to be vulnerable to adversarial perturbations of the input, which cause them to misclassify with high confidence. Adversarial perturbations are small but targeted modifications to the input often undetectable by the human eye. Adversarial perturbations pose risk to applications that rely on machine learning models. Neural Networks have been shown to be able to classify distributed denial of service (DDoS) attacks by learning a dataset of attack characteristics visualized using three-axis hive plots. In this work we present a novel application of a classifier trained to classify DDoS attacks that is robust to some of the most common, known, classes of gradient-based and gradient-free adversarial attacks.","venue":"2020 11th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","listofauthors":"Michael Guarino, Pablo Rivas, C. DeCusatis","citations":1,"year":2020,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2020,\n\tdoi = {10.1109/uemcon51285.2020.9298167},\n\turl = {https://doi.org/10.1109%2Fuemcon51285.2020.9298167},\n\tyear = 2020,\n\tmonth = {oct},\n\tpublisher = {{IEEE}},\n\tauthor = {Michael Guarino and Pablo Rivas and Casimer DeCusatis},\n\ttitle = {Towards Adversarially Robust {DDoS}-Attack Classification}\n}","authorsSemantic":[8]},{"id":417,"title":"Deep Learning for Beginners","doi":null,"description":"Implementing supervised, unsupervised, and generative deep learning (DL) models using Keras and Dopamine over TensorFlow.Key FeaturesUnderstand the fundamental machine learning concepts useful in deep learningLearn the underlying mathematical concepts as you implement deep learning models from scratchExplore easy-to-understand examples and use cases that will help you build a solid foundation in DLBook DescriptionWith information on the web exponentially increasing, it has become more difficult than ever to navigate through everything to find reliable content that will help you get started with deep learning. This book is designed to help you if you're a beginner looking to work on deep learning and build deep learning models from scratch, and already have the basic mathematical and programming knowledge required to get started.The book begins with a basic overview of machine learning, guiding you through setting up popular Python frameworks. You will also understand how to prepare data by cleaning and preprocessing it for deep learning, and gradually go on to explore neural networks. A dedicated section will give you insights into the working of neural networks by helping you get hands-on with training single and multiple layers of neurons. Later, you will cover popular neural network architectures such as CNNs, RNNs, AEs, VAEs, and GANs with the help of simple examples, and you will even build models from scratch. At the end of each chapter, you will find a question and answer section to help you test what you've learned through the course of the book.By the end of this book, you'll be well-versed with deep learning concepts and have the knowledge you need to use specific algorithms with various tools for different tasks.What you will learnImplement RNNs and Long short-term memory for image classification and Natural Language Processing tasksExplore the role of CNNs in computer vision and signal processingUnderstand the ethical implications of deep learning modelingUnderstand the mathematical terminology associated with deep learningCode a GAN and a VAE to generate images from a learned latent spaceImplement visualization techniques to compare AEs and VAEsWho this book is forThis book is for aspiring data scientists and deep learning engineers who want to get started with the fundamentals of deep learning and neural networks. Although no prior knowledge of deep learning or machine learning is required, familiarity with linear algebra and Python programming is necessary to get started.","venue":"","listofauthors":"Pablo Rivas, Laura Montoya","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":418,"title":"Dog Breed Identification with a Neural Network over Learned Representations from The Xception CNN Architecture","doi":null,"description":"Machine learning is a growing field that has greatly increased with the continuing advancements in technology. This area provides many tools that can perform different tasks on large data sets. The focus of this paper is on classification tools. Classification tools are utilized in order to classify or predict the breeds of dogs based on an input image. Many methods are used in attempts to classify the images in the data set. The data set comes from a Kaggle competition in which the goal is to predict the breed of dog in the image. Participants tried many different methods, some of which helped inspire this research. The classification tools that are explored here are a Convolutional Neural Network and Xception with a Multilayer Perceptron. The paper explores the trial and error in all of the methods as well as the final model that was used to predict and classify the dog breeds. While the final model has a much better prediction rate than the original attempt, there is an acknowledgement of the errors made throughout the process. With this acknowledgement comes areas to improve and ideas to further explore this model as a classification tool","venue":"","listofauthors":"K. Mulligan, Pablo Rivas","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":419,"title":"Application-Agnostic Chatbot Deployment Considerations: A Case Study","doi":"10.1109/CSCI49370.2019.00070","description":"Advances in machine learning are making possible the interaction between humans and machines, coming closer to passing the Turing test. Chatbots, specifically, are a technology that uses the latest advances in natural language processing and machine learning to understand text and produce text in response to input. While this is an important achievement today, we must consider specific challenges that chatbot deployments might pose. This paper looks back to a historical event that took place in 2016 with the purpose of extracting important, memorable, lessons. The study suggests that certain assumptions with respect to societal values are of paramount importance and need to be considered carefully along with a proper platform selection.","venue":"2019 International Conference on Computational Science and Computational Intelligence (CSCI)","listofauthors":"Pablo Rivas, Chelsi Chelsi, Nishit Nishit, Laharika Ravula","citations":1,"year":2019,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1109/csci49370.2019.00070},\n\turl = {https://doi.org/10.1109%2Fcsci49370.2019.00070},\n\tyear = 2019,\n\tmonth = {dec},\n\tpublisher = {{IEEE}},\n\tauthor = {Pablo Rivas and Chelsi Chelsi and Nishit Nishit and Laharika Ravula},\n\ttitle = {Application-Agnostic Chatbot Deployment Considerations: A Case Study}\n}","authorsSemantic":[8]},{"id":423,"title":"Empirical Study of Sentence Embeddings for English Sentences Quality Assessment","doi":"10.1109/CSCI49370.2019.00065","description":"Novel deep learning and machine translation techniques have greatly advanced the field of computational linguistics enabling us to find meaningful latent spaces for text analysis. While several embedding techniques exist for words, sentences, and entire documents, the potential applications are still being explored. In this paper we present the impact of top-performing sentence embedding methodologies on the accuracy of a neural model trained to assess the quality of English sentences. We focus our efforts in the methodologies called Language Agnostic SEntence Representation (LASER), Sentence to Vector (S2V), and Universal Sentence Encoder (USE) to observe their ability to capture information related to sentence quality. Our study suggests that these state-of-the-art sentence embeddings are unable to capture sufficient information regarding sentence correctness and quality in the English language.","venue":"2019 International Conference on Computational Science and Computational Intelligence (CSCI)","listofauthors":"Pablo Rivas, Marcus Zimmermann","citations":0,"year":2019,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1109/csci49370.2019.00065},\n\turl = {https://doi.org/10.1109%2Fcsci49370.2019.00065},\n\tyear = 2019,\n\tmonth = {dec},\n\tpublisher = {{IEEE}},\n\tauthor = {Pablo Rivas and Marcus Zimmermann},\n\ttitle = {Empirical Study of Sentence Embeddings for English Sentences Quality Assessment}\n}","authorsSemantic":[8]},{"id":424,"title":"Machine Learning for DDoS Attack Classification Using Hive Plots","doi":"10.1109/UEMCON47517.2019.8993021","description":"Cyberattacks have been on the increase as computing power and data storage have become more accessible. The use of recent advances in machine learning across different fields has increased the potential adoption of new algorithms in solving important technological problems. In this paper we describe a novel application of machine learning for the detection and classification of distributed denial of service (DDoS) cybersecurity attacks. Attack pattern training data is obtained from honeypots which we created to impersonate various APIs on a cloud computing network. Attack characteristics including source IP address, country of origin, and time of attack are collected from our honeypots and visualized using a three-axis hive plot. We then implemented and trained a non-probabilistic binary linear attack pattern classifier. A support vector machine and a convolutional neural network were trained using a supervised learning model with labeled data sets. Experimental results suggest that our models can detect DDoS attacks with high accuracy rates.","venue":"2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","listofauthors":"Pablo Rivas, C. DeCusatis, Matthew Oakley, Alex Antaki, Nicholas Blaskey, S. LaFalce, Stephen Stone","citations":3,"year":2019,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1109/uemcon47517.2019.8993021},\n\turl = {https://doi.org/10.1109%2Fuemcon47517.2019.8993021},\n\tyear = 2019,\n\tmonth = {oct},\n\tpublisher = {{IEEE}},\n\tauthor = {Pablo Rivas and Casimer DeCusatis and Matthew Oakley and Alex Antaki and Nicholas Blaskey and Steven LaFalce and Stephen Stone},\n\ttitle = {Machine Learning for {DDoS} Attack Classification Using Hive Plots}\n}","authorsSemantic":[8]},{"id":425,"title":"Government AI Readiness Meta-Analysis for Latin America And The Caribbean","doi":"10.1109/istas48451.2019.8937869","description":"Artificial intelligence (AI)-based technology has the potential of transforming how governments function, making them better able to serve, protect, and improve the quality of life of their constituents. As governments of developing countries continue to shift to more advanced digital platforms, they have adopted practices and policies that have a direct impact on the future of AI-based technology. This research aims to discuss factors that may have a direct impact on the AI preparedness of Latin America and the Caribbean (LAC) countries. It evaluates a recent ranking developed by the international development research center (IDRC) focused on indicators towards the development or use of AI technology in governance, infrastructure, technological skills, and public services against each country's economic metrics including unemployment rate, gross domestic product per capita purchasing power parity (GDP-PPP), cost to hire an AI researcher, and countrywide education levels. It also reviews metrics and factors outside of economics which have a direct impact on AI readiness and its effects on each country's citizens, including automation potential and data privacy policies. This research discusses current issues with existing evaluation criteria for AI readiness and expands factors and considerations essential to LAC countries as they embrace the AI revolution.","venue":"2019 IEEE International Symposium on Technology and Society (ISTAS)","listofauthors":"Laura Montoya, Pablo Rivas","citations":3,"year":2019,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1109/istas48451.2019.8937869},\n\turl = {https://doi.org/10.1109%2Fistas48451.2019.8937869},\n\tyear = 2019,\n\tmonth = {nov},\n\tpublisher = {{IEEE}},\n\tauthor = {Laura Montoya and Pablo Rivas},\n\ttitle = {Government {AI} Readiness Meta-Analysis for Latin America And The Caribbean}\n}","authorsSemantic":[8]},{"id":426,"title":"Deep Sparse Autoencoders for American Sign Language Recognition Using Depth Images","doi":null,"description":"In this paper we address the problem of hand gestures recognition in the American Sign Language using machine learning. We propose a five layer unsupervised encoder-decoder neural model. We use a dataset of segmented images captured with a depth-sensor camera for different subjects. The average accuracy obtained was of 98.9% and as high as 99.4% on unseen data.","venue":"","listofauthors":"Pablo Rivas, Ezequiel Rivas, O. Velarde, Samuel Gonzalez","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":427,"title":"Finding Time Series Breakpoints with Fully Connected Neural Networks","doi":null,"description":"Breakpoint analysis is a technique that helps individuals better understand and model time series. This paper describes how a fully connected neural network can predict breakpoints in time series. Using a sigmoidal activation, an MSE loss function, an Adam optimizer and drop out rate this function is able to estimate the location of breakpoints. This model is training on simulated time series with clear breaks created by mean changes and multiple trials were run to identify the optimal hyper-parameters. Then, the model is applied to real world pelican population data. Comparing results to other breakpoint analysis approaches this neural network model identifies the general location where a known breakpoint occurs in this pelican data.","venue":"","listofauthors":"Amy Pitts, Pablo Rivas","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":428,"title":"Societal Benefits and Risks of Artificial Intelligence : A Succinct Survey","doi":null,"description":"In recent years we have learned that with great AI power comes great social responsibility. Thus, we studied the major societal benefits of AI in the areas of health care, education, and agriculture, discussing the positive change that AI has brought to each. We further address the risks society faces when AI is practiced without proper care, lack of responsibility, poor assessment of bias, and a complete disregard for ethically aligned designs. We aim to expose where we fall short so that we can move forward as a society.","venue":"","listofauthors":"Sabrina Bergsten, Pablo Rivas","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":429,"title":"Rule-Based Sentence Quality Modeling and Assessment using Deep LSTM Features","doi":null,"description":"People trying to master the art of writing correct sentences in English for the first time often find themselves in the struggle of learning the essential rules to do so. In an effort to assist in this process we studied how to represent a sentence with the purpose of analyzing its quality according to wellknown rules. We make use of recent developments in deep learning to achieve a rich representation of single sentences, commonly known as embedding [1]. Sentence quality has been traditionally evaluated in the context of machine translation to measure success [2, 3]. However, our work measures the quality of a sentence as the end product of the model. Recently, recurrent neural networks (RNNs) with long-short term memories (LSTMs) have been used for word embedding [4], sentence embedding [5], and paragraph or document embedding [6] with very good results. These works have demonstrated the utility and robustness of finding discriminative vectorial representations that preserve context and major structures of words and bodies of text. Thus, we decided to study their utility in modeling and assessing the overall quality of individual sentences.","venue":"","listofauthors":"Pablo Rivas","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":430,"title":"El software como derecho de autor","doi":null,"description":"La Tesis propuesta El Software como Derecho de Autor constituye un trabajo de investigacion en donde pretende dilucidar y aclarar los conceptos y relacion entre los programas de computo, llamados por su nombre en ingles, Software, en relacion con la doctrina y normas juridicas sobre Derecho de Autor.","venue":"","listofauthors":"Julio Mauricio Barros Uguña, Pablo Rivas","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[8]},{"id":431,"title":"Session details: Theme: Software design and development: SE - Software engineering track","doi":"10.1145/3389671","description":"null","venue":"","listofauthors":"Byungjeong Lee, Eunjee Song, Tao Zhang","citations":0,"year":2020,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2020,\n\tdoi = {10.1145/3389671},\n\turl = {https://doi.org/10.1145%2F3389671},\n\tyear = 2020,\n\tmonth = {mar},\n\tpublisher = {{ACM}},\n\tauthor = {Byungjeong Lee and Eunjee Song and Tao Zhang},\n\ttitle = {Session details: Theme: Software design and development: {SE} - Software engineering track}\n}","authorsSemantic":[9]},{"id":432,"title":"Session details: Theme: Software design and development: SE - Software engineering track","doi":"10.1145/3329389","description":"A special track on Software Engineering (SE Track) aims to be a forum for scientists, engineers and practitioners, in academia and industry to share new ideas, experiences and results, and to present their latest findings in any aspects of Software Engineering. SE Track emphasizes the design, implementation, management and applications of Software Engineering.","venue":"Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing","listofauthors":"Tao Zhang, Byungjeong Lee, Eunjee Song","citations":0,"year":2019,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2019,\n\tdoi = {10.1145/3329389},\n\turl = {https://doi.org/10.1145%2F3329389},\n\tyear = 2019,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\teditor = {Tao Zhang and Byungjeong Lee and Eunjee Song},\n\ttitle = {Session details: Theme: Software design and development: {SE} - Software engineering track}\n}","authorsSemantic":[9]},{"id":433,"title":"Session details: Software design and development: SE - software engineering track","doi":"10.1145/3258643","description":"null","venue":"","listofauthors":"Eunjee Song, Byungjeong Lee, T. Zhang","citations":0,"year":2018,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2018,\n\tdoi = {10.1145/3258643},\n\turl = {https://doi.org/10.1145%2F3258643},\n\tyear = 2018,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\teditor = {Eunjee Song and Byungjeong Lee and Tao Zhang},\n\ttitle = {Session details: Software design and development: {SE} - software engineering track}\n}","authorsSemantic":[9]},{"id":434,"title":"Special issue on software reuse","doi":"10.1002/spe.2504","description":"Reuse is a common practice in software development, and it is critical for both cost savings and quality assurance. When properly done, reuse of artifacts (e.g., design patterns and source code) often improves productivity, in turn leading to cost savings. More importantly software reuse improves the quality of software, because of the reuse of quality code components, and makes it possible to leverage the collective wisdom of numerous seasoned software engineers who faced similar challenges and found optimal solutions widely accepted by the software developer community. Types of reusable software artifacts vary ranging from those found in the requirements phase to the design phase, and to the implementation and testing phases. They include domain requirements, architectural and design patterns, Commercial Off-The-Shelf, code, reusable tests, and so on. The compatibility of different types of reusable artifacts needs to be studied and analyzed as a basis of reuse-based development, which also establishes traceability throughout the development process. Systematic approaches that facilitate not only reuse of the artifacts of a single type but also the combinatorial use of multiple types of reusable artifacts are necessary. Non-functional aspects such as security, reusability, and maintainability in the context of software reuse are also of increasing research interest. Reuse practices have rapidly expanded to emerging areas such as mobile computing, cloud computing, and service-oriented development, which has gained significant attention of the research community. This special issue solicited papers related to topics including process reuse, requirements reuse, architecture and design reuse, component reuse, code reuse, test reuse, domain modeling, aspectoriented development, metrics for software reusability, industrial experience with software reuse, reuse-driven software process, pattern-based software development, product-line engineering, reuse of non-functional aspects, tools supporting software reuse, barriers to reuse, and business perspectives of reuse. The call for the special issue received 17 submissions of which seven high-quality papers were accepted after a rigorous three-phase peer review process. The first paper titled Synergies and Tradeoffs in Software Reuse – a Systematic Mapping Study by Denise Bombonatti, Miguel Goulão, and Ana Moreira reports a study on systematic mapping of how reusability relates to other non-functional requirements and how different contextual factors influence the success of a reuse initiative. The authors conclude that the relationships are discussed rather informally, and people, organizational, and domain factors are very relevant as well as the technological constraints that apply to a particular reuse context. They highlight the need for further research to better understand how exactly the different non-functional requirements and context factors affect reusability. The second paper titled Variability Management of Plugin-Based Systems Using Feature Models by André Santos presents an automated approach for mapping the artifacts of plugin-based component frameworks to feature models. Their approach is designed to capture the variability and configurability of the architecture of a plugin-based system in feature models. They implemented the proposed approach in the Eclipse Equinox component framework to visualize the variability of a plugin-based system and generate system variants. They carried out an experiment where they developed a small plugin-based product line on top of Equinox in the context of an advanced software development course. The third paper titled Perils of Opportunistically Reusing Software Module by Naveen Kulkarni and Vasudeva Varma discusses mismatches between the software under development and reused external modules in opportunistic reuse practices and the lack of methods to pro-actively identify and resolve mismatches. To address the concerns, they argue that there should be some support for","venue":"Softw. Pract. Exp.","listofauthors":"Dae-Kyoo Kim, Eunjee Song, J. Ryoo, Y. R. Reddy","citations":0,"year":2017,"publisher":"Wiley","pages":"941-942","volume":"47","number":"7","bibtex":"@article{2017,\n\tdoi = {10.1002/spe.2504},\n\turl = {https://doi.org/10.1002%2Fspe.2504},\n\tyear = 2017,\n\tmonth = {may},\n\tpublisher = {Wiley},\n\tvolume = {47},\n\tnumber = {7},\n\tpages = {941--942},\n\tauthor = {Dae-Kyoo Kim and Eunjee Song and Jungwoo Ryoo and Y. Raghu Reddy},\n\ttitle = {Special issue on software reuse}\n}","authorsSemantic":[9]},{"id":435,"title":"Efficient processing of large-scale sparse matrix-matrix multiplications on a single machine","doi":"10.1109/SMC.2017.8122896","description":"Graphs are very widely used to represent datasets in various real-world applications. Many algorithms dealing with graphs represent them in the form of sparse matrices and process them through sparse matrix operations. Recently, as the size of a graph increases rapidly, a single-machine-based graph engine has emerged as a general framework for effectively performing large-scale sparse matrix operations. Sparse matrices multiplication (SpGEMM) is a core building block for developing a variety of graph algorithms such as all source shortest paths, betweenness centrality, breadth first search from multiple sources, subgraph indexing, and graph construction. This paper addresses how to perform two sparse matrices multiplication (SpGEMM) efficiently on a single-machine-based graph engine. To effectively perform the large-scale SpGEMM, we have two issues: (1) selecting an appropriate type of matrix products (i.e., inner products, outer products, and row-row products), and (2) allocating memory space to matrices according to different types of matrix products. We first formulate the cost models for the three matrix product types by reflecting their characteristics. We observed the commonality in the processes of the matrix multiplication and the join in a relational database. Based on this observation, we propose memory allocation schemes for three matrix products by borrowing the idea of memory allocation employed in join. Finally, we show the effectiveness of our approach via extensive experiments with real-life datasets.","venue":"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","listofauthors":"Yong-Yeon Jo, Kyuhwan Lee, Myung-Hwan Jang, Sang-Wook Kim, Eunjee Song","citations":1,"year":2017,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2017,\n\tdoi = {10.1109/smc.2017.8122896},\n\turl = {https://doi.org/10.1109%2Fsmc.2017.8122896},\n\tyear = 2017,\n\tmonth = {oct},\n\tpublisher = {{IEEE}},\n\tauthor = {Yong-Yeon Jo and Kyuhwan Lee and Myung-Hwan Jang and Sang-Wook Kim and Eunjee Song},\n\ttitle = {Efficient processing of large-scale sparse matrix-matrix multiplications on a single machine}\n}","authorsSemantic":[9]},{"id":436,"title":"Introducing Attribute-Based Access Control to AUTOSAR","doi":"10.4271/2016-01-0069","description":"null","venue":"","listofauthors":"Dae-Kyoo Kim, Eunjee Song, Huafeng Yu","citations":4,"year":2016,"publisher":"SAE International","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.4271/2016-01-0069},\n\turl = {https://doi.org/10.4271%2F2016-01-0069},\n\tyear = 2016,\n\tmonth = {apr},\n\tpublisher = {{SAE} International},\n\tauthor = {Dae-Kyoo Kim and Eunjee Song and Huafeng Yu},\n\ttitle = {Introducing Attribute-Based Access Control to {AUTOSAR}}\n}","authorsSemantic":[9]},{"id":437,"title":"Session details: Volume II: Software design and development, and system software and security: Software engineering track","doi":"10.1145/3252784","description":"null","venue":"","listofauthors":"Eunjee Song, Byungjeong Lee","citations":0,"year":2016,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2016,\n\tdoi = {10.1145/3252784},\n\turl = {https://doi.org/10.1145%2F3252784},\n\tyear = 2016,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\tauthor = {Eunjee Song and Byungjeong Lee},\n\ttitle = {Session details: Volume {II}: Software design and development, and system software and security: Software engineering track}\n}","authorsSemantic":[9]},{"id":438,"title":"Workshop Program Committee SAW 2015","doi":null,"description":"null","venue":"","listofauthors":"R. Ellison, R. Kazman, Dae-Kyoo Kim, Suntae Kim, P. Laplante, Moussa Ouedraogo, J. Ryoo, S. Schrittwieser, Eunjee Song, Paul Tavolato, S. Tjoa, Carol Woody","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":439,"title":"Session details: Volume II: Software development, system software and security: Software engineering track","doi":"10.1145/3251673","description":"null","venue":"","listofauthors":"Byungjeong Lee, Eunjee Song","citations":0,"year":2015,"publisher":"ACM","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2015,\n\tdoi = {10.1145/3251673},\n\turl = {https://doi.org/10.1145%2F3251673},\n\tyear = 2015,\n\tmonth = {apr},\n\tpublisher = {{ACM}},\n\tauthor = {Byungjeong Lee and Eunjee Song},\n\ttitle = {Session details: Volume {II}: Software development, system software and security: Software engineering track}\n}","authorsSemantic":[9]},{"id":440,"title":"Building hybrid access control by configuring RBAC and MAC features","doi":"10.1016/j.infsof.2014.02.003","description":"Abstract Context Role-Based Access Control (RBAC) and Mandatory Access Control (MAC) are widely used access control models. They are often used together in domains where both data integrity and information flow are concerned. However, there is little work on techniques for building hybrid access control of RBAC and MAC. Objective In this work, we present a systematic approach for developing a hybrid access control model using feature modeling with the aim of reducing development complexity and error-proneness. Method In the approach, RBAC and MAC are defined in terms of features based on partial inheritance. Features are then configured for specific access control requirements of an application. Configured features are composed homogeneously and heterogeneously to produce a hybrid access model for the application. The resulting hybrid model is then instantiated in the context of the application to produce an initial design model supporting both RBAC and MAC. We evaluate the approach using a hospital system and present its tool support. Results RBAC and MAC features that are specifically configured for the application are systematically incorporated into a design model. The heterogeneous features of RBAC and MAC are not only present in the resulting model, but also semantically composed for seamless integration of RBAC and MAC. Discharging the proof obligations of composition rules to the resulting model proves its correctness. The successful development of the prototype demonstrates its practicality. Conclusion Features in the access control domain are relatively small in size and are suitable to be defined as design building blocks. The formal definition of partial inheritance and composition methods in the presented approach enables precisely specifying access control features and feature configuration, which paves the way for systematic development of a hybrid access control model in an early development phase.","venue":"Inf. Softw. Technol.","listofauthors":"Sangsig Kim, Dae-Kyoo Kim, Lunjin Lu, Eunjee Song","citations":5,"year":2014,"publisher":"Elsevier BV","pages":"763-792","volume":"56","number":"7","bibtex":"@article{2014,\n\tdoi = {10.1016/j.infsof.2014.02.003},\n\turl = {https://doi.org/10.1016%2Fj.infsof.2014.02.003},\n\tyear = 2014,\n\tmonth = {jul},\n\tpublisher = {Elsevier {BV}},\n\tvolume = {56},\n\tnumber = {7},\n\tpages = {763--792},\n\tauthor = {Sangsig Kim and Dae-Kyoo Kim and Lunjin Lu and Eunjee Song},\n\ttitle = {Building hybrid access control by configuring {RBAC} and {MAC} features}\n}","authorsSemantic":[9]},{"id":441,"title":"A framework-based approach for interactive multimedia application development","doi":"10.1145/2401603.2401683","description":"The interactive multimedia application has become more popular and the development of this kind of application requires a new type of cooperative work that involves experts from many different areas. For example, domain experts, user interface developers, media production experts, and software engineers could be a few of them. Many traditional design approaches and development/authoring tools, however, lack of a systematic approach to supporting this heterogeneous application nature that requires creating complex application logic with rich graphical and interactive user interface design elements.\n In this paper, we propose a framework-based approach that supports the separation of the crosscutting concern of application logic from its graphics design elements. One benefit from this separation in our framework, compared to other approaches using existing authoring tools, is to reduce the implementation effort and to improve its future extensibility. We have verified our approach by using our framework for extending a Flash-based multimedia application, called Energy City, through a joint project with an actual museum customer.","venue":"RACS","listofauthors":"Jun Lin, Jonathan Drake, Hanil Kim, Eunjee Song","citations":1,"year":2012,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2012,\n\tdoi = {10.1145/2401603.2401683},\n\turl = {https://doi.org/10.1145%2F2401603.2401683},\n\tyear = 2012,\n\tpublisher = {{ACM} Press},\n\tauthor = {Jun Lin and Jonathan Drake and Hanil Kim and Eunjee Song},\n\ttitle = {A framework-based approach for interactive multimedia application development}\n}","authorsSemantic":[9]},{"id":442,"title":"An aspect-oriented testability framework","doi":"10.1145/2401603.2401682","description":"We can specify design constraints of a software system using a formal language such as Object Constraint Language (OCL). However, OCL is not executable and cannot be explicitly used as a testing method. So runtime constraint checking is needed to mitigate this problem. Checking design constraints at runtime can help to detect design drift. On the other hand, making a software testable is also hard since testing requires active involvement of programmers. Testable software introduces a quality characteristics of software artifact called testability. This paper mainly concentrates on improving observability, one of the major factors that determine testability of a software artifact. To avoid design drift and make software more testable, this paper proposes an aspect-oriented testability framework for object oriented software testing, which enables runtime constraint checking against implementation code and improves the testability of a software system. This framework completely takes the advantage of aspect-oriented programming (AOP) approaches and is totally modular and plug-and-playable.","venue":"RACS","listofauthors":"Nankai Pan, Eunjee Song","citations":7,"year":2012,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2012,\n\tdoi = {10.1145/2401603.2401682},\n\turl = {https://doi.org/10.1145%2F2401603.2401682},\n\tyear = 2012,\n\tpublisher = {{ACM} Press},\n\tauthor = {Nankai Pan and Eunjee Song},\n\ttitle = {An aspect-oriented testability framework}\n}","authorsSemantic":[9]},{"id":443,"title":"Modified hierarchical privacy-aware role based access control model","doi":"10.1145/2401603.2401679","description":"Privacy-aware role based access control (PRBAC) model extends much appreciated RBAC model and it is one of the leading models to define privacy policies and their enforcement. PRBAC extension, to hierarchical PRBAC (H-PRBAC), can express most of the privacy policies. But the limitation in H-PRBAC's Purpose hierarchy can lead to the explosion of policy assignments. In this paper, we review and provide an enhancement needed in the H-PRBAC model to overcome these limitations. Another limitation of PRBAC model is the lack of condition redundancy checking required for the assignment of new policies. In this paper, we provide an algorithm that was much needed for this purpose and review its complexity.","venue":"RACS","listofauthors":"Sanjeev Arora, Eunjee Song, Yoonjeong Kim","citations":5,"year":2012,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2012,\n\tdoi = {10.1145/2401603.2401679},\n\turl = {https://doi.org/10.1145%2F2401603.2401679},\n\tyear = 2012,\n\tpublisher = {{ACM} Press},\n\tauthor = {Sanjeev Arora and Eunjee Song and Yoonjeong Kim},\n\ttitle = {Modifiedhierarchical privacy-aware role based access control model}\n}","authorsSemantic":[9]},{"id":444,"title":"An Approach to Verifying Security and Timing Properties in UML Models","doi":"10.1109/ICECCS.2010.10","description":"In this paper, we present an approach to verify whether a UML design model satisfies its domain-specific security and time-related requirements in an integrated tool environment. This approach is based on a UML metamodel extension mechanism given as profiles. As a model verification tool, we chose the USE (UML-based Specification Environment) since additional functional and non-functional constraints in a UML model should be formally specified using the OCL (Object Constraint Language). In order to address both security and timing properties together in a model, we combine two profiles, UMLsec for security and MARTE (UML profile for Modeling and Analysis of Real-Time and Embedded systems) for time, into the UML metamodel. Then, this combined metamodel is converted to a form of USE specification so that it can be used for verifying models using USE. In this approach, however, this combined metamodel is considered as a large class model in USE because USE does not support profiles. Therefore, models to be verified are created as object models that are instances of the given class model, i.e. the extended metamodel in our case. Our approach is illustrated with a distributed, interoperable wireless communications-based railroad control system called the Positive Train Control (PTC) System.","venue":"2010 15th IEEE International Conference on Engineering of Complex Computer Systems","listofauthors":"Vidhi Thapa, Eunjee Song, Hanil Kim","citations":13,"year":2010,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2010,\n\tdoi = {10.1109/iceccs.2010.10},\n\turl = {https://doi.org/10.1109%2Ficeccs.2010.10},\n\tyear = 2010,\n\tmonth = {mar},\n\tpublisher = {{IEEE}},\n\tauthor = {Vidhi Thapa and Eunjee Song and Hanil Kim},\n\ttitle = {An Approach to Verifying Security and Timing Properties in {UML} Models}\n}","authorsSemantic":[9]},{"id":445,"title":"Toward an Integrated Tool Environment for Static Analysis of UML Class and Sequence Models","doi":"10.3217/jucs-016-17-2435","description":"There is a need for more rigorous analysis techniques that developers can use for verifying the critical properties in UML models. The UML-based Specification Environment (USE) tool supports verification of invariants, preconditions, and post- conditions specified in the Object Constraint Language (OCL). Due to its animation and analysis power, it is useful when checking critical non-functional properties such as security policies. However, the USE requires one to specify a model using its own textual language and does not allow one to import any model specification files created by other UML modeling tools. Hence, you would create a model with OCL constraints using a modeling tool such as the IBM Rational Software Architect (RSA) and then use the USE for the model verification. This approach, however, requires a manual transformation between two different specification formats, which diminishes advan- tage of using tools for model-level verification. In this paper, we describe our own implementation of a specification transformation engine based on the Model-Driven Architecture (MDA) framework. Our approach currently supports automatic tool-level transformations to USE from UML modeling tools built on the Eclipse-based Modeling Framework (EMF).","venue":"J. Univers. Comput. Sci.","listofauthors":"Wuliang Sun, Eunjee Song, P. Grabow, Devon M. Simmonds","citations":1,"year":2010,"publisher":"Verlag der Technischen Universität Graz","pages":null,"volume":null,"number":null,"bibtex":"@misc{https://doi.org/10.3217/jucs-016-17-2435,\n  doi = {10.3217/JUCS-016-17-2435},\n  url = {http://www.jucs.org/doi?doi=10.3217/jucs-016-17-2435},\n  author = {{Devon M. Simmonds} and {Eunjee Song} and {Paul C. Grabow} and {Wuliang Sun}},\n  title = {Toward an Integrated Tool Environment for Static Analysis of UML Class and Sequence Models},\n  publisher = {Verlag der Technischen Universität Graz},\n  year = {2010}\n}\n","authorsSemantic":[9]},{"id":446,"title":"International Seminar on Software Engineering and its Application in Education","doi":"10.5176/978-981-08-7466-7_ITCSE-54","description":"null","venue":"","listofauthors":"Emanuel S. Grant, Thomas Stokke, Hubert A. Johnson, Eunjee Song, V. Y. Sien, Y. W. Choong, Teresa Salta, Jannet T. Redoban","citations":0,"year":2010,"publisher":"Global Science and Technology Forum","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2010,\n\tdoi = {10.5176/978-981-08-7466-7_itcse-54},\n\turl = {https://doi.org/10.5176%2F978-981-08-7466-7_itcse-54},\n\tyear = 2010,\n\tmonth = {dec},\n\tpublisher = {Global Science and Technology Forum},\n\tauthor = {Emanuel S. Grant and Thomas Stokke and Hubert Johnson and Eunjee Song and Ven Yu Sien and Yeow Wei Choong and Teresa Salta and Jannet Redoban},\n\ttitle = {International Seminar on Software Engineering and its Application in Education}\n}","authorsSemantic":[9]},{"id":447,"title":"Privacy-Aware Role Based Access Control Model: Revisited for Multi-Policy Conflict Detection","doi":"10.1109/ICISA.2010.5480349","description":"Privacy-aware Role-based Access Control (P-RBAC) model is one of Role-Based Access Control (RBAC) model extensions that has been proposed to help the enforcement of privacy policies. Despite the enhanced privacy protection mechanism by P-RBAC, its pair-wise policy conflict detection algorithm has been pointed out as one of its limitations because conflicts within more than two policies are not detected. In this paper, we review and extend P-RBAC's existing conflict detection algorithm so that it can check conflicts in multiple policies. Our review includes the performance comparison on two conflict detection algorithms, triple-policy conflict detection vs. double-policy conflict detection. We find out that as the number of policies increases and the number of same condition variables in the policies increases, the running time complexity gets bigger. Especially for tripe-policy conflict, the probability of triple policy conflict also affects the complexity.","venue":"2010 International Conference on Information Science and Applications","listofauthors":"Yoonjeong Kim, Eunjee Song","citations":10,"year":2010,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2010,\n\tdoi = {10.1109/icisa.2010.5480349},\n\turl = {https://doi.org/10.1109%2Ficisa.2010.5480349},\n\tyear = 2010,\n\tpublisher = {{IEEE}},\n\tauthor = {Yoonjeong Kim and Eunjee Song},\n\ttitle = {Privacy-Aware Role Based Access Control Model: Revisited for Multi-Policy Conflict Detection}\n}","authorsSemantic":[9]},{"id":448,"title":"A property-based verification approach in aspect-oriented modeling","doi":"10.1145/1529282.1529398","description":"Aspect-oriented modeling (AOM) techniques have been advocated as solutions to support separation of crosscutting features from other application design concerns. In an AOM approach, crosscutting features are described by aspect models and other application features are described by a primary model [1]. However, composing an aspect model with a primary model can result in conflicts or compromised behaviors. Therefore, a key issue in applying the AOM approach is determining whether composition of an aspect model and a primary model produces a composed model that has desired properties. We extend the previous aspect composition approaches by France et al. [1] and Song et al. [2] by supporting a way to generate proof obligations that must be discharged in order to establish that a desired property holds in the composed class model. Fig. 1 shows an overview of our verifiable composition approach. The composition of a primary model class diagram and an aspect model class diagram (refer to the action (1) in Fig. 1) is accomplished according to a named-based composition proposed by [1]. Specifying the given property statement using the Object Constraint Language (OCL) provides the property to be verified denoted as Pprop (refer to (2)). The operation behavior in a composed model needs to be verified against this property. A proof obligation is generated and evaluated when a sequence diagram is derived from the operation specification in the composed class diagram (refer to (3)). If any faulty composition is notified during the evaluation, the current sequence diagram, which is partially derived at that point, and the current proof obligation may be used to determine at which part of the composition the property fails to hold. The information that is available when the composition stops, can be used by a developer to determine what needs to be done to correct the situation. Otherwise, a sequence diagram is obtained. For details of the action (3) in Fig. 1, refer to our earlier work in [3].","venue":"SAC '09","listofauthors":"Eunjee Song, Hanil Kim, Wuliang Sun","citations":0,"year":2009,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2009,\n\tdoi = {10.1145/1529282.1529398},\n\turl = {https://doi.org/10.1145%2F1529282.1529398},\n\tyear = 2009,\n\tpublisher = {{ACM} Press},\n\tauthor = {Eunjee Song and Hanil Kim and Wuliang Sun},\n\ttitle = {A property-based verification approach in aspect-oriented modeling}\n}","authorsSemantic":[9]},{"id":449,"title":"Model interfaces for two-way obliviousness","doi":"10.1145/1529282.1529386","description":"A key problem in software development is producing systems that are maintainable even as the concerns at play evolve. Aspect-oriented programming (AOP) seeks to foster maintainability by isolating the specifications of cross-cutting concerns, allowing them to be modified in relative isolation from the rest of the system. Research in aspect-oriented modeling (AOM) aims to develop a model-layer analogue of AOP, allowing integration with accepted modeling practices. Aspects usually allow developers of the primary model to be oblivious to the aspects that modify the primary model; because of this, aspects can be closely coupled to potentially transient details of the primary model. When those details change, the aspects that depend on them may no longer have the desired effect. In this paper, we introduce model interfaces as a solution to the problem of obliviousness by extending a graph-transformational approach to AOM.","venue":"SAC '09","listofauthors":"N. Roberts, Eunjee Song, P. Grabow","citations":1,"year":2009,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2009,\n\tdoi = {10.1145/1529282.1529386},\n\turl = {https://doi.org/10.1145%2F1529282.1529386},\n\tyear = 2009,\n\tpublisher = {{ACM} Press},\n\tauthor = {Nathan V. Roberts and Eunjee Song and Paul C. Grabow},\n\ttitle = {Model interfaces for two-way obliviousness}\n}","authorsSemantic":[9]},{"id":450,"title":"A Comparison of Aspect-Oriented Approaches to Model Driven Engineering","doi":null,"description":"null","venue":"Software Engineering Research and Practice","listofauthors":"Devon M. Simmonds, Y. R. Reddy, Eunjee Song, Emanuel S. Grant","citations":6,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":451,"title":"XMI2USE: A Tool for Transforming XMI to USE Specifications","doi":"10.1007/978-3-642-04947-7_18","description":"The UML-based Specification Environment (USE) tool supports syntactic analysis, type checking, consistency checking, and dynamic validation of invariants and pre-/post conditions specified in the Object Constraint Language (OCL). Due to its animation and analysis power, it is useful when checking critical non-functional properties such as security policies. However, the USE tool requires one to specify (i.e., \"write\") a model using its own textual language and does not allow one to import any model specification files created by other UML modeling tools. Hence, to make the best use of existing UML tools, we often create a model with OCL constraints using a modeling tool such as the IBM Rational Software Architect (RSA) and then use the USE tool for model validation. This approach, however, requires a manual transformation between the specifications of two different tool formats, which is error-prone and diminishes the benefit of automated model-level validations. In this paper, we describe our own implementation of a specification transformation engine that is based on the Model Driven Architecture (MDA) framework and currently supports automatic tool-level transformations from RSA to USE.","venue":"ER Workshops","listofauthors":"Wuliang Sun, Eunjee Song, P. Grabow, Devon M. Simmonds","citations":7,"year":2009,"publisher":"Springer Berlin Heidelberg","pages":"147-156","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-3-642-04947-7_18},\n\turl = {https://doi.org/10.1007%2F978-3-642-04947-7_18},\n\tyear = 2009,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {147--156},\n\tauthor = {Wuliang Sun and Eunjee Song and Paul C. Grabow and Devon M. Simmonds},\n\ttitle = {{XMI}2USE: A Tool for Transforming {XMI} to {USE} Specifications}\n}","authorsSemantic":[9]},{"id":452,"title":"Verifiable Aspect Composition in UML Models","doi":"10.1109/SSIRI.2008.61","description":"Aspect-oriented modeling (AOM) can achieve better separation of concerns than standard object-oriented modeling, at the cost of some additional complexity in the aspect composition specification. This complexity may obscure or, if the composition specification is incorrect, produce design errors. To facilitate correction of these errors, we describe a process by which desired features may be verified in such a way that, if the verification fails, it is relatively straightforward to identify suspect elements of the composition specification.","venue":"2008 Second International Conference on Secure System Integration and Reliability Improvement","listofauthors":"Eunjee Song, N. Roberts","citations":1,"year":2008,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2008,\n\tdoi = {10.1109/ssiri.2008.61},\n\turl = {https://doi.org/10.1109%2Fssiri.2008.61},\n\tyear = 2008,\n\tmonth = {jul},\n\tpublisher = {{IEEE}},\n\tauthor = {Eunjee Song and Nathan V. Roberts},\n\ttitle = {Verifiable Aspect Composition in {UML} Models}\n}","authorsSemantic":[9]},{"id":453,"title":"Checking Policy Enforcement in an Access Control Aspect Model","doi":null,"description":"From a software design perspective, access control policies are requirements that must be addressed in a design. For example, access control policies are constraints that determine the type of access authorized users have on information resources. In this paper, we show how one can formulate access control policies as a policy model, formulate an access control aspect model that enforces policies as an aspect, and verify whether the aspect model enforces the given policies or not. As an access control policy example, we use Role-Based Access Control (RBAC).","venue":"","listofauthors":"Eunjee Song, I. Ray, Hanil Kim","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":454,"title":"Using UML to model relational database operations","doi":"10.1016/j.csi.2006.05.006","description":"The Unified Modeling Language (UML) is being used as the de-facto standard in the software industry. With the adoption of UML 2.0, the new enhancements allow this version to describe many of the elements found in today's software technology as well as Model Driven Architecture and Service-Oriented Architecture. Although the Object Management Group (OMG) has released several UML Profiles to tailor the language to specific areas, relational database modeling is not fully addressed in these profiles. Many existing software applications involve complex application layer implemented in object-oriented programming languages and at the same time use relational database systems as the back-end data store. Modeling the whole system in a consistent manner will help developers and end users better understand the application. In this work we show how to model relational database operations using UML. Atomic database operations are modeled based on our framework and are used as building blocks to model more complex database operations.","venue":"Comput. Stand. Interfaces","listofauthors":"Eunjee Song, Shuxin Yin, I. Ray","citations":23,"year":2007,"publisher":"Elsevier BV","pages":"343-354","volume":"29","number":"3","bibtex":"@article{2007,\n\tdoi = {10.1016/j.csi.2006.05.006},\n\turl = {https://doi.org/10.1016%2Fj.csi.2006.05.006},\n\tyear = 2007,\n\tmonth = {mar},\n\tpublisher = {Elsevier {BV}},\n\tvolume = {29},\n\tnumber = {3},\n\tpages = {343--354},\n\tauthor = {Eunjee Song and Shuxin Yin and Indrakshi Ray},\n\ttitle = {Using {UML} to model relational database operations}\n}","authorsSemantic":[9]},{"id":455,"title":"Directives for Composing Aspect-Oriented Design Class Models","doi":"10.1007/11687061_3","description":"An aspect-oriented design model consists of a set of aspect models and a primary model. Each aspect model describes a feature that crosscuts elements in the primary model. Aspect and primary models are composed to obtain an integrated design view. In this paper we describe a composition approach that utilizes a merging algorithm and composition directives. Composition directives are used when the default merging algorithm is known or expected to yield incorrect models. Our prototype tool supports default class diagram composition.","venue":"LNCS Trans. Aspect Oriented Softw. Dev.","listofauthors":"Y. R. Reddy, Sudipto Ghosh, R. France, Greg Straw, J. Bieman, N. McEachen, Eunjee Song, G. Georg","citations":166,"year":2006,"publisher":"Springer Berlin Heidelberg","pages":"75-105","volume":null,"number":null,"bibtex":"@incollection{2006,\n\tdoi = {10.1007/11687061_3},\n\turl = {https://doi.org/10.1007%2F11687061_3},\n\tyear = 2006,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {75--105},\n\tauthor = {Y. R. Reddy and S. Ghosh and R. B. France and G. Straw and J. M. Bieman and N. McEachen and E. Song and G. Georg},\n\ttitle = {Directives for Composing Aspect-Oriented Design Class Models}\n}","authorsSemantic":[9]},{"id":456,"title":"Verifiable composition of access control and application features","doi":"10.1145/1063979.1064001","description":"Access control features are often spread across and tangled with other functionality in a design. This makes modifying and replacing these features in a design difficult. Aspect-oriented modeling (AOM) techniques can be used to support separation of access control concerns from other application design concerns. Using an AOM approach, access control features are described by aspect models and other application features are described by a primary model. Composition of aspect and primary models yields a design model in which access control features are integrated with other application features. In this paper, we present, through an example, an AOM approach that supports verifiable composition of behaviors described in access control aspect models and primary models. Given an aspect model, a primary model, and a specified property, the composition technique produces proof obligations as the behavioral descriptions in the aspect and primary models are composed. One has to discharge the proof obligations to establish that the composed model has the specified property.","venue":"SACMAT '05","listofauthors":"Eunjee Song, Y. R. Reddy, R. France, I. Ray, G. Georg, R. T. Alexander","citations":47,"year":2005,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2005,\n\tdoi = {10.1145/1063979.1064001},\n\turl = {https://doi.org/10.1145%2F1063979.1064001},\n\tyear = 2005,\n\tpublisher = {{ACM} Press},\n\tauthor = {Eunjee Song and Raghu Reddy and Robert France and Indrakshi Ray and Geri Georg and Roger Alexander},\n\ttitle = {Verifiable composition of access control and application features}\n}","authorsSemantic":[9]},{"id":458,"title":"Model Composition Directives","doi":"10.1007/978-3-540-30187-5_7","description":"An aspect-oriented design model consists of a set of aspect models and a primary model. Each of these models consists of a number of different kinds of UML diagrams. The models must be composed to identify conflicts and analyze the system as a whole. We have developed a systematic approach for composing class diagrams in which a default composition procedure based on name matching can be customized by user-defined composition directives. This paper describes a set of composition directives that constrain how class diagrams are composed.","venue":"UML","listofauthors":"Greg Straw, G. Georg, Eunjee Song, Sudipto Ghosh, R. France, J. Bieman","citations":88,"year":2004,"publisher":"Springer Berlin Heidelberg","pages":"84-97","volume":null,"number":null,"bibtex":"@incollection{2004,\n\tdoi = {10.1007/978-3-540-30187-5_7},\n\turl = {https://doi.org/10.1007%2F978-3-540-30187-5_7},\n\tyear = 2004,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {84--97},\n\tauthor = {Greg Straw and Geri Georg and Eunjee Song and Sudipto Ghosh and Robert France and James M. Bieman},\n\ttitle = {Model Composition Directives}\n}","authorsSemantic":[9]},{"id":459,"title":"A UML-based pattern specification technique","doi":"10.1109/TSE.2004.1271174","description":"Informally described design patterns are useful for communicating proven solutions for recurring design problems to developers, but they cannot be used as compliance points against which solutions that claim to conform to the patterns are checked. Pattern specification languages that utilize mathematical notation provide the needed formality, but often at the expense of usability. We present a rigorous and practical technique for specifying pattern solutions expressed in the unified modeling language (UML). The specification technique paves the way for the development of tools that support rigorous application of design patterns to UML design models. The technique has been used to create specifications of solutions for several popular design patterns. We illustrate the use of the technique by specifying observer and visitor pattern solutions.","venue":"IEEE Transactions on Software Engineering","listofauthors":"R. France, Dae-Kyoo Kim, Sudipto Ghosh, Eunjee Song","citations":326,"year":2004,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"193-206","volume":"30","number":"3","bibtex":"@article{2004,\n\tdoi = {10.1109/tse.2004.1271174},\n\turl = {https://doi.org/10.1109%2Ftse.2004.1271174},\n\tyear = 2004,\n\tmonth = {mar},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {30},\n\tnumber = {3},\n\tpages = {193--206},\n\tauthor = {R.B. France and  Dae-Kyoo Kim and S. Ghosh and  Eunjee Song},\n\ttitle = {A {UML}-based pattern specification technique}\n}","authorsSemantic":[9]},{"id":460,"title":"A UML-Based Metamodeling Language to Specify Design Patterns","doi":null,"description":"A design pattern describes a generic solution for problems that occur repeatedly. Current descriptions of design patterns describe solutions with graphical notation and complementing text. To encourage the use of design patterns, the development of pattern supporting tools is imperative. This requires design patterns to be specified precisely. There has been considerable work done on pattern specifications. They suffer from either complication or lack of formality and features. In this paper, we describe a metamodeling technique to specify design patterns that is formal and easy to understand and use. The technique uses the Role-Based Metamodeling Language (RBML) based on the UML. The RBML is able to capture various design perspectives of patterns such as static structure, interactions, and state-based behavior. We give an overview of the RBML and demonstrate the technique using the Iterator design pattern. We show how pattern specifications can be used for checking pattern conformance using the model of a television remote control application.","venue":"","listofauthors":"Dae-Kyoo Kim, Sudipto Ghosh, Eunjee Song","citations":53,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":461,"title":"A Metamodeling Approach to Pattern-Based Model Refactoring","doi":"10.1109/MS.2003.1231152","description":"Design patterns capture development solutions to design problems in forms that make the designs more modular, modifiable, reusable, and understandable. This metamodeling approach to pattern-based refactoring of design models incorporates the precise specification of design patterns and transformation rules.","venue":"IEEE Softw.","listofauthors":"R. France, Sudipto Ghosh, Eunjee Song, Dae-Kyoo Kim","citations":92,"year":2003,"publisher":"Institute of Electrical and Electronics Engineers (IEEE)","pages":"52-58","volume":"20","number":"5","bibtex":"@article{2003,\n\tdoi = {10.1109/ms.2003.1231152},\n\turl = {https://doi.org/10.1109%2Fms.2003.1231152},\n\tyear = 2003,\n\tmonth = {sep},\n\tpublisher = {Institute of Electrical and Electronics Engineers ({IEEE})},\n\tvolume = {20},\n\tnumber = {5},\n\tpages = {52--58},\n\tauthor = {R. France and S. Chosh and E. Song and D.K. Kim},\n\ttitle = {A metamodeling approach to pattern-based model refactoring}\n}","authorsSemantic":[9]},{"id":462,"title":"Using Roles to Characterize Model Families","doi":"10.1007/978-94-017-2740-2_9","description":"The development of reusable requirements and design artifacts often requires one to characterize families of problem and solution models. This paper presents a metamodeling approach to characterizing a family of models. A characterization is expressed as a Role Model that consists of roles that can be played by UML model elemets. In this paper we describe how a family of UML static structural diagrams that have the structural properties defined by a pattern can be characterized by a Static Role Model (SRM). The Abstract Factory pattern is used to illustrate how SRMs can be used to specify reusable designs expressed as patterns.","venue":"","listofauthors":"Dae-Kyoo Kim, Eunjee Song, Sudipto Ghosh","citations":31,"year":2003,"publisher":"Springer Netherlands","pages":"179-195","volume":null,"number":null,"bibtex":"@incollection{2003,\n\tdoi = {10.1007/978-94-017-2740-2_9},\n\turl = {https://doi.org/10.1007%2F978-94-017-2740-2_9},\n\tyear = 2003,\n\tpublisher = {Springer Netherlands},\n\tpages = {179--195},\n\tauthor = {Robert B. France and Dae-Kyoo Kim and Eunjee Song and Sudipto Ghosh},\n\ttitle = {Using Roles to Characterize Model Families}\n}","authorsSemantic":[9]},{"id":463,"title":"A role-based metamodeling approach to specifying design patterns","doi":"10.1109/CMPSAC.2003.1245379","description":"Design patterns describe solutions to recurring design problems in the development of software designs. To encourage the use of design patterns, we are investigating tool support for incorporating patterns into UML models. The development of such tools requires patterns to be specified at the metamodel level. Patterns may be specified using roles, where a role is played by model elements. However, the notion of role in the object-oriented community is strictly based on objects, and does not allow the use of the word \"role\" in any other place where the context is not object-based. In this paper, we propose a notion of role that can be used to specify design patterns at the metamodel level. We survey the characteristics of object-based roles and generalize them. Based on the generalized notion of a role define a new notion of a model role which is played by a model element. We illustrate the use of model roles with a specification of a variant of the Observer design pattern.","venue":"Proceedings 27th Annual International Computer Software and Applications Conference. COMPAC 2003","listofauthors":"Dae-Kyoo Kim, R. France, Sudipto Ghosh, Eunjee Song","citations":61,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/cmpsac.2003.1245379},\n\turl = {https://doi.org/10.1109%2Fcmpsac.2003.1245379},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {Dae-Kyoo Kim and R. France and S. Ghosh and  Eunjee Song},\n\ttitle = {A role-based metamodeling approach to specifying design patterns}\n}","authorsSemantic":[9]},{"id":465,"title":"Using Role-Based Modeling Language (RBML) to characterize model families","doi":"10.1109/ICECCS.2002.1181503","description":"Cost-effective development of large, integrated computer-based systems can be realized through systematic reuse of development experiences throughout the development process. We describe a technique for representing reusable modeling experiences. The technique allows developers to express domain-specific design patterns as a sub-language of the modeling language, the UML. Use of the sub-language to build application-specific UML models results in the reuse of the embedded design experiences. We use a notation called the (meta)Role-Based Modeling Language (RBML) to define UML sub-languages. A (meta-)Role Model is a specialization of the UML (Unified Modeling Language) meta-model, that is, it determines a sub-language of the UML. We show how RBML can be used to define domain-specific design patterns.","venue":"Eighth IEEE International Conference on Engineering of Complex Computer Systems, 2002. Proceedings.","listofauthors":"Dae-Kyoo Kim, R. France, Sudipto Ghosh, Eunjee Song","citations":29,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/iceccs.2002.1181503},\n\turl = {https://doi.org/10.1109%2Ficeccs.2002.1181503},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {Dae-Kyoo Kim and R. France and S. Ghosh and  Eunjee Song},\n\ttitle = {Using Role-Based Modeling Language ({RBML}) to characterize model families}\n}","authorsSemantic":[9]},{"id":466,"title":"Metarole-Based Modeling Language (RBML) Specification V1. 0","doi":null,"description":"A protective bag for placing over fruit, vegetables, and entire small plants. The bag is fabricated from a perforated material passing sunlight, water, and air, but having perforations sufficiently small to exclude insects. The bag is closed by a liner at the neck of the bag of beeswax, metal foil, or any material which is malleable and holds its configuration after pinching or other manipulation. This characteristic enables the bag to have an opening of any size less than that of the fully open neck, to locate the opening anywhere along the neck, and to be closed when the bag is placed over a fruit. Optionally, the bag is colored with a hue selected to discourage a selected insect pest.","venue":"","listofauthors":"Dae-Kyoo Kim, Eunjee Song","citations":11,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":467,"title":"Using Roles for Pattern-Based Model Refactoring","doi":null,"description":"Design patterns capture some of the best software development experiences in forms that are intended to facilitate reuse. We treat a design pattern as a characterization of a family of solutions, where the solutions are expressed as UML (Unified Modeling Language) design models. We present a new notation that we call Role Models to characterize pattern solutions, and describe how they can be used to support systematic pattern-based model refactoring.","venue":"","listofauthors":"Eunjee Song, R. France, Dae-Kyoo Kim, Sudipto Ghosh","citations":11,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":468,"title":"Using Role Models as Precise Characterizations of Model Families","doi":null,"description":"Cost-effective development of large, integrated computerbased systems can be realized through systematic reuse practices which require the creation of reusable artifacts. Such artifacts can be obtained from all phases of software development. In our work, we focus on artifacts that describe software design models. The development of reusable design models requires one to characterize families of problems and their solutions. We use a notation that we call Role Models to characterize families of UML design models. A Role Model is a specialization of the UML (Unified Modeling Language) metamodel, that is, it is a sub-language of the UML. We show how Role Models can describe structural and behavioral properties of model families. We illustrate the use of our Role Models to describe a family of designs for checkin-checkout systems.","venue":"","listofauthors":"Sudipto Ghosh, Dae-Kyoo Kim, R. France, Eunjee Song","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":469,"title":"객체지향 언어의 구현 및 규칙기반 시스템의 설계와 구현에 관한 연구 = A study on the implementation of object oriented language and the design of rule-based system","doi":null,"description":"null","venue":"","listofauthors":"송은강, Eunjee Song","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":470,"title":"2 Related Work 2 . 1 Related Work on UML-based Pattern Formalization","doi":null,"description":"This paper presents a metamodeling approach to precisely expressing pattern properties. In this work, a pattern is treated as a characterization of a family of solutions, where the solutions are expressed as UML (Unified Modeling Language) design models. This treatment of patterns paves the way for the development of pattern-based model refactoring techniques.","venue":"","listofauthors":"R. France, Dae-Kyoo Kim, Eunjee Song, Sudipto Ghosh","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":471,"title":"Pattern-based Model Refactoring a Metamodeling Approach to Pattern-based Model Refactoring Our Approach","doi":null,"description":"design pattern is one way to transform a model. For example, a developer might modify an existing design using a pattern to produce a target model in which the pattern is realized. 1,2 This article shows how to define and apply design patterns in the context of model transformations. The design models we discuss, both source and target, are expressed in the Unified Modeling Language. 3 The process of transforming a model using a design pattern is called pattern-based refac-toring. We can achieve rigorous pattern-based refactoring by developing metamodels called transformation specifications that characterize families of transformations. Or, you might think of metamodels as defining a transformation language; the characterized transformations are \" sentences \" in this language. The meta-models act as points against which we can check model transformations for conformance. Figure 1 shows an example of pattern-based model refactoring using the Gang of Four's Bridge pattern. 1 (The Gang, also known as T he success of model-driven architecture methods hinges on the support they provide for three things: creating meaningful models in a timely way; specifying and applying patterns that reflect useful and reusable design abstractions; and defining and systematically applying model transformations that support model evolution, refinement, and code generation. Model transformation occurs when we modify a source model to produce a target model. Applying a well-defined model-driven development Design patterns capture some of the best software development solutions to design problems in forms that are intended to facilitate reuse. The authors' metamodeling approach to pattern-based refactoring incorporates the precise specification of design patterns and transformation rules. Their example uses the Abstract Factory pattern and a small UML model. how to separate an abstraction from its implementations so that you can vary the abstractions and implementations independently. The UML design model in Figure 1 has a Display class associated with an ImageImpl1 class that defines a class of image implementations. This model is refactored so that the Display class is now associated with an Image class structure that allows image implementations linked to the display to be varied at runtime. The refactoring shown in Figure 1 might proceed as follows: 1. Identify the design elements in the source model that will participate in the pattern solution. Modify them so that they can participate as the pattern specifies. In this case, we identify the Display class as the client (not shown in Figure 2) using the Abstraction, and …","venue":"","listofauthors":"E. Gamma, Richard Helm, Ralph E. Johnson, John Vlissides The, R. France, Sudipto Ghosh, Eunjee Song, Dae-Kyoo Kim","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[9]},{"id":472,"title":"Semantic Atomicity","doi":"10.1007/978-1-4614-8265-9_720","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"G. Speegle","citations":0,"year":2018,"publisher":"Springer New York","pages":"3381-3386","volume":null,"number":null,"bibtex":"@incollection{2018,\n\tdoi = {10.1007/978-1-4614-8265-9_720},\n\turl = {https://doi.org/10.1007%2F978-1-4614-8265-9_720},\n\tyear = 2018,\n\tpublisher = {Springer New York},\n\tpages = {3381--3386},\n\tauthor = {Greg Speegle},\n\ttitle = {Semantic Atomicity}\n}","authorsSemantic":[10]},{"id":473,"title":"Compensating Transactions","doi":"10.1007/978-1-4614-8265-9_735","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"G. Speegle","citations":0,"year":2018,"publisher":"Springer New York","pages":"528-529","volume":null,"number":null,"bibtex":"@incollection{2018,\n\tdoi = {10.1007/978-1-4614-8265-9_735},\n\turl = {https://doi.org/10.1007%2F978-1-4614-8265-9_735},\n\tyear = 2018,\n\tpublisher = {Springer New York},\n\tpages = {528--529},\n\tauthor = {Greg Speegle},\n\ttitle = {Compensating Transactions}\n}","authorsSemantic":[10]},{"id":476,"title":"Signaling pathway prediction by path frequency in protein-protein interaction networks","doi":"10.1109/BIBM.2013.6732602","description":"A signaling pathway, which is represented as a chain of interacting proteins for a biological process, can be predicted from protein-protein interaction (PPI) networks. However, pathway prediction is computationally challenging because of (1) inefficiency in searching all possible paths from the large-scale PPI networks and (2) unreliability of current PPI data generated by automated high-throughput methods. In this paper, we propose a novel approach to efficiently predict signaling pathways from PPI networks when a starting protein (source) and an ending protein (target) are given. Our approach is a combination of topological analysis of the networks and ontological analysis of interacting proteins. Starting from the source, this method repeatedly extends the list of proteins to form a pathway based on the improved support model (iSup). This model integrates (1) the frequency of the paths towards the target and (2) the semantic similarity between each adjacent pair in a pathway. The path frequency is computed by a heuristic data-mining technique to determine the most frequent paths towards the target in a PPI network. The semantic similarity is measured by the distance of the information contents of Gene Ontology (GO) terms annotating interacting proteins. To further improve computational efficiency, we propose two additional strategies: filtering the PPI networks and precomputing approximate path frequency. The experiment with the yeast PPI data demonstrates that our approach predicted MAPK signaling pathways with higher accuracy and efficiency than other existing methods.","venue":"2013 IEEE International Conference on Bioinformatics and Biomedicine","listofauthors":"Yilan Bai, G. Speegle, Young-Rae Cho","citations":1,"year":2013,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{2013,\n\tdoi = {10.1109/bibm.2013.6732602},\n\turl = {https://doi.org/10.1109%2Fbibm.2013.6732602},\n\tyear = 2013,\n\tmonth = {dec},\n\tpublisher = {{IEEE}},\n\tauthor = {Yilan Bai and Greg Speegle and Young-Rae Cho},\n\ttitle = {Signaling pathway prediction by path frequency in protein-protein interaction networks}\n}","authorsSemantic":[10]},{"id":477,"title":"Managing and Mentoring Capstone Design Teams: Considerations and Practices for Faculty*","doi":null,"description":"This paper presents the findings from a panel session at the 2010 Capstone Design Conference in Boulder, Colorado in which panelists and participants had a lively discussion about practices associated with managing and mentoring student teams.The three broad topics discussed at the sessionwere themethods of assigning teams, product versus process learning objectives for design teams, and non-technical aspects of team performance (e.g. race and gender dynamics, professional and interpersonal communication). For each topic, the paper describes the wide variety of views and approaches (some contradictory) that were explored regarding each topic, as well as the factors affecting choice of approach. In addition, the paper highlights three themes that recurred across the topics: 1) clear learning objectives for capstone or any project-based activity are central to effectively designing andmentoring teams; 2) faculty participants do care deeply about their students and take steps to act in ways that benefit students, and 3) both positive and negative aspects of student attitudes and behaviors may reflect faculty attitudes and behaviors, implying that we should examine and act to improve our departmental cultures if we hope to affect student performance. The results of this discussion point strongly to the need for more research on teaming in capstone courses to better understand the relationships among curricular environment, student development, and learning outcomes.","venue":"","listofauthors":"M. Paretti, R. Layton, S. Laguette, G. Speegle","citations":39,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":478,"title":"Computer Human Interaction (CHI)","doi":"10.1007/978-0-387-39940-9_2248","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, B. Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":2,"year":2009,"publisher":"Springer US","pages":"432-432","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2248},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2248},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {432--432},\n\ttitle = {Computer Human Interaction ({CHI})}\n}","authorsSemantic":[10]},{"id":479,"title":"Cooperative Content Distribution","doi":"10.1007/978-0-387-39940-9_2299","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":0,"year":2009,"publisher":"Springer US","pages":"495-495","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2299},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2299},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {495--495},\n\ttitle = {Cooperative Content Distribution}\n}","authorsSemantic":[10]},{"id":480,"title":"Connectionist Model","doi":"10.1007/978-0-387-39940-9_2274","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":14,"year":2009,"publisher":"Springer US","pages":"449-449","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2274},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2274},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {449--449},\n\ttitle = {Connectionist Model}\n}","authorsSemantic":[10]},{"id":481,"title":"Statistical Disclosure Control (SDC)","doi":"10.1007/978-0-387-39940-9_3685","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"R. Topor, K. Salem, Amarnath Gupta, K. Goda, J. Gehrke, N. Palmer, M. Sharaf, A. Labrinidis, J. Roddick, A. Fuxman, Renée J. Miller, Wang-Chiew Tan, Anastasios Kementsietsidis, Philippe Bonnet, D. Shasha, R. Peikert, Bertram Ludäscher, S. Bowers, T. McPhillips, Harald Naumann, K. Voruganti, J. Domingo-Ferrer, Ben Carterette, Panagiotis G. Ipeirotis, M. Arenas, Y. Manolopoulos, Y. Theodoridis, V. Tsotras, B. Carminati, J. Jurjens, E. Fernandez, Murat Kantarcioglu, Jaideep Vaidya, I. Ray, A. Vakali, C. Sirangelo, E. Pitoura, H. Gupta, S. Chaudhuri, G. Weikum, U. Leser, D. Embley, F. Giunchiglia, P. Shvaiko, Mikalai Yatskevich, E. Chang, C. Parent, S. Spaccapietra, E. Zimányi, G. Anadiotis, S. Kotoulas","citations":0,"year":2009,"publisher":"Springer US","pages":"2782-2782","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_3685},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_3685},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {2782--2782},\n\ttitle = {Statistical Disclosure Control ({SDC})}\n}","authorsSemantic":[10]},{"id":482,"title":"Semantic Atomicity","doi":"10.1007/978-0-387-39940-9_720","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"G. Speegle","citations":0,"year":2009,"publisher":"Springer US","pages":"2588-2591","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_720},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_720},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {2588--2591},\n\tauthor = {Greg Speegle},\n\ttitle = {Semantic Atomicity}\n}","authorsSemantic":[10]},{"id":483,"title":"Cross-media Information Retrieval","doi":"10.1007/978-0-387-39940-9_2323","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":0,"year":2009,"publisher":"Springer US","pages":"528-528","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2323},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2323},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {528--528},\n\ttitle = {Cross-media Information Retrieval}\n}","authorsSemantic":[10]},{"id":484,"title":"Correlation Clustering","doi":"10.1007/978-0-387-39940-9_2309","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":23,"year":2009,"publisher":"Springer US","pages":"506-506","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2309},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2309},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {506--506},\n\ttitle = {Correlation Clustering}\n}","authorsSemantic":[10]},{"id":485,"title":"Cache Performance","doi":"10.1007/978-0-387-39940-9_2150","description":" Caches take advantage of locality to speed up most data accesses. — Increasing the block size can take advantage of spatial locality. — Increasing cache associativity helps reduce the miss rate.  Today we'll finish up with associativity and do two more things. — We'll try to quantify the benefits of different cache designs, and see how caches affect overall performance. — We'll also investigate some main memory organizations that can help increase memory system performance.  Next Monday we'll introduce some of the issues involved with writing to caches, and talk about cache configurations in modern processors.","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":1,"year":2009,"publisher":"Springer US","pages":"301-301","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2150},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2150},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {301--301},\n\ttitle = {Cache Performance}\n}","authorsSemantic":[10]},{"id":486,"title":"Conceptual Data Model","doi":"10.1007/978-0-387-39940-9_2255","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":3,"year":2009,"publisher":"Springer US","pages":"437-437","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2255},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2255},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {437--437},\n\ttitle = {Conceptual Data Model}\n}","authorsSemantic":[10]},{"id":487,"title":"Conditional Routing","doi":"10.1007/978-0-387-39940-9_2271","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":12,"year":2009,"publisher":"Springer US","pages":"446-446","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2271},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2271},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {446--446},\n\ttitle = {Conditional Routing}\n}","authorsSemantic":[10]},{"id":488,"title":"Secret-Key Encryption","doi":"10.1007/978-0-387-39940-9_3544","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"R. Topor, K. Salem, Amarnath Gupta, K. Goda, J. Gehrke, N. Palmer, M. Sharaf, A. Labrinidis, J. Roddick, A. Fuxman, Renée J. Miller, Wang-Chiew Tan, Anastasios Kementsietsidis, Philippe Bonnet, D. Shasha, R. Peikert, Bertram Ludäscher, S. Bowers, T. McPhillips, Harald Naumann, K. Voruganti, J. Domingo-Ferrer, Ben Carterette, Panagiotis G. Ipeirotis, M. Arenas, Y. Manolopoulos, Y. Theodoridis, V. Tsotras, B. Carminati, J. Jurjens, E. Fernández, Murat Kantarcioglu, Jaideep Vaidya, I. Ray, A. Vakali, C. Sirangelo, E. Pitoura, H. Gupta, S. Chaudhuri, G. Weikum, U. Leser, D. Embley, F. Giunchiglia, P. Shvaiko, Mikalai Yatskevich, Edward Y. Chang, C. Parent, S. Spaccapietra, E. Zimányi, G. Anadiotis, S. Kotoulas","citations":1,"year":2009,"publisher":"Springer US","pages":"2523-2523","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_3544},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_3544},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {2523--2523},\n\ttitle = {Secret-Key Encryption}\n}","authorsSemantic":[10]},{"id":489,"title":"Current Time","doi":"10.1007/978-0-387-39940-9_2330","description":"Correspondence Address: (57) ABSTRACT CARLSON, GASKEY & OLDS, P.C. 4OO WEST MAPLE ROAD SUTE 350 BIRMINGHAM, MI 48009 (US) Brushed DC electric motors have a rotor with commutator portions moving into and out of contact with brushes. As each of these contacts end, a back EMF force is induced into a main current Supply signal. The number of these periodic (73) Assignee: Masco Corporation forces can be counted and utilized to identify the position of (21) Appl. No.: 11/134,925 the component being rotated by the DC electric motor. This method is relatively insensitive to environmental changes, (22) Filed: May 23, 2005 and thus more accurate than the existing art.","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":0,"year":2009,"publisher":"Springer US","pages":"545-545","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2330},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2330},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {545--545},\n\ttitle = {Current Time}\n}","authorsSemantic":[10]},{"id":490,"title":"Compensating Transactions","doi":"10.1007/978-0-387-39940-9_735","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"G. Speegle","citations":0,"year":2009,"publisher":"Springer US","pages":"405-406","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_735},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_735},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {405--406},\n\tauthor = {Greg Speegle},\n\ttitle = {Compensating Transactions}\n}","authorsSemantic":[10]},{"id":491,"title":"Service Choreography","doi":"10.1007/978-0-387-39940-9_3590","description":"Service choreography is a form of service composition in which the interaction protocol between several partner services is defined from a global perspective[1] . The intuition underlying the notion of service choreography can be summarised as follows: “Dancers dance following a global scenario without a single point of control\" That is, at run-time each participant in a service choreography executes its part of it (i.e. its role) according to the behavior of the other participants[2] . A choreography's role specifies the expected messaging behavior of the participants that will play it in terms of the sequencing and timing of the messages that they can consume and produce[3] .","venue":"Encyclopedia of Database Systems","listofauthors":"R. Topor, K. Salem, Amarnath Gupta, K. Goda, J. Gehrke, N. Palmer, Mohamed A. Sharaf, A. Labrinidis, J. Roddick, A. Fuxman, Renée J. Miller, W. Tan, Anastasios Kementsietsidis, Philippe Bonnet, D. Shasha, R. Peikert, Bertram Ludäscher, S. Bowers, T. McPhillips, Harald Naumann, K. Voruganti, J. Domingo-Ferrer, Ben Carterette, Panagiotis G. Ipeirotis, M. Arenas, Y. Manolopoulos, Y. Theodoridis, V. Tsotras, B. Carminati, J. Jurjens, Eduardo B. Fernandez, Murat Kantarcioglu, Jaideep Vaidya, I. Ray, A. Vakali, C. Sirangelo, E. Pitoura, H. Gupta, S. Chaudhuri, G. Weikum, U. Leser, D. Embley, F. Giunchiglia, P. Shvaiko, Mikalai Yatskevich, Edward Y. Chang, C. Parent, S. Spaccapietra, E. Zimányi, G. Anadiotis, S. Kotoulas","citations":0,"year":2009,"publisher":"Springer US","pages":"2632-2632","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_3590},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_3590},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {2632--2632},\n\ttitle = {Service Choreography}\n}","authorsSemantic":[10]},{"id":492,"title":"Conflict Serializability","doi":"10.1007/978-0-387-39940-9_2273","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":0,"year":2009,"publisher":"Springer US","pages":"448-448","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2273},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2273},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {448--448},\n\ttitle = {Conflict Serializability}\n}","authorsSemantic":[10]},{"id":493,"title":"Storage Array","doi":"10.1007/978-0-387-39940-9_3692","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"R. Topor, K. Salem, Amarnath Gupta, K. Goda, J. Gehrke, N. Palmer, M. Sharaf, A. Labrinidis, J. Roddick, A. Fuxman, Renée J. Miller, Wang-Chiew Tan, Anastasios Kementsietsidis, Philippe Bonnet, D. Shasha, R. Peikert, Bertram Ludäscher, S. Bowers, T. McPhillips, Harald Naumann, K. Voruganti, J. Domingo-Ferrer, Ben Carterette, Panagiotis G. Ipeirotis, M. Arenas, Y. Manolopoulos, Y. Theodoridis, V. Tsotras, B. Carminati, J. Jurjens, E. Fernandez, Murat Kantarcioglu, Jaideep Vaidya, I. Ray, A. Vakali, C. Sirangelo, E. Pitoura, H. Gupta, S. Chaudhuri, G. Weikum, U. Leser, D. Embley, F. Giunchiglia, P. Shvaiko, Mikalai Yatskevich, E. Chang, C. Parent, S. Spaccapietra, E. Zimányi, G. Anadiotis, S. Kotoulas","citations":0,"year":2009,"publisher":"Springer US","pages":"2798-2798","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_3692},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_3692},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {2798--2798},\n\ttitle = {Storage Array}\n}","authorsSemantic":[10]},{"id":494,"title":"Designing a Capstone Course to Simulate the Industrial Environment","doi":null,"description":"null","venue":"","listofauthors":"G. Speegle","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":495,"title":"Conceptual Modeling for Spatio-Temporal Applications","doi":"10.1007/978-0-387-39940-9_2261","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":1,"year":2009,"publisher":"Springer US","pages":"438-438","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2261},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2261},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {438--438},\n\ttitle = {Conceptual Modeling for Spatio-Temporal Applications}\n}","authorsSemantic":[10]},{"id":496,"title":"SQL: Practical Guide for Developers","doi":null,"description":"Chapter 1: Databasics Chapter 2: Single Table Retrieval Chapter 3: Taming Tables Chapter 4: Aggregating Results Chapter 5: Multiple Table Queries using Simple Subqueries Chapter 6: Multiple Table Queries Using Joins Chapter 7: Set Based Queries Chapter 8: Advanced Subqueries Chapter 9: Creating a Database Chapter 10: Database Data Chapter 11: Transaction Management Chapter 12: Authorization Chapter 13: Advanced Topics Chapter 14: SQL Programming","venue":"","listofauthors":"M. J. Donahoo, G. Speegle","citations":6,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":497,"title":"An E-commerce Example","doi":"10.1016/B978-155860736-1/50036-8","description":"This chapter describes methods to use JDBC in Internet applications, specifically e-commerce. It discusses applets, then moves on to the more realistic multitiered environment. Applets are small Java programs that are executed in a browser. However, there are problems with using JDBC in an applet environment. One category of these problems deals with security, and another deals with technical issues. Next, the chapter discusses Multitiered Architecture Example. One of the advantages of a multitiered system is that it solves problems at three different levels. It can handle data presentation with an applet, business logic with a servlet, and database access with PL/SQL. The only important issue is how to communicate between the components. Communication between the servlet and the database is the same as the use of CallableStatements. The applet sends information to the servlet using the HTTP protocol. Essentially, it constructs the URL of the servlet, including parameters. This invokes the doGetO method in the servlet. The servlet sends information back to the applet via a binary socket connection. This allows the servlet to send serialized objects to the applet, which greatly increases the sophistication of the reply. In order to simplify the applet code, a class called Communication exists to handle the basic issues.","venue":"","listofauthors":"G. Speegle","citations":0,"year":2002,"publisher":"Elsevier","pages":"73-96","volume":null,"number":null,"bibtex":"@incollection{2002,\n\tdoi = {10.1016/b978-155860736-1/50036-8},\n\turl = {https://doi.org/10.1016%2Fb978-155860736-1%2F50036-8},\n\tyear = 2002,\n\tpublisher = {Elsevier},\n\tpages = {73--96},\n\tauthor = {Gregory D. Speegle},\n\ttitle = {An E-commerce Example}\n}","authorsSemantic":[10]},{"id":498,"title":"Updating the Database","doi":"10.1016/B978-155860736-1/50034-4","description":"Within SQL, the three commands to modify the database are Update, Insert, and Delete. JDBC handles the three cases in a similar manner. Performing basic update operations is almost identical to performing queries; however, there are two differences. Firstthe SQL command must be Update, Insert, or Delete, and second, the Statement method executeUpdate() is used instead of executeQuery(). PreparedStatements can also be used to update the database. The PreparedStatement object is created in exactly the same way; only the SQL statement is an Insert, Update, or Delete statement. This chapter discusses a simple update program to add tapes to an inventory one at a time; a sophisticated update mechanism that allows several updates to be processed at once; and the update mode for JDBC 2.0 ResultSets to have a customer return a tape.","venue":"","listofauthors":"G. Speegle","citations":1,"year":2002,"publisher":"Elsevier","pages":"43-53","volume":null,"number":null,"bibtex":"@incollection{2002,\n\tdoi = {10.1016/b978-155860736-1/50034-4},\n\turl = {https://doi.org/10.1016%2Fb978-155860736-1%2F50034-4},\n\tyear = 2002,\n\tpublisher = {Elsevier},\n\tpages = {43--53},\n\tauthor = {Gregory D. Speegle},\n\ttitle = {Updating the Database}\n}","authorsSemantic":[10]},{"id":499,"title":"Chapter 7 – How to Stay Current with JDBC","doi":"10.1016/B978-155860736-1/50037-X","description":"This chapter discusses JDBC 3.0, javax.sql Package, java.sql, Java Server Pages (JSPs), and security. JDBC 3.0 is to round out the API by filling in smaller areas of missing functionality. As such, it does not have a major impact on an introductory work; however, further work almost certainly requires JDBC 3.0. Basic JDBC functionality is not changed by the JDBC 3.0 API; however, certain desirable features are available. Further, the chapter discusses javax.sql, much of which is based on using general DataSource objects instead of just databases. In fact, a DataSource object is similar to the DriverManager class in the java.sql package. As SQL has continued to expand, one of the improvements is its ability to represent more complex data. Specifically, there is an SQL type called ARRAY. The ARRAY type allows arrays to be stored as elements in a row. Another expanded data type captured in java.sql is the character large object (CLOB) interface. CLOBs are useful for storing very large character constructs. Next, the chapter discusses JSPs that are designed for dynamically generated Web content. A single JSP file contains both Java and HTML. When the file is requested, the web server executes the Java and outputs the created HTML. Finally, the chapter discusses security that is one of the most crucial aspects of connecting a database to the Internet. Security applies both to information sent from the user to the database, and to the database itself.","venue":"","listofauthors":"G. Speegle","citations":0,"year":2002,"publisher":"Elsevier","pages":"97-100","volume":null,"number":null,"bibtex":"@incollection{2002,\n\tdoi = {10.1016/b978-155860736-1/50037-x},\n\turl = {https://doi.org/10.1016%2Fb978-155860736-1%2F50037-x},\n\tyear = 2002,\n\tpublisher = {Elsevier},\n\tpages = {97--100},\n\tauthor = {Gregory D. Speegle},\n\ttitle = {How to Stay Current with {JDBC}}\n}","authorsSemantic":[10]},{"id":500,"title":"Introduction to JDBC","doi":"10.1016/B978-155860736-1/50031-9","description":"JDBC is an API defined in the java.sql and javax.sql packages for connecting an arbitrary database to a Java program. This chapter uses an analogy to understand the basic working of JDBC. It supposes that one operates a small business that sells gadgets. The gadgets are produced in a factory, across a river from the store. Without a boat or a bridge, there is no way to get to the factory or for the factory to deliver goods to the store. This represents the situation with a database and a Java program: the Java program is the store, and the database is the factory. Without something to help, there is no way for the Java program to make requests of the database, and the database cannot communicate with the Java program. It would be logical to build a bridge over the river to transport the products from the factory to the store. The analogous piece of software for the Java program and database is an interface called the driver. The driver is loaded by setting the jdbc.drivers property of the Java virtual machine (JVM). As modern rivers are crossed by many bridges, there are many drivers to connect Java programs to databases.","venue":"","listofauthors":"G. Speegle","citations":2,"year":2002,"publisher":"Elsevier","pages":"1-14","volume":null,"number":null,"bibtex":"@incollection{2002,\n\tdoi = {10.1016/b978-155860736-1/50031-9},\n\turl = {https://doi.org/10.1016%2Fb978-155860736-1%2F50031-9},\n\tyear = 2002,\n\tpublisher = {Elsevier},\n\tpages = {1--14},\n\tauthor = {Gregory D. Speegle},\n\ttitle = {Introduction to {JDBC}}\n}","authorsSemantic":[10]},{"id":501,"title":"Advanced JDBC Topics","doi":"10.1016/B978-155860736-1/50035-6","description":"This chapter covers four topics, namely JDBC drivers, metadata, binary large objects, and transactions. The DriverManager selects the appropriate driver from the ones that are already loaded. Another parameter required to make a connection to the database is called URL in the table. It defines the protocol for the Driver. All three drivers use “jdbc:” as the protocol. All URLs for JDBC drivers should start with “jdbc:” Databases allow users to define tables, keys, domains, columns, and more. The only way a database can manage data without knowing the schema in advance is to have metadata,that is, data about the data. The metadata describes the data in a consistent format. This allows the database to manage the data through the description rather than through the actual data. Metadata is also used within JDBC to help manage data. Under the default execution of the JDBC API, whenever a Statement object invokes the executeQuery(), execute Update( ), or execute() methods, a transaction starts at the database. If the statement executes normally, the transaction commits. If an exception is thrown, the transaction aborts. Finally, this chapter discusses Blob, which stands for “binary large object,” and can be anything from images to sounds to executable files. JDBC provides access to multimedia data through the Blob interface.","venue":"","listofauthors":"G. Speegle","citations":0,"year":2002,"publisher":"Elsevier","pages":"55-72","volume":null,"number":null,"bibtex":"@incollection{2002,\n\tdoi = {10.1016/b978-155860736-1/50035-6},\n\turl = {https://doi.org/10.1016%2Fb978-155860736-1%2F50035-6},\n\tyear = 2002,\n\tpublisher = {Elsevier},\n\tpages = {55--72},\n\tauthor = {Gregory D. Speegle},\n\ttitle = {Advanced {JDBC} Topics}\n}","authorsSemantic":[10]},{"id":502,"title":"Querying the Database","doi":"10.1016/B978-155860736-1/50033-2","description":"This chapter explains different ways to query the database by creating different Statement objects. The Statement class is extended by two subclasses: PreparedStatement and CallableStatement. CallableStatement is a subclass of PreparedStatement. When a PreparedStatement is created, part of the query is sent to the database. The query includes the tables used in the query and the names of the columns returned. Parts of the Where clause can be omitted with the understanding that they are filled in later. The ability to do part of the query ahead of time is very beneficial when large numbers of similar queries are going to be sent to the database. A database management system (DBMS) caches the information about the query and thus performs subsequent queries faster than when each query is sent individually. The CallableStatement interface executes a stored procedure at the DBMS. Both PreparedStatements and CallableStatements generate dynamic and updatable ResultSets.","venue":"","listofauthors":"G. Speegle","citations":0,"year":2002,"publisher":"Elsevier","pages":"33-42","volume":null,"number":null,"bibtex":"@incollection{2002,\n\tdoi = {10.1016/b978-155860736-1/50033-2},\n\turl = {https://doi.org/10.1016%2Fb978-155860736-1%2F50033-2},\n\tyear = 2002,\n\tpublisher = {Elsevier},\n\tpages = {33--42},\n\tauthor = {Gregory D. Speegle},\n\ttitle = {Querying the Database}\n}","authorsSemantic":[10]},{"id":503,"title":"Presenting Information to Users","doi":"10.1016/B978-155860736-1/50032-0","description":"This chapter investigates the first part of JDBC that is to present information to the users. In the world of interactive programs, a textual interface is unappealing and inefficient for users. To present tabular information stored in databases with Java, a graphic component that resembles the table structure of databases is needed. The graphical component of choice is the JTable class in the javax.swing package. The use of JTables to represent relational database tables is natural. Several third-party extensions to the JTable class automatically make this connection. A database table contains rows, and each row comprises columns. Within JDBC, the table resulting from a query is mapped to a ResultSet object. A ResultSet object translates into a Vector of Vectors. Each row in the ResultSet is turned into a Vector by adding all of the fields to a vector. Then all row Vectors are added to another Vector. Finally, JTable objects have rows mapped to a row Vector. The columns in the select clause of the query become the columns in the JTable, and all of the rows in the database satisfying the Where clause appear as rows in the JTable.","venue":"","listofauthors":"G. Speegle","citations":0,"year":2002,"publisher":"Elsevier","pages":"15-31","volume":null,"number":null,"bibtex":"@incollection{2002,\n\tdoi = {10.1016/b978-155860736-1/50032-0},\n\turl = {https://doi.org/10.1016%2Fb978-155860736-1%2F50032-0},\n\tyear = 2002,\n\tpublisher = {Elsevier},\n\tpages = {15--31},\n\tauthor = {Gregory D. Speegle},\n\ttitle = {Presenting Information to Users}\n}","authorsSemantic":[10]},{"id":504,"title":"JDBC: Practical Guide for Java Programmers","doi":null,"description":"1 - Introduction to JDBC 2 - Presenting Information to Users 3 - Querying the Database 4 - Updating the Database 5 - Advanced JDBC Topics 6 - An eCommerce Example 7 - How to Stay Current with JDBC 8 - Appendix","venue":"","listofauthors":"G. Speegle","citations":3,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":505,"title":"Extending databases to support image editing","doi":"10.1109/ICME.2000.869586","description":"In order to understand similarity between images, recent research has focused on adaptable searches (Seidl and Kreigel, 1997) and fuzzy queries (Fagin, 1998). However, one of the best means for determining similarity between images is to know how the image was created (Brown et al., 1999). If the image is a combination of other images in the database, then there is a great deal of similarity between the base images and the created one. This requires extending the database to support image editing operations. We have built a preliminary system which does this by using a Web-based image editor and an image server. The editor and the server understand a logical model language that represents images. This paper then explores the issues of performance for deriving images from a sequence of operations.","venue":"2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)","listofauthors":"G. Speegle, Allen M. Gao, Shaowen Hu, L. Gruenwald","citations":5,"year":0,"publisher":"IEEE","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/icme.2000.869586},\n\turl = {https://doi.org/10.1109%2Ficme.2000.869586},\n\tpublisher = {{IEEE}},\n\tauthor = {G. Speegle and A.M. Gao and  Shaowen Hu and L. Gruenwald},\n\ttitle = {Extending databases to support image editing}\n}","authorsSemantic":[10]},{"id":506,"title":"Issues in using specifications to improve content-based search of multimedia data","doi":"10.1109/DANTE.1999.844954","description":"Many current multimedia database management systems perform content-based retrieval of images by extracting the values of various features from every object stored in their system. This can be time-consuming, especially if extracting the features requires a human to analyze each object. This process can be minimized in multimedia database systems that store images as a sequence of editing operations, called specifications, instead of the usual binary format. This paper discusses the advantages and applicability of such systems and the issues that must be resolved in order to develop them.","venue":"Proceedings 1999 International Symposium on Database Applications in Non-Traditional Environments (DANTE'99) (Cat. No.PR00496)","listofauthors":"L. Brown, L. Gruenwald, G. Speegle","citations":1,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/dante.1999.844954},\n\turl = {https://doi.org/10.1109%2Fdante.1999.844954},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {L. Brown and L. Gruenwald and G. Speegle},\n\ttitle = {Issues in using specifications to improve content-based search of multimedia data}\n}","authorsSemantic":[10]},{"id":507,"title":"A Meta-Structure for Supporting Multimedia Editing in Object-Oriented Databases","doi":"10.1007/BFb0053474","description":"Multimedia databases include many types of new data. One common property of these data items is that they are very large. We exploit the concept of transformational representations to logically store images without the bitmap. This technique is similax to views in traditional database systems. The technique is useful when users axe editing images in the database to create new images. Our method produces significant space savings over already compressed images, and potentially greater savings for video and audio. The method requires the support of multimedia editors. This paper emphasizes the meta-structure needed by the database to support multimedia editing.","venue":"BNCOD","listofauthors":"G. Speegle, Xiaojun Wang, L. Gruenwald","citations":8,"year":1998,"publisher":"Springer Berlin Heidelberg","pages":"89-102","volume":null,"number":null,"bibtex":"@incollection{1998,\n\tdoi = {10.1007/bfb0053474},\n\turl = {https://doi.org/10.1007%2Fbfb0053474},\n\tyear = 1998,\n\tpublisher = {Springer Berlin Heidelberg},\n\tpages = {89--102},\n\tauthor = {Greg Speegle and Xiaojun Wang and Le Gruenwald},\n\ttitle = {A meta-structure for supporting multimedia editing in object-oriented databases}\n}","authorsSemantic":[10]},{"id":508,"title":"Research issues in view-based multimedia database systems","doi":null,"description":"Audio, video, graphics, and text can be combined into stunning multimedia presentations which enable people to learn, buy, and have fun in ways never before possible. However multimedia is often unorganized and required tremendous storage space. Typical images are measured in tens or hundreds of kilobytes. To control the amount of space needed in this new environment, this paper proposes a multimedia database management system that uses logical representations instead of physical representations, to store edited objects. Research issues associated with such a system are discussed.","venue":"","listofauthors":"L. Gruenwald, G. Speegle","citations":6,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":509,"title":"Content-based Retrieval","doi":"10.1007/978-0-387-39940-9_2282","description":"null","venue":"Encyclopedia of Database Systems","listofauthors":"K. A. Ross, C. Jensen, R. Snodgrass, C. Dyreson, Spiros Skiadopoulos, C. Sirangelo, M. Larsgaard, G. Grahne, Daniel Kifer, H. Jacobsen, H. Hinterberger, A. Deutsch, Alan Nash, K. Wada, W. Aalst, C. Dyreson, P. Mitra, I. Witten, Bing Liu, C. Aggarwal, M. Özsu, Chimezie Ogbuji, Chintan Patel, C. Weng, Adam Wright, Amnon Shabo (Shvo), D. Russler, R. Rocha, Y. Lussier, James L. Chen, M. Zaki, A. Corral, M. Vassilakopoulos, D. Gunopulos, D. Wolfram, S. Venkatasubramanian, M. Vazirgiannis, I. Davidson, Sunita Sarawagi, L. Peyton, G. Speegle, V. Vianu, D. V. Gucht, O. Etzion, F. Curbera, AnnMarie Ericsson, M. Berndtsson, J. Mellin, P. Gray, Goce Trajcevski, O. Wolfson","citations":5,"year":2009,"publisher":"Springer US","pages":"466-466","volume":null,"number":null,"bibtex":"@incollection{2009,\n\tdoi = {10.1007/978-0-387-39940-9_2282},\n\turl = {https://doi.org/10.1007%2F978-0-387-39940-9_2282},\n\tyear = 2009,\n\tpublisher = {Springer {US}},\n\tpages = {466--466},\n\ttitle = {Content-based Retrieval}\n}","authorsSemantic":[10]},{"id":510,"title":"Views of media objects in multimedia databases","doi":"10.1109/MMDBMS.1995.520419","description":"Multimedia database systems must manage large amounts of graphical, textual, and audio data. To compound the problem, if a multimedia designer creates a new media image from an existing item, current systems store the new object as an independent image. This space is wasted since the data already exists in the system. In order to eliminate this wasted space, views of stored objects should be used instead of replicated data. Views are well known in relational databases to have both benefits and drawbacks. In order to overcome these drawbacks, the paper defines a view schema, which mirrors the media hierarchy; a view dag which determines the base files used to create the view; and view independence, a paradigm for performing operations on views. The criteria for a canonical media algebra are outlined. A canonical media algebra allows multiple editors to instantiate a view. Finally, optimization of specifications is discussed.","venue":"Proceedings. International Workshop on Multi-Media Database Management Systems","listofauthors":"G. Speegle","citations":8,"year":0,"publisher":"IEEE Comput. Soc. Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/mmdbms.1995.520419},\n\turl = {https://doi.org/10.1109%2Fmmdbms.1995.520419},\n\tpublisher = {{IEEE} Comput. Soc. Press},\n\tauthor = {G. Speegle},\n\ttitle = {Views of media objects in multimedia databases}\n}","authorsSemantic":[10]},{"id":511,"title":"Formal aspects of concurrency control in long-duration transaction systems using the NT/PV model","doi":"10.1145/185827.185854","description":"In the typical database system, an execution is correct if it is equivalent to some serial execution. This criterion, called serializability, is unacceptable for new database applications which require long-duration transactions. We present a new transaction model which allows correctness criteria more suitable for these applications. This model combines three enhancements to the standard model: nested transactions, explicit predicates, and multiple versions. These features yield the name of the new model, nested transactions with predicates and versions, or NT/PV.\nThe modular nature of the NT/PV model allows a straightforward representation of simple systems. It also provides a formal framework for describing complex interactions. The most complex interactions the model allows can be captured by a protocol which exploits all of the semantics available to the NT/PV model. An example of these interactions is shown in a CASE application. The example shows how a system based on the NT/PV model is superior to both standard database techniques and unrestricted systems in both correctness and performance.","venue":"TODS","listofauthors":"H. F. Korth, G. Speegle","citations":67,"year":1994,"publisher":"Association for Computing Machinery (ACM)","pages":"492-535","volume":"19","number":"3","bibtex":"@article{1994,\n\tdoi = {10.1145/185827.185854},\n\turl = {https://doi.org/10.1145%2F185827.185854},\n\tyear = 1994,\n\tmonth = {sep},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {19},\n\tnumber = {3},\n\tpages = {492--535},\n\tauthor = {Henry F. Korth and Greg Speegle},\n\ttitle = {Formal aspects of concurrency control in long-duration transaction systems using the {NT}/{PV} model}\n}","authorsSemantic":[10]},{"id":512,"title":"Resolving Result Set Contention in Heterogeneous Library Information Systems","doi":null,"description":"Federated databases are one of the challenges for database system builders. In an e ort to create these systems, current implementations require every member of the federation to agree to a particular standard. We examine query optimization under the Z39.50 standard for heterogeneous library information systems, looking speci cally at the problem of managing temporary relations within a library system limiting the number of these relations held by a single user. We propose a strategy which uses knowledge of the current user query and an approximation of a set of future user queries in order to choose which temporary relation is the least likely to be useful in answer the current and future queries. Simulation results indicate that this strategy, called Look-Ahead, performs better than strategies based on operating system concepts. Work supported in part by a summer sabbatical from Baylor University","venue":"","listofauthors":"G. Speegle, M. J. Donahoo","citations":0,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":513,"title":"Quantifying the benefits of semantics","doi":"10.1145/131214.131267","description":"An active area of current research is the use of semantics in concurrency control. Simulations using a new concurrency control protocol, called complex two-phase locking, can quantify the benefits of using semantics within long-duration transaction systems. It is then possible to determine if the benefits gained are worth the human effort required to obtain the semantics.","venue":"CSC '92","listofauthors":"G. Speegle, Andrew L. Gordon","citations":1,"year":1992,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1992,\n\tdoi = {10.1145/131214.131267},\n\turl = {https://doi.org/10.1145%2F131214.131267},\n\tyear = 1992,\n\tpublisher = {{ACM} Press},\n\tauthor = {Gregory D. Speegle and Andrew L. Gordon},\n\ttitle = {Quantifying the benefits of semantics}\n}","authorsSemantic":[10]},{"id":514,"title":"Intra-Transaction Concurrency Control and the NT/PV Model","doi":null,"description":"In nested transactions systems, it is possible for each level of a transaction to have its own correctness criteria. This results in a complex, multilevel concurrency control problem. We propose an approach to this problem called the intra-transaction concurrency control paradigm for handling multilevel concurrency control applications. This paradigm is based on defining a restricted scope for the concurrency control protocols used at each level of nesting. As such, the process of deriving multilevel protocols is simplified. To demonstrate this, two example protocols, one for the class conflict serializability (CSR) and one for the class conflict predicate correct (CPC) are presented.","venue":"","listofauthors":"H. F. Korth, G. Speegle","citations":1,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":515,"title":"The nt/pv model: a new representation for long-duration transaction systems","doi":null,"description":"null","venue":"","listofauthors":"G. Speegle, H. F. Korth","citations":2,"year":0,"publisher":null,"pages":null,"volume":null,"number":null,"bibtex":null,"authorsSemantic":[10]},{"id":516,"title":"Long-duration transactions in software design projects","doi":"10.1109/ICDE.1990.113512","description":"An example of a software development application is considered, and the formal model of H. Korth and G. Speegle (1988) is applied to show how this example could be represented as a set of database transactions. It is shown that, although the standard notion of correctness (serializability) is too strict, the notion of correctness in the Korth and Speegle model allows sufficient concurrency with acceptable overhead. An extrapolation is made from this example to draw some conclusions regarding the potential usefulness of a formal approach to the management of long-duration design transactions.<<ETX>>","venue":"[1990] Proceedings. Sixth International Conference on Data Engineering","listofauthors":"H. F. Korth, G. Speegle","citations":38,"year":0,"publisher":"IEEE Comput. Soc","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1,\n\tdoi = {10.1109/icde.1990.113512},\n\turl = {https://doi.org/10.1109%2Ficde.1990.113512},\n\tpublisher = {{IEEE} Comput. Soc},\n\tauthor = {H.F. Korth and G.D. Speegle},\n\ttitle = {Long-duration transactions in software design projects}\n}","authorsSemantic":[10]},{"id":517,"title":"Formal model of correctness without serializabilty","doi":"10.1145/50202.50248","description":"In the classical approach to transaction processing, a concurrent execution is considered to be correct if it is equivalent to a non-concurrent schedule. This notion of correctness is called serializability. Serializability has proven to be a highly useful concept for transaction systems for data-processing style applications. Recent interest in applying database concepts to applications in computer-aided design, office information systems, etc. has resulted in transactions of relatively long duration. For such transactions, there are serious consequences to requiring serializability as the notion of correctness. Specifically, such systems either impose long-duration waits or require the abortion of long transactions. In this paper, we define a transaction model that allows for several alternative notions of correctness without the requirement of serializability. After introducing the model, we investigate classes of schedules for transactions. We show that these classes are richer than analogous classes under the classical model. Finally, we show the potential practicality of our model by describing protocols that permit a transaction manager to allow correct non-serializable executions","venue":"SIGMOD '88","listofauthors":"H. F. Korth, G. Speegle","citations":122,"year":1988,"publisher":"ACM Press","pages":null,"volume":null,"number":null,"bibtex":"@inproceedings{1988,\n\tdoi = {10.1145/50202.50248},\n\turl = {https://doi.org/10.1145%2F50202.50248},\n\tyear = 1988,\n\tpublisher = {{ACM} Press},\n\tauthor = {H. K. Korth and G. Speegle},\n\ttitle = {Formal model of correctness without serializabilty}\n}","authorsSemantic":[10]},{"id":518,"title":"Formal Model of Correctness Without Serializability","doi":"10.1145/971701.50248","description":"null","venue":"SIGMOD Conference","listofauthors":"H. F. Korth, G. Speegle","citations":12,"year":1988,"publisher":"Association for Computing Machinery (ACM)","pages":"379-386","volume":"17","number":"3","bibtex":"@article{1988,\n\tdoi = {10.1145/971701.50248},\n\turl = {https://doi.org/10.1145%2F971701.50248},\n\tyear = 1988,\n\tmonth = {jun},\n\tpublisher = {Association for Computing Machinery ({ACM})},\n\tvolume = {17},\n\tnumber = {3},\n\tpages = {379--386},\n\tauthor = {H. K. Korth and G. Speegle},\n\ttitle = {Formal model of correctness without serializabilty}\n}","authorsSemantic":[10]}]